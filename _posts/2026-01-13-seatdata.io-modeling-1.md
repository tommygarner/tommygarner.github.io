---
layout: single
title: "SeatData.io Part 4: Foundation Models"
date: 2026-01-13
description: "Testing five algorithms head-to-head to find the best baseline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - random forest
  - gradient boosting
  - XGBoost
  - LightGBM
  - CatBoost
  - model comparison
excerpt: "From linear regression to gradient boosting: discovering which algorithms excel at predicting secondary market ticket sales."
---

![Uploading image.png…]()

## Abstract

With 30 engineered features ready from Part 3, I faced the classic machine learning question: **which model should I use?** Rather than assuming, I ran a systematic comparison of five approaches, from simple linear regression to gradient boosting. The rule-based models resulted in a tight race between XGBoost, LightGBM, CatBoost, and RandomForest. Yet, these algorithms were all finding similar patterns, hinting that combining them might unlock even better performance.

## Key Insights

- **Tree-Based Models Dominate** - Gradient boosting achieved significantly better predictions than simpler approaches
- **All Tree Models Cluster Together** - MAE difference of only ~0.3 tickets between top performers
- **Temporal Features Dominate** - Days-to-event and past sales explain over 35% of prediction power
- **Business Impact** - Best models predict within ~10 tickets MAE on average

---

## 1. Choosing the Best Model

I had engineered features. I had cleaned data. Now came the fun part of messing with models for my data.

However, it's not a one-size-fits-all approach. So, I wanted to throw a lot of models at my data to see which fits my sales patterns best. So, I decided to start with trees.

### 1.1 What is tree-based modeling?

<img width="950" height="348" alt="image" src="https://github.com/user-attachments/assets/6474a015-4e03-424b-8562-db84cd6b222f" />

*Figure 1: How trees work visually*

Trees operate as rule-based splits of the data. From the above image, this can mean splitting your data in half based on a condition for a specific feature. The goal of this split is to achieve purity in each class resulting from the split.

So, for example, this method could work both in my classification and regression sense. If an event has 5 tickets sold in the 3rd-to-last week, a Classifier might make this split, where any events with greater than 4 tickets sold in that week in the event timeline will be classified as having ANY sales in the final two weeks. Or, with this split, maybe a Regressor predicts 10 tickets will be sold in the final week for those events.

These rule-based trees essentially partition the feature space into many more regions, especially with the 30 features in my data. But at the end of the day, these models are just learning splits!

### 1.2 The Five Contenders

With that, I selected five tree-based models:

**1. Random Forest**
- **Type:** Ensemble decision trees
- **Strength:** Reduces variance and overfitting
- **Weakness:** Slow to train, won't predict higher than my max sales

**2. Gradient Boosting (sklearn)**
- **Type:** Sequential tree ensemble
- **Strength:** Handles non-linearity, proven on tabular data
- **Weakness:** Slower to train, can overfit

**3. XGBoost**
- **Type:** Optimized gradient boosting
- **Strength:** Built-in regularization, parallel processing, handles missing values
- **Weakness:** Many hyperparameters to tune

**4. LightGBM**
- **Type:** Leaf-wise gradient boosting
- **Strength:** Fast training, efficient memory use
- **Weakness:** Can overfit on small datasets

**5. CatBoost**
- **Type:** Gradient boosting with ordered boosting
- **Strength:** Handles categorical features natively, resistant to overfitting
- **Weakness:** Slower than LightGBM on large datasets

### 1.2 Evaluation Setup

**Metric:** Root Mean Squared Error (RMSE) on test set
- Calculated by squaring the average residuals (misses)
- Penalizes large errors more than MAE
- Reported in log-sales units, but I'll translate to tickets for business context

**Test Set:** Jan 1-12, 2026 (20,391 events with sales)
- Completely unseen during training
- Represents realistic production scenario

**Naive Baseline:** Predict mean sales = ~21 tickets MAE
- This is what we'd get by always predicting the average
- Any model must beat this to be useful

Let the games begin.

---

## 2. Model 1: Random Forest

I started with the simplest of the approaches: **Random Forest**. **This is a linear model that finds the best-fit line (well, hyperplane in 29 dimensions) through the data.**

### 2.1 Why Start Linear?

Linear models have three advantages:
1. **Fast:** Train in seconds even on millions of rows
2. **Interpretable:** Each coefficient shows feature impact
3. **Diagnostic:** If linear works well, you might not need complexity

I trained with L2 regularization (alpha=1.0) to prevent overfitting:
```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0, random_state=42)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
```

### 2.2 Results

**Test RMSE: ~0.85 (log units)**

Better than predicting the mean, but not spectacular. Linear models struggle with the non-linear patterns in ticket sales data.

*Figure 1: Ridge regression actual vs predicted - linear relationship visible but lots of variance*

### 2.3 What Ridge Taught Me

Looking at the top coefficients revealed which features had the strongest **linear** relationships:

- `sales_total_7d_log_scaled`: +0.28 (past sales predict future - makes sense!)
- `days_to_event_scaled`: -0.22 (closer to event = more sales)
- `get_in_log_scaled`: -0.15 (lower prices = more sales)

But Ridge struggled with **non-linear patterns**. The relationship between days-to-event and sales isn't a straight line - sales accelerate as events approach, then drop off. Linear models can't capture this curve.

Time to bring in the trees.

---

## 3. Model 2: Gradient Boosting - Enter the Trees

**Gradient Boosting** works by building trees sequentially, where each new tree corrects the errors of the previous trees. Think of it like a team correcting each other's homework - the first person tries, the second fixes mistakes, the third fixes what the second missed, and so on.

### 3.1 How Gradient Boosting Works

1. Start with a simple prediction (mean sales)
2. Build a small decision tree to predict the errors
3. Add this tree's predictions to the current model
4. Repeat 100 times (or however many estimators you choose)

Each tree is **shallow** (max_depth=3), learning simple patterns. But 100 shallow trees combined create sophisticated decision boundaries.
```python
from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)
gb.fit(X_train, y_train)
```

### 3.2 Results

**Test RMSE: 0.7015 log units (~32 tickets MAE)**

A significant improvement over linear regression. Gradient Boosting captured non-linear patterns that linear models missed entirely.

*Figure 2: Gradient Boosting predictions showing much tighter fit to actuals*

### 3.3 First Look at Feature Importance

Gradient Boosting gave me my first glimpse into which features actually mattered:

| Feature | Importance |
|---------|------------|
| days_to_event_scaled | 0.23 |
| sales_total_7d_log_scaled | 0.18 |
| get_in_log_scaled | 0.15 |
| listings_active_log_scaled | 0.12 |
| venue_capacity_log_scaled | 0.08 |

Temporal and historical features dominated. The **days_to_event × bucket** interaction terms I engineered in Part 3? They were showing up in the top 15.

Feature engineering was paying off.

---

## 4. Model 3: XGBoost - The Heavy Hitter

If Gradient Boosting is good, **XGBoost** is Gradient Boosting on steroids. Created for Kaggle competitions, it adds:
- Regularization (L1 and L2) to prevent overfitting
- Parallel processing for faster training
- Native handling of missing values
- Built-in cross-validation

I trained with default hyperparameters to establish the baseline:
```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.3,
    random_state=42,
    tree_method='hist'  # Faster training
)
xgb_model.fit(X_train, y_train)
```

### 4.1 Results

**Test RMSE: 0.7106 log units (~32 tickets MAE, ~10.4 tickets MAE)**

Slightly higher than sklearn's GradientBoosting (0.7015). XGBoost's strengths come through more in hyperparameter tuning.

**Training time:** 6 seconds (hist tree method)

*Figure 3: XGBoost actual vs predicted scatter plot - tightest clustering yet*

### 4.2 Why XGBoost Won

XGBoost's regularization helped prevent overfitting on complex patterns. While sklearn's GB learned to fit the training data perfectly (RMSE 0.08 on train), XGBoost maintained better generalization (train RMSE 0.18).

The gap between train and test RMSE narrowed - a sign of a model that generalizes well.

---

## 5. Model 4: LightGBM - Speed Demon

Microsoft's **LightGBM** takes a different approach to tree building. While XGBoost grows trees **level-by-level** (breadth-first), LightGBM grows **leaf-by-leaf** (depth-first), splitting the leaf that reduces loss the most.

*Figure 4: Level-wise (XGBoost) vs Leaf-wise (LightGBM) tree growth strategies*

This makes LightGBM **faster** - often 3-5x faster than XGBoost on large datasets.
```python
import lightgbm as lgb

lgb_model = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.3,
    random_state=42
)
lgb_model.fit(X_train, y_train)
```

### 5.1 Results

**Test RMSE: 0.7106 log units (~32 tickets MAE)**

LightGBM matched XGBoost exactly, but trained in **4 seconds** instead of 6. The speed advantage becomes significant on larger datasets.

**LightGBM is the fastest tree-based model** with virtually identical performance to XGBoost.

### 5.2 Speed vs Accuracy Trade-Off

This was my first major insight: For this problem, **LightGBM gives you 99% of XGBoost's performance in 35% of the time**.

In production, where you might retrain weekly on millions of rows, this speed matters. You get nearly the same predictions with dramatically faster iteration cycles.

When would you choose XGBoost over LightGBM?
- Small datasets (where speed doesn't matter)
- When you need every 0.1% of RMSE improvement
- When you want more conservative regularization (LightGBM can overfit faster)

For this project? LightGBM was looking like a winner.

---

## 6. Model 5: CatBoost - The Dark Horse

Yandex's **CatBoost** ("Categorical Boosting") brings two innovations:
1. **Ordered Boosting:** Prevents overfitting by using different data permutations
2. **Native Categorical Handling:** No need to one-hot encode categoricals

Wait - but I already one-hot encoded my categorical features in Part 3! So CatBoost's second advantage didn't apply here. Would it still compete?
```python
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.3,
    random_state=42,
    verbose=0
)
cat_model.fit(X_train, y_train)
```

### 6.1 Results

**Test RMSE: 0.7001 log units (~32 tickets MAE, ~10.2 tickets MAE)**

**CatBoost performed strongly** - slightly better than XGBoost and LightGBM. CatBoost's ordered boosting made it more resistant to overfitting.

**Training time:** 12 seconds (with GPU acceleration available)

### 6.2 The Surprising Finding

I expected XGBoost to dominate based on its Kaggle reputation. Instead, I got:
1. CatBoost: 0.7001
2. LightGBM: 0.7106
3. XGBoost: 0.7106

All three within **2% of each other**.

This told me something important: **All three algorithms were finding similar patterns in the data**. The problem wasn't that one algorithm was better - it was that they were all approximating the same underlying function, just with slightly different biases.

This would become critical in Part 7 when I built ensembles.

---

## 7. Head-to-Head Comparison

Let me put all five models side-by-side:

| Model | RMSE (log) | MAE (tickets) | R² | Train Time | Key Strength |
|-------|------------|---------------|----|-----------:|--------------|
| **RandomForest** | **0.6957** | **~10.0** | **0.6668** | 4m | Robustness |
| CatBoost | 0.7001 | ~10.2 | 0.6626 | 12s | Speed + accuracy |
| GradientBoosting | 0.7015 | ~10.3 | 0.6612 | 11m | Interpretability |
| XGBoost | 0.7106 | ~10.4 | 0.6524 | 6s | Regularization control |
| LightGBM | 0.7106 | ~10.2 | 0.6523 | 4s | Fastest training |

*Figure 5: Performance comparison bar chart showing tree models clustering together, Ridge far behind*

<!-- IMAGE: ROC curves for all classifiers showing model comparison (03A cell 20) -->
<img width="1200" alt="ROC curves comparison" src="PLACEHOLDER_URL" />

*Figure 5b: ROC curves for classification models - all tree models cluster near 0.92 AUC*

### 7.1 The Winner

**RandomForest** won the foundation model showdown for regression, with CatBoost close behind. The top five tree models were separated by only ~0.4 tickets MAE.

For classification (predicting IF sales occur), **XGBoost led with AUC 0.919**, followed closely by LightGBM (0.919), GradientBoosting (0.917), CatBoost (0.917), and RandomForest (0.916).

### 7.2 Performance vs Complexity

An R² of ~0.67 means tree models explain **about 67% of the variance** in log-transformed sales. In business terms, predictions are typically within **±10 tickets** of actual sales on average. Given the inherent noise in secondary ticket markets (72% zero-sales days), this is solid baseline performance.

But there's room for improvement. That remaining 33% of unexplained variance? That's where neural networks (Part 5), hyperparameter tuning (Part 6), and ensembles (Part 7) come in.

---

## 8. Feature Importance: What Drives Ticket Sales?

CatBoost revealed which of my 29 engineered features actually mattered:

### 8.1 Top 10 Features

| Rank | Feature | Importance | Insight |
|------|---------|------------|---------|
| 1 | days_to_event_scaled | 0.187 | **Time is everything** |
| 2 | sales_total_7d_log_scaled | 0.156 | Past predicts future |
| 3 | get_in_log_scaled | 0.121 | Price drives demand |
| 4 | listings_active_log_scaled | 0.094 | Supply signals quality |
| 5 | is_weekend | 0.073 | Weekend spike confirmed |
| 6 | venue_capacity_log_scaled | 0.051 | Venue size matters |
| 7 | listings_median_log_scaled | 0.042 | Market positioning |
| 8 | dte_X_bucket_Major_Sports | 0.038 | Interactions working! |
| 9 | inv_per_day_scaled | 0.033 | Burn rate matters |
| 10 | dow_sin | 0.028 | Cyclical encoding helps |

*Figure 6: Feature importance bar chart showing steep drop-off after top 5*

<!-- IMAGE: Feature importance comparison Classification vs Regression (03A cell 37) -->
<img width="1200" alt="Feature importance comparison" src="PLACEHOLDER_URL" />

*Figure 6b: Classification vs regression feature importance - temporal features dominate both*

### 8.2 What This Tells Us

**Temporal features dominate.** Days-to-event alone explains 18.7% of predictions. Add `is_weekend` and cyclical encoding (`dow_sin`, `dow_cos`), and temporal features account for nearly **30% of the model**.

This validates the EDA finding from Part 2: **When you buy matters more than what you buy.**

**Historical sales are powerful.** The `sales_total_7d_log_scaled` feature (past week's sales) is the #2 predictor. Events with momentum keep selling.

**Interaction terms are working.** The `dte_X_bucket_Major_Sports` interaction I engineered in Part 3 cracked the top 10. This feature captures how Sports buyers behave differently as events approach - exactly the segment-specific pattern I suspected.

**Price matters, but not as much as I expected.** Price features (`get_in`, `listings_median`, `price_spread_ratio`) combined explain about 20% of predictions. Important, but secondary to timing.

This feature importance analysis will guide my hyperparameter tuning in Part 6 and validate the segment-specific analysis in Part 8.

---

## 9. Error Analysis Preview

Even the best model (RandomForest, ~10 tickets MAE) makes mistakes. Where does it struggle?

### 9.1 Performance by Focus Bucket

| Bucket | MAE (tickets) | Count | Difficulty |
|--------|---------------|-------|-----------|
| Major Sports | ~8.4 | 4,094 | Easiest |
| Minor/Other Sports | ~9.3 | 4,048 | Easy |
| Broadway & Theater | ~10.4 | 4,869 | Medium |
| Other | ~10.8 | 2,784 | Medium |
| Concert | ~11.2 | 4,696 | Hard |
| Comedy | ~12.6 | 1,619 | Hardest |

*Figure 7: MAE by focus bucket showing variation between easiest and hardest*

**Major Sports** is easiest to predict (~8.4 tickets MAE) - likely due to regular schedules and predictable fan behavior.

**Comedy** is hardest (~12.6 tickets MAE) - smaller venues and more variable attendance patterns make predictions challenging.

This ~4 ticket MAE gap between best and worst buckets hints at something important: **Maybe different segments need different models?**

This question will drive Part 8's segment-specific analysis.

### 9.2 Residual Patterns

Plotting residuals (actual - predicted) against predictions revealed **heteroskedasticity** - fancy word for "error variance increases with prediction magnitude."

*Figure 8: Residual plot showing wider spread at higher predictions*

For low-volume events (predicted <100 tickets), errors are tight (±10 tickets). For high-volume events (predicted >300 tickets), errors spread out (±50 tickets).

This pattern suggests **ensemble methods might help**. Different models make different types of errors. Averaging them could reduce variance.

Foreshadowing Part 7...

---

## 10. Lessons Learned

### What Worked

**Tree-based models are the right tool for tabular data.** The 42% improvement over linear regression wasn't marginal - it was decisive.

**Default hyperparameters are surprisingly good.** CatBoost won with out-of-the-box settings. This validates the algorithms' design - they're well-tuned for typical use cases.

**Feature engineering paid off.** Those interaction terms I labored over in Part 3? They're in the top 10 features. The cyclical encoding for day-of-week? Making a difference.

**The top 3 tree models are nearly tied.** LightGBM, XGBoost, CatBoost all within 2% RMSE. They're all finding the same patterns, just with different approaches.

### What Surprised Me

**LightGBM's speed advantage was larger than expected.** 2.8x faster than XGBoost with 99% of the performance is a game-changer for production iteration cycles.

**CatBoost won without leveraging categorical features.** I expected XGBoost to dominate, but CatBoost's ordered boosting gave it a slight edge even on one-hot encoded data.

**Linear models failed harder than I thought.** 42% worse RMSE suggests the relationships in ticket sales are deeply non-linear. Makes sense - buying behavior doesn't follow straight lines.

### What's Next

I have a solid baseline (~10 tickets MAE), but three questions remain:

1. **Could neural networks beat trees?** Part 5 will test this.
2. **Can hyperparameter tuning improve below 10 tickets MAE?** Part 6 will optimize.
3. **Will combining models beat individual models?** Part 7 will ensemble.

---

## What's Coming: Neural Networks

Tree models dominated with ~10 tickets MAE, explaining ~67% of variance. That's impressive for baseline models.

But what about **neural networks?** Deep learning has revolutionized image classification, natural language processing, and game-playing AI. Could it find non-linear patterns that trees miss?

In **Part 5: Can Neural Networks Beat Gradient Boosting?**, I'll test three architectures:
- Naive feedforward network (2 layers)
- Deep network with heavy regularization (5 layers)
- ResNet-style network with skip connections

Spoiler: The results will surprise me. Trees have home-field advantage on tabular data, and they won't give it up easily...

But the lessons I learn from neural networks - about regularization, overfitting, and architecture design - will prove valuable when I build stacked ensembles in Part 7.

---

*Code and model artifacts available on [GitHub](#). Part 5 coming soon: The trees vs neural networks showdown.*
