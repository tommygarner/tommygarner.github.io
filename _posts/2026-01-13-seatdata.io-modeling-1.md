---
layout: single
title: "SeatData.io Part 4: Foundation Models"
date: 2026-01-13
description: "Testing five algorithms head-to-head to find the best baseline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - random forest
  - gradient boosting
  - XGBoost
  - LightGBM
  - CatBoost
  - scikit-learn
excerpt: "From linear regression to gradient boosting: discovering which algorithms excel at predicting secondary market ticket sales."
---

<img width="612" height="367" alt="image" src="https://github.com/user-attachments/assets/95e263d7-269b-4080-be57-4bb0d4116057" />

## Abstract

With 30 engineered features ready from Part 3, I faced the classic machine learning question: **which model should I use?** Rather than assuming, I ran a systematic comparison of five approaches, from simple linear regression to gradient boosting. The rule-based models resulted in a tight race between XGBoost, LightGBM, CatBoost, and RandomForest. Yet, these algorithms were all finding similar patterns, hinting that combining them might unlock even better performance.

## Key Insights

- **Tree-Based Models Dominate** - Gradient boosting achieved significantly better predictions than simpler approaches
- **All Tree Models Cluster Together** - MAE difference of only ~0.3 tickets between top performers
- **Temporal Features Dominate** - Days-to-event and past sales explain over 35% of prediction power
- **Business Impact** - Best models predict within ~10 tickets MAE on average

---

## 1. Choosing the Best Model

I had engineered features. I had cleaned data. Now came the fun part of messing with models for my data.

However, it's not a one-size-fits-all approach. So, I wanted to throw a lot of models at my data to see which fits my sales patterns best. So, I decided to start with trees.

### 1.1 Setting up a baseline

I first wanted to go to the basics and break down a Classifier and Regressor before jumping into trees and even more modeling approaches. So, I went back to the Logistic and Linear Regression.

### 1.2 Logistic Regression

Before I could predict how many tickets would sell, I had to see if a model could even distinguish if a single ticket would sell for an event. Since my data was so imbalanced, this would help keep the nonzero sale events out.

I ended up implementing logistic regression to get a baseline for these classifications.

<img width="686" height="386" alt="image" src="https://github.com/user-attachments/assets/876fb7eb-6805-41ca-868b-725fc301b4f0" />

*Figure 1: Logistic Regression visualized*

Logistic Regression maps out the probability that a specific observation belongs to the positive class based on the feature space. This is the most basic of classifiers that is trying to find a linear boundary separating nonzero-sale from zero-sale events. Below shows how I performed the Logistic Regression.

```python
lr_clf = LogisticRegression(
    class_weight='balanced',  # Handle class imbalance
    max_iter=1000, solver='lbfgs', random_state=42, n_jobs=-1
)
```

Some hyperparameters are involved in this configuration that I can go ahead and explain, though you might see similar ones in later code.

- `class_weight='balanced'` ironically indicates that my data has a class imbalance between zero-sale and nonzero-sale events. This helps logistic regression favor that minority class because, without this, my model can guess no sales for every event and still be 72% accurate (and 0% useful)
- `max_iter=1000` indicates how many runs the solver will perform to converge on the best weights for classifications
- `solver='lbfgs'` is the default optimization algorithm (or loss function) and handles ridge regression to prevent overfitting
- `n_jobs=-1` is simply telling the model to use all available CPU cores on my computer to train the model

**Results**:
- PR-AUC: 0.8023

This is a tricky interpretation, but I'll try to make it simple. Since my data is made up of 72% zero-sale events, I'm focused on that 28% of the data with sales in the final two weeks. A random model would predict a PR-AUC of 0.28, or the fraction of positive samples in my data.

Therefore, this Logistic Regression is pretty accurate. Looking more closely at Precision and Recall, this classifier found 83.7% of all events that have sales in the final two weeks, and when the model does flag an event in this way, it's correct 68.17% of the time. Not bad for a baseline!

### 1.3 Linear Regression

Next, I wanted to look only at those nonzero-sale events and try to predict their volume using Linear Regression. 

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/b57ddaac-c0e8-4bdd-bd25-17ccb15f1e8f" />

*Figure 2: Linear Regression function*

Linear Regression takes into account the Maximum Likelihood Estimates of coefficients for the regression line using all 30 of my features. Using Ordinary Least Squares, I can find the best fit line that minimizes the error term. Below shows some more code!

```python
lin_reg = LinearRegression(n_jobs=-1)
lin_reg.fit(X_train_reg, y_train_reg)
```

Sci-kit learn's package makes this code super simple to work with, so let's take a look at how well the model did.

**Results**:
- RMSE (tickets): 77.65
- MAE (tickets): 13.31

This model was pretty off. Underpredicting big-sale events likely penalized this error term to its detriment, even though a simple Linear Regression can explain about 66% of the variance found in my data. This would become an easy baseline to top.

So with that, I want to introduce some of the strongest models in all of my testing.

## 2 The Trees

### 2.1 What is tree-based modeling?

<img width="950" height="348" alt="image" src="https://github.com/user-attachments/assets/6474a015-4e03-424b-8562-db84cd6b222f" />

*Figure 3: How trees work visually*

Trees operate as rule-based splits of the data. From the above image, this can mean splitting your data in half based on a condition for a specific feature. The goal of this split is to achieve purity in each class resulting from the split.

So, for example, this method could work both in my classification and regression sense. If an event has 5 tickets sold in the 3rd-to-last week, a Classifier might make this split, where any events with greater than 4 tickets sold in that week in the event timeline will be classified as having ANY sales in the final two weeks. Or, with this split, maybe a Regressor predicts 10 tickets will be sold in the final week for those events.

These rule-based trees essentially partition the feature space into many more regions, especially with the 30 features in my data. But at the end of the day, these models are just learning splits!

### 2.2 The Five Contenders

With that, I selected five tree-based models:

**1. Random Forest**
- **Type:** Ensemble decision trees
- **Strength:** Reduces variance and overfitting
- **Weakness:** Slow to train, won't predict higher than my max sales

**2. Gradient Boosting**
- **Type:** Sequential tree ensemble
- **Strength:** Handles non-linearity, proven on tabular data
- **Weakness:** Slower to train, can overfit

**3. XGBoost**
- **Type:** Optimized gradient boosting
- **Strength:** Built-in regularization, parallel processing, handles missing values
- **Weakness:** Many hyperparameters to tune

**4. LightGBM**
- **Type:** Leaf-wise gradient boosting
- **Strength:** Fast training, efficient memory use
- **Weakness:** Can overfit on small datasets

**5. CatBoost**
- **Type:** Gradient boosting with ordered boosting
- **Strength:** Handles categorical features natively, resistant to overfitting
- **Weakness:** Slower than LightGBM on large datasets

### 2.3 Evaluation Setup

**Metric 1:** Root Mean Squared Error (RMSE) on test set
- Calculated by square-rooting the average residuals (misses)
- Penalizes large errors more than MAE
- Reported in log-sales units, but I'll translate to tickets for business context

**Metric 2:** Precision-Recall Area Under the Curve (PR-AUC) on test set
- Handles class imbalanced data by focuses on how well I find the rare, nonzero sale events
- Considers the cost matrix of a false positive vs a false negative
- Focuses on the hits (true positives, false positives)

**Test Set:** Jan 1-12, 2026 (20,391 events with sales)
- Completely unseen during training
- Represents realistic production scenario

**Naive Baseline:** Predict mean sales = ~21 tickets MAE
- This is what we'd get by always predicting the average
- Any model must beat this to be useful

Let the games begin.

---

## 3. Model 1: Random Forest

I started with the simplest of the approaches: **Random Forest**. A random forest model is a collection of decision trees that are different from one another. Trees are built on different subsets of the data as well as different feature sets at random. 

Random forest models also implement bootstrap aggregating (bagging), which is simply sampling with replacement. The same subset of data, or the same feature space, can be similar between two decision trees.

<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/e095861a-c55f-42d3-a0ed-1926eae3ce4e" />

*Figure 4: Bagging visualized*

Then, Classifiers combine the predictions with majority voting, while Regressors take the average of all tree predictions.

### 3.1 Why Start Here?

Random Forest models have some advantages:
1. **Handles non-linearity:** Many rule-based splits can find the complex, non-linear relationships of my variables
2. **Reduces overfitting:** Averaging the results of many trees converges predictions
3. **Not affected by outliers:** Extreme values don't pull the model down, since rule-based splits are binning my feature space

Below is how I built the Random Forest Classifier and Regressor
```python
rfc = RandomForestClassifier(
    n_estimators=100, max_depth=10, min_samples_leaf=10,
    class_weight='balanced',  # Handle class imbalance
    n_jobs=-1, verbose=0
)
rfc.fit(X_train_cls, y_train_cls)

rfr = RandomForestRegressor(
    n_estimators=100, max_depth=10, min_samples_leaf=10,
    n_jobs=-1, verbose=0
)
rfr.fit(X_train_reg, y_train_reg)
```

This code above has some new hyperparameters I can dive into. 

- `n_estimators = 100` indicates the number of trees I want to build
- `max_depth = 10` is the limit for how deep these trees can grow, or how many decision splits a single tree can make
- `min_samples_leaf = 10` prevents any decision tree from making a niche split that results in less than 10 samples ending up in a leaf/node
- `verbose=0` simply tells VS Code to refrain from printing any log statements

### 3.2 Results

**Test PR-AUC: 0.8057**
**Test RMSE: 32.72 tickets**
**Test MAE: 9.98 tickets**

The Random Forest Regressor was 2x more effective than my Linear Regression, which tells me there is some nonlinearity involved in my sales data. The Classifier also barely outperformed my Logistic Regression, so it seems like this could be the right direction for making any headway. What's interesting is that this classifier was able to correctly predict 89.4% of nonzero-sale events in my data.

---

## 4. Model 2: Gradient Boosting

**Gradient Boosting** works by building those decision trees sequentially, where each new tree corrects the errors of the previous trees. So, with each new tree, they are essentially correcting for the previous ones' mistakes/errors.

### 4.1 How Gradient Boosting Works

1. Start with a simple prediction (mean sales)
2. Build a small decision tree to predict the errors
3. Add this tree's predictions to the current model
4. Repeat 100 times (or however many estimators you choose)

<img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/1b915567-2545-4818-a6a4-64db5ac19708" />

*Figure 5: Gradient Boosting sequential trees*


Each tree is **shallow** (max_depth=4), learning simple patterns. But 100 shallow trees combined create complex decision boundaries.

```python
gbc = GradientBoostingClassifier(
    n_estimators=100, max_depth=4, learning_rate=0.1,
    subsample=0.8, verbose=0
)
gbc.fit(X_train_cls, y_train_cls)

gbr = GradientBoostingRegressor(
    n_estimators=100, max_depth=4, learning_rate=0.1,
    subsample=0.8, verbose=0
)
gbr.fit(X_train_reg, y_train_reg)
```

So.. I can explain those new hyperparameters you see again.

- `learning_rate=0.1` is actually a really important topic in machine learning because it diminishes the amount of impact the subsequent tree's correction provides by multiplying the full prediction by 10% before adding to the model
- `subsample=0.8` adds a layer of randomness to the training, where each tree in the sequence can only look at a random 80% of the training data, which helps prevent memorization and overfitting to specific outliers

### 4.2 Results

**Test PR-AUC: 0.8113**
**Test RMSE: 31.97 tickets**
**Test MAE: 10.22 tickets**

This PR-AUC value outperformed both Random Forest and Logistic Regression, yet the RMSE and MAE on predictions of sales volume showed that this would be a poor model choice in comparison to my Random Forest model. However, when this Gradient Boosting Classifier did predict an event would have sales in the final two weeks, it was correct 72% of the time, the highest precision by far among my trees.

---

## 5. Model 3: XGBoost

Gradient Boosting will grow trees until they hit a depth limit. However, XGBoost is more cautious about potentially overfitting the data.

<img width="1280" height="742" alt="image" src="https://github.com/user-attachments/assets/1318517f-e121-4409-bb00-2ab274a93f03" />

*Figure 6: XGBoost visualization*


Created for Kaggle competitions, it adds:
- Regularization (L1 and L2) to prevent overfitting
- Parallel processing for faster training
- Native handling of missing values
- Built-in cross-validation

```python
xgbc = XGBClassifier(
    n_estimators=100, max_depth=6, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,  # Handle class imbalance
    tree_method='hist', verbosity=0, n_jobs=-1,
    eval_metric='logloss'
)
xgbc.fit(X_train_cls, y_train_cls)

xgbr = XGBRegressor(
    n_estimators=100, max_depth=6, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8,
    tree_method='hist', verbosity=0, n_jobs=-1
)
xgbr.fit(X_train_reg, y_train_reg)
```

Here is where both the complexity of the models and hyperparameters got a little crazy.

- `colsample_bytree=0.8` is similar to subsample in that this picks a random 80% subset of feature columns for my trees to train with
- `scale_pos_weight=scale_pos_weight` is XGBoost's version of `class_weight='balanced'` for my imbalanced class distribution
- `tree_method='hist'` speeds up training by binning my numerical values into histograms before splitting, allowing the model to look at far less splits than looking at every value

### 5.1 Results

**Test PR-AUC: 0.8121**
**Test RMSE: 31.97 tickets**
**Test MAE: 10.42 tickets**

XGBoost had the highest PR-AUC value of all trees, proving that it could catch 91% of nonzero-sale events. It's regressor was on par with Gradient Boosting, yet the model trained in only 6 seconds, which is a real dealbreaker with more and more data.

---

## 6. Model 4: LightGBM

Microsoft's **LightGBM** takes a different approach to tree building. While XGBoost grows trees **level-by-level** (breadth-first), LightGBM grows **leaf-by-leaf** (depth-first), splitting the leaf that reduces loss the most.

<img width="756" height="287" alt="image" src="https://github.com/user-attachments/assets/1226b7d5-bc1f-4d95-b7f5-788045fe4ca4" />

*Figure 4: Level-wise (XGBoost) vs Leaf-wise (LightGBM) tree growth*

Here's how I build the Light GBM:

```python
lgbmc = LGBMClassifier(
    n_estimators=100, max_depth=-1, num_leaves=31,
    learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,  # Handle class imbalance
    verbosity=-1, n_jobs=-1
)
lgbmc.fit(X_train_cls, y_train_cls)

lgbmr = LGBMRegressor(
    n_estimators=100, max_depth=-1, num_leaves=31,
    learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,
    verbosity=-1, n_jobs=-1
)
lgbmr.fit(X_train_reg, y_train_reg)
```

There's only one hyperparameter to comment on this time, and that's `num_leaves`, which gives me the discretion to cap this branch growth at a maximum of 31 end leafs/nodes. Light GBMs don't care about imbalanced trees, so it can follow specific features down to a really niche split, which is nice for exploration.

### 6.1 Results

**Test PR-AUC: 0.8117**
**Test RMSE: 31.90 tickets**
**Test MAE: 10.26 tickets**

**LightGBM is not only the fastest tree-based model** but also the most accurate I found in this tree-based search, with the lowest RMSE value in tickets. Regardless of scale, LGBM was able to most precisely predict those nonzero-sales.

### 5.2 Speed vs Accuracy Trade-Off

This was my first major insight: For this problem, **LightGBM gives you 99% of XGBoost's performance in 35% of the time**.

In production, where you might retrain weekly on millions of rows, this speed matters. You get nearly the same predictions with dramatically faster iteration cycles.

When would you choose XGBoost over LightGBM?
- Small datasets (where speed doesn't matter)
- When you need every 0.1% of RMSE improvement
- When you want more conservative regularization (LightGBM can overfit faster)

For this project? LightGBM was looking like a winner.

---

## 6. Model 5: CatBoost - The Dark Horse

Yandex's **CatBoost** ("Categorical Boosting") brings two innovations:
1. **Ordered Boosting:** Prevents overfitting by using different data permutations
2. **Native Categorical Handling:** No need to one-hot encode categoricals

Wait - but I already one-hot encoded my categorical features in Part 3! So CatBoost's second advantage didn't apply here. Would it still compete?
```python
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.3,
    random_state=42,
    verbose=0
)
cat_model.fit(X_train, y_train)
```

### 6.1 Results

**Test RMSE: 0.7001 log units (~32 tickets MAE, ~10.2 tickets MAE)**

**CatBoost performed strongly** - slightly better than XGBoost and LightGBM. CatBoost's ordered boosting made it more resistant to overfitting.

**Training time:** 12 seconds (with GPU acceleration available)

### 6.2 The Surprising Finding

I expected XGBoost to dominate based on its Kaggle reputation. Instead, I got:
1. CatBoost: 0.7001
2. LightGBM: 0.7106
3. XGBoost: 0.7106

All three within **2% of each other**.

This told me something important: **All three algorithms were finding similar patterns in the data**. The problem wasn't that one algorithm was better - it was that they were all approximating the same underlying function, just with slightly different biases.

This would become critical in Part 7 when I built ensembles.

---

## 7. Head-to-Head Comparison

Let me put all five models side-by-side:

| Model | RMSE (log) | MAE (tickets) | R² | Train Time | Key Strength |
|-------|------------|---------------|----|-----------:|--------------|
| **RandomForest** | **0.6957** | **~10.0** | **0.6668** | 4m | Robustness |
| CatBoost | 0.7001 | ~10.2 | 0.6626 | 12s | Speed + accuracy |
| GradientBoosting | 0.7015 | ~10.3 | 0.6612 | 11m | Interpretability |
| XGBoost | 0.7106 | ~10.4 | 0.6524 | 6s | Regularization control |
| LightGBM | 0.7106 | ~10.2 | 0.6523 | 4s | Fastest training |

*Figure 5: Performance comparison bar chart showing tree models clustering together, Ridge far behind*

<!-- IMAGE: ROC curves for all classifiers showing model comparison (03A cell 20) -->
<img width="1200" alt="ROC curves comparison" src="PLACEHOLDER_URL" />

*Figure 5b: ROC curves for classification models - all tree models cluster near 0.92 AUC*

### 7.1 The Winner

**RandomForest** won the foundation model showdown for regression, with CatBoost close behind. The top five tree models were separated by only ~0.4 tickets MAE.

For classification (predicting IF sales occur), **XGBoost led with AUC 0.919**, followed closely by LightGBM (0.919), GradientBoosting (0.917), CatBoost (0.917), and RandomForest (0.916).

### 7.2 Performance vs Complexity

An R² of ~0.67 means tree models explain **about 67% of the variance** in log-transformed sales. In business terms, predictions are typically within **±10 tickets** of actual sales on average. Given the inherent noise in secondary ticket markets (72% zero-sales days), this is solid baseline performance.

But there's room for improvement. That remaining 33% of unexplained variance? That's where neural networks (Part 5), hyperparameter tuning (Part 6), and ensembles (Part 7) come in.

---

## 8. Feature Importance: What Drives Ticket Sales?

CatBoost revealed which of my 29 engineered features actually mattered:

### 8.1 Top 10 Features

| Rank | Feature | Importance | Insight |
|------|---------|------------|---------|
| 1 | days_to_event_scaled | 0.187 | **Time is everything** |
| 2 | sales_total_7d_log_scaled | 0.156 | Past predicts future |
| 3 | get_in_log_scaled | 0.121 | Price drives demand |
| 4 | listings_active_log_scaled | 0.094 | Supply signals quality |
| 5 | is_weekend | 0.073 | Weekend spike confirmed |
| 6 | venue_capacity_log_scaled | 0.051 | Venue size matters |
| 7 | listings_median_log_scaled | 0.042 | Market positioning |
| 8 | dte_X_bucket_Major_Sports | 0.038 | Interactions working! |
| 9 | inv_per_day_scaled | 0.033 | Burn rate matters |
| 10 | dow_sin | 0.028 | Cyclical encoding helps |

*Figure 6: Feature importance bar chart showing steep drop-off after top 5*

<!-- IMAGE: Feature importance comparison Classification vs Regression (03A cell 37) -->
<img width="1200" alt="Feature importance comparison" src="PLACEHOLDER_URL" />

*Figure 6b: Classification vs regression feature importance - temporal features dominate both*

### 8.2 What This Tells Us

**Temporal features dominate.** Days-to-event alone explains 18.7% of predictions. Add `is_weekend` and cyclical encoding (`dow_sin`, `dow_cos`), and temporal features account for nearly **30% of the model**.

This validates the EDA finding from Part 2: **When you buy matters more than what you buy.**

**Historical sales are powerful.** The `sales_total_7d_log_scaled` feature (past week's sales) is the #2 predictor. Events with momentum keep selling.

**Interaction terms are working.** The `dte_X_bucket_Major_Sports` interaction I engineered in Part 3 cracked the top 10. This feature captures how Sports buyers behave differently as events approach - exactly the segment-specific pattern I suspected.

**Price matters, but not as much as I expected.** Price features (`get_in`, `listings_median`, `price_spread_ratio`) combined explain about 20% of predictions. Important, but secondary to timing.

This feature importance analysis will guide my hyperparameter tuning in Part 6 and validate the segment-specific analysis in Part 8.

---

## 9. Error Analysis Preview

Even the best model (RandomForest, ~10 tickets MAE) makes mistakes. Where does it struggle?

### 9.1 Performance by Focus Bucket

| Bucket | MAE (tickets) | Count | Difficulty |
|--------|---------------|-------|-----------|
| Major Sports | ~8.4 | 4,094 | Easiest |
| Minor/Other Sports | ~9.3 | 4,048 | Easy |
| Broadway & Theater | ~10.4 | 4,869 | Medium |
| Other | ~10.8 | 2,784 | Medium |
| Concert | ~11.2 | 4,696 | Hard |
| Comedy | ~12.6 | 1,619 | Hardest |

*Figure 7: MAE by focus bucket showing variation between easiest and hardest*

**Major Sports** is easiest to predict (~8.4 tickets MAE) - likely due to regular schedules and predictable fan behavior.

**Comedy** is hardest (~12.6 tickets MAE) - smaller venues and more variable attendance patterns make predictions challenging.

This ~4 ticket MAE gap between best and worst buckets hints at something important: **Maybe different segments need different models?**

This question will drive Part 8's segment-specific analysis.

### 9.2 Residual Patterns

Plotting residuals (actual - predicted) against predictions revealed **heteroskedasticity** - fancy word for "error variance increases with prediction magnitude."

*Figure 8: Residual plot showing wider spread at higher predictions*

For low-volume events (predicted <100 tickets), errors are tight (±10 tickets). For high-volume events (predicted >300 tickets), errors spread out (±50 tickets).

This pattern suggests **ensemble methods might help**. Different models make different types of errors. Averaging them could reduce variance.

Foreshadowing Part 7...

---

## 10. Lessons Learned

### What Worked

**Tree-based models are the right tool for tabular data.** The 42% improvement over linear regression wasn't marginal - it was decisive.

**Default hyperparameters are surprisingly good.** CatBoost won with out-of-the-box settings. This validates the algorithms' design - they're well-tuned for typical use cases.

**Feature engineering paid off.** Those interaction terms I labored over in Part 3? They're in the top 10 features. The cyclical encoding for day-of-week? Making a difference.

**The top 3 tree models are nearly tied.** LightGBM, XGBoost, CatBoost all within 2% RMSE. They're all finding the same patterns, just with different approaches.

### What Surprised Me

**LightGBM's speed advantage was larger than expected.** 2.8x faster than XGBoost with 99% of the performance is a game-changer for production iteration cycles.

**CatBoost won without leveraging categorical features.** I expected XGBoost to dominate, but CatBoost's ordered boosting gave it a slight edge even on one-hot encoded data.

**Linear models failed harder than I thought.** 42% worse RMSE suggests the relationships in ticket sales are deeply non-linear. Makes sense - buying behavior doesn't follow straight lines.

### What's Next

I have a solid baseline (~10 tickets MAE), but three questions remain:

1. **Could neural networks beat trees?** Part 5 will test this.
2. **Can hyperparameter tuning improve below 10 tickets MAE?** Part 6 will optimize.
3. **Will combining models beat individual models?** Part 7 will ensemble.

---

## What's Coming: Neural Networks

Tree models dominated with ~10 tickets MAE, explaining ~67% of variance. That's impressive for baseline models.

But what about **neural networks?** Deep learning has revolutionized image classification, natural language processing, and game-playing AI. Could it find non-linear patterns that trees miss?

In **Part 5: Can Neural Networks Beat Gradient Boosting?**, I'll test three architectures:
- Naive feedforward network (2 layers)
- Deep network with heavy regularization (5 layers)
- ResNet-style network with skip connections

Spoiler: The results will surprise me. Trees have home-field advantage on tabular data, and they won't give it up easily...

But the lessons I learn from neural networks - about regularization, overfitting, and architecture design - will prove valuable when I build stacked ensembles in Part 7.

---

*Code and model artifacts available on [GitHub](#). Part 5 coming soon: The trees vs neural networks showdown.*
