---
layout: single
title: "SeatData.io Part 5: Neural Networks"
date: 2026-01-15
description: "Testing neural network architectures in a two-stage classification and regression pipeline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - neural networks
  - deep learning
  - keras
  - tensorflow
excerpt: "Applying deep learning to ticket sales prediction to find for accuracy improvements while tuning up complexity."
published: false
---

## Abstract

Tree-based models dominated Part 4, with the best two-stage pipeline (GradientBoosting + LightGBM) achieving **RMSE 19.15 tickets and MAE 3.89 tickets**. 

But I wanted to test if neural networks could match or beat these results. Using the same two-stage approach; first classifying whether sales will occur, then predicting volume, I tested four more architectures: Naive Bayes, MLP, Skip-Connection Networks, and LSTM. 

The results surprised me: **neural networks nearly matched tree performance**, with the best combination achieving an **MAE of 3.65 tickets**, 3% better than trees, despite taking 100x longer to train.

## Key Insights

- **Neural Networks Competitive on Two-Stage Task** - Best NN pipeline achieved MAE 3.65 tickets vs trees at 3.89 tickets
- **MLP Dominates Among NN Architectures** - Simple feedforward network outperformed skip connections and LSTM; sequential is better
- **Classification Performance Strong** - All NNs achieved 0.79+ PR-AUC, with MLP reaching 0.8059
- **Training Time Trade-Off** - NNs took 20+ minutes vs 3.5 seconds for LightGBM with only marginal improvement

---

## 1. The Two-Stage Approach

In Part 4, I discovered that the best prediction strategy wasn't a single model... it was a **two-stage pipeline**:

### 1.1 How Did I End Up at Two Stages?

My data has a big class imbalance, where **71.7% of events have zero sales in the final two weeks**. This creates two distinct challenges:

**Challenge 1:** Predicting IF any tickets will sell (classification)
**Challenge 2:** Predicting HOW MANY tickets will sell (regression)

A single regression model struggles because it must:
- Predict exactly 0 for the 71.7% of zero-sale events
- Predict continuous values (1, 5, 10, 50 tickets) for the 28.3% with sales
- Balance the loss function between these competing objectives

The two-stage approach separates these concerns:

```
Stage 1 (Classifier): Will this event have ANY sales in the final two weeks?
- NO results in 0 ticket-prediction
- YES allows Stage 2 continuation
Stage 2 (Regressor): How many tickets will sell?
```

This improved Part 4's best single-stage regressor (LightGBM: 31.90 tickets RMSE) to a two-stage pipeline that achieved **19.15 tickets RMSE** on all test data.

### 1.2 Neural Networks

Neural networks function very similar to neurons in our brain, adding weights and biases between each connection. Yet, these complex models are built to learn the non-linear patterns in data by applying activation functions, such as ReLU or Tanh, which I'll talk about later.

Neural Nets are similar to trees in that they are very flexible. A single network can simultaneously:
- Output a probability (sigmoid activation) for classification
- Output a continuous value (linear activation) for regression

But I took a different approach. I wanted to **train separate networks for each stage**, allowing each to optimize for its specific task. This also let me test combining tree classifiers with NN regressors to find the optimized pipeline.

---

## 2. Data Preparation for Neural Networks

Before training, I needed to prepare data differently than I did for trees in Part 4.

### 2.1 Scaling

Trees don't care about feature scales. XGBoost happily splits on `days_to_event` (range 0-338) and `dow_sin` (range -1 to 1) without complaint.

Neural networks **care.**

Gradient descent is the training algorithm for NNs. This method updates weights proportionally to feature magnitudes. If one feature ranges from 0-1000 and another from 0-1, the optimizer will:
- Take huge steps for the 0-1000 feature
- Take tiny steps for the 0-1 feature
- Struggle to converge

The solution is found in scaling these feature spaces to force Gradient Descent to take equal steps. For this, I used **StandardScaler**

```python
from sklearn.preprocessing import StandardScaler

scaler_cls = StandardScaler()
X_train_cls_scaled = scaler_cls.fit_transform(X_train_cls)
X_test_cls_scaled = scaler_cls.transform(X_test_cls)

scaler_reg = StandardScaler()
X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
X_test_reg_scaled = scaler_reg.transform(X_test_reg)
```

This transforms every feature to:
- Mean zero (center the data)
- Standard deviation of 1 (same scale)

Importantly, I fit the scaler on **training data only**, then applied the same transformation to test data. This prevents data leakage because I'm preventing my test data from seeing the distribution of features. 

### 2.2 Time-Based Validation Split

For early stopping, I needed a validation set to use within training these Neural Nets. I used **the last 3 days of training data** (just before the test period):

- Training: Oct 1 - Dec 28
- Validation: Dec 29-31 (for early stopping)
- Test: Jan 1-12 (final evaluation)

This mimics a final production, where model-users would want to predict the future using only the past.

```python
VAL_START = train_df['snapshot_date'].max() - pd.Timedelta(days=3)
val_mask = train_df['snapshot_date'] >= VAL_START

X_train_nn = X_train_cls_scaled[~val_mask]
X_val_nn = X_train_cls_scaled[val_mask]
```

### 2.3 Class Imbalance Handling

With 71.7% zero-sales events, classifiers would achieve 71.7% accuracy by always predicting "no sales." I addressed this with **class weights**:

```python
neg_count = (y_train_cls == 0).sum()  # Zero-sale events
pos_count = (y_train_cls == 1).sum()  # Nonzero-sale events
scale_pos_weight = neg_count / pos_count  # ~2.53

class_weight_dict = {0: 1.0, 1: scale_pos_weight}
```

This outright tells the network: "Misclassifying a positive sample costs 2.53× more than misclassifying a negative sample," defining my data's cost matrix. The network learns to find that 28.3% of sales events.

---

## 3. Architecture 1: Naive Bayes

Before diving into neural networks, I established a simple baseline.

### 3.1 Why Naive Bayes?

Naive Bayes makes a strong assumption: **features are independent given the class**. For ticket sales, this means:

```
P(sales | price, days_to_event, venue_capacity, etc..) =
    P(sales | price) × P(sales | days_to_event) × P(sales | venue_capacity) x ...
```

This is obviously wrong (price and days_to_event interact, as I saw in my EDA), but Naive Bayes is fast and interpretable.

```python
from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train_cls_scaled, y_train_cls)
```

### 3.2 Results (Classification Only)

- **AUC-ROC: 0.8943**
- **PR-AUC: 0.7624**
- **Precision: 0.7167** (when it predicts sales, it's right 72% of the time)
- **Recall: 0.7577** (finds 76% of all events with sales)
- **Training Time: 2.2 seconds**

For a model that assumes independence and trains in 2 seconds, this is impressive. It correctly identifies 3 out of 4 events that will have sales.

It's good to note that Naive Bayes can only do classification, since it tries to find the probability of a class or category. So here, I chose not to implement what could be a Bayesian Regression, or simply predicting yesterday's sales will be the same as today's.

---

## 4. Architecture 2: MLP Neural Network

Next, I wanted to finally jump into deep learning with a Multi-Layer Perceptron (MLP).

### 4.1 MLP Classifier Architecture

```python
def build_mlp_classifier(input_dim, layers_config=[256, 128, 64], dropout=0.3):
    inputs = Input(shape=(input_dim,))
    x = BatchNormalization()(inputs)

    for units in layers_config:
        x = Dense(units, kernel_regularizer=regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Dropout(dropout)(x)

    outputs = Dense(1, activation='sigmoid')(x)  # Probability output
    return Model(inputs, outputs)
```

This creates a 4-layer network:
- **Layer 1:** 256 input neurons
- **Layer 2:** 128 hidden neurons
- **Layer 3:** 64 hidden neurons
- **Output:** 1 output neuron with sigmoid activation (probability of sales)

Each layer uses:
- **L2 Regularization:** Ridge penalty on large weights (0.001 strength)
- **Batch Normalization:** Stabilizes training by normalizing layer inputs
- **Dropout (30%):** Randomly drops neurons during training to prevent overfitting
- **ReLU Activation:** `f(x) = max(0, x)` to introduce non-linearity

### 4.2 MLP Regressor Architecture

The regression network is pretty identical to the classifier except that:
- Trained only on the 28.3% of events with sales
- Output layer uses **linear activation** 
- Loss function is **MSE** instead of binary cross-entropy

```python
def build_mlp_regressor(input_dim, layers_config=[256, 128, 64], dropout=0.3):
    # ... same architecture ...
    outputs = Dense(1, activation='linear')(x)  # Continuous output
    return Model(inputs, outputs)
```

### 4.3 Training

After defining these functions, I need to put some guardrails around building these models, such as defining stopping points when the model begins overfitting the data.

```python
early_stop = callbacks.EarlyStopping(
    monitor='val_loss', patience=10, restore_best_weights=True
)
reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
)

mlp_cls.fit(
    X_train_nn, y_train_nn_cls,
    validation_data=(X_val_nn, y_val_nn_cls),
    epochs=100, batch_size=1024,
    callbacks=[early_stop, reduce_lr],
    class_weight=class_weight_dict  # Handle imbalance
)
```

**Early Stopping:** If validation loss doesn't improve for 10 epochs, stop training and restore the best weights.

**Learning Rate Reduction:** If validation loss plateaus for 5 epochs, cut the learning rate in half. This helps the optimizer escape local minima.

### 4.4 Results

**Classification:**
- **AUC-ROC: 0.9140** (best among all classifiers)
- **PR-AUC: 0.8059** (handles class imbalance well)
- **Precision: 0.6213** (when it predicts sales, it's right 64% of the time)
- **Recall: 0.8973** (finds 88% of all events with sales)
- **F1-Score: 0.7342**
- **Training Time: 20 minutes**

**Regression (on events with sales only):**
- **RMSE (tickets): 38.5**
- **MAE (tickets): 9.77**
- **R²: 0.6860** (explains 68.6% of variance)
- **Training Time: 4.5 minutes**

The MLP classifier dominated, achieving the highest AUC-ROC (0.9140) and finding 89.73% of sales events. The high recall came at a cost though, as precision dropped to 62%. But for this problem, I'd rather overpredict sales than miss opportunities.

The regressor performed worse than Part 4's LightGBM (31.90 tickets RMSE), achieving **38.5 tickets RMSE (about 21% worse)**. 

---

## 5. Architecture 3: Skip-Connection Neural Network

Inspired by ResNet (Residual Networks), I tested whether skip connections could improve performance.

### 5.1 Vanishing Gradients

In deep networks, gradients (signals for weight updates) get smaller as they propagate backward through layers. By layer 5, gradients might be 0.0001, essentially too small to update weights effectively.

This is the **vanishing gradient problem**, limiting how deep you can go.

### 5.2 Skip Connections

ResNet solved this vanishing gradient by adding **skip connections** for gradients:

```python
def build_skip_classifier(input_dim, hidden_units=128, num_blocks=3):
    inputs = Input(shape=(input_dim,))
    x = Dense(hidden_units)(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    for _ in range(num_blocks):
        residual = x  # Save the input to this block

        # Process through two layers
        x = Dense(hidden_units, kernel_regularizer=regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Dropout(0.3)(x)

        x = Dense(hidden_units)(x)
        x = BatchNormalization()(x)

        x = Add()([x, residual])  # Skip connection
        x = Activation('relu')(x)

    outputs = Dense(1, activation='sigmoid')(x)
    return Model(inputs, outputs)
```

The `Add()` layer creates a shortcut: "Here's the processed signal AND the original signal from before this block." Gradients can now flow backward through these shortcuts, enabling deeper networks.

### 5.3 Results

**Classification:**
- **AUC-ROC: 0.9129** 
- **PR-AUC: 0.8013** 
- **Precision: 0.6150**
- **Recall: 0.8972**
- **F1-Score: 0.7297**
- **Training Time: 11 minutes**

**Regression:**
- **RMSE (tickets): 32.96**
- **MAE (tickets): 10.5**
- **R²: 0.6431**
- **Training Time: 11.5 minutes**

Skip connections did seem to help. Though the classifier performed essentially the same as MLP, the regressor was **better** (38.5 vs 32.96 tickets RMSE). But R² dropped, suggesting worse fit.

The architecture added complexity and marginal performance. While exploring these skip connections, I found that they are most helpful in very deep networks when trying to preserve signal, going beyond 50 layers, for example. At 3 residual blocks, they're probably overkill.

---

## 6. Architecture 4: LSTM

LSTM (Long Short-Term Memory) networks excel at sequences. But my features aren't sequential, they're tabular (price, days_to_event, venue_capacity).

### 6.1 Why Test LSTM?

I wanted to see if LSTM could find **implicit sequential patterns** in feature ordering. Maybe the network would learn: "If `days_to_event` is small AND `get_in` is high, weight `listings_active` differently," almost similar to the rule-based learning I tried previously.

```python
def build_lstm_classifier(input_dim, lstm_units=64):
    inputs = Input(shape=(input_dim,))
    x = Reshape((input_dim, 1))(inputs)  # Treat features as sequence
    x = LSTM(lstm_units, return_sequences=False, dropout=0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    return Model(inputs, outputs)
```

The `Reshape` layer treats 30 features as a sequence of length 30, each with 1 value. The LSTM processes this "sequence" left-to-right.

### 6.2 Results

**Classification:**
- **AUC-ROC: 0.9024**
- **PR-AUC: 0.7851**
- **Precision: 0.6124**
- **Recall: 0.8885**
- **F1-Score: 0.7251**
- **Training Time: 43 minutes**

**Regression:**
- **RMSE (tickets): 42**
- **MAE (tickets): 11.68**
- **R²: 0.5524**
- **Training Time: 14 minutes**

LSTM struggled. The classifier was the worst NN. The regressor was also the worst I had seen beyond Linear Regression: **42 tickets RMSE**, 9% worse than MLP's 38.5.

Why? **Tabular data has no sequential structure.** The order of features is arbitrary. LSTM wasted capacity learning left-to-right dependencies that don't exist.

This confirmed that **LSTMs are the wrong tool for tabular data.**

---

## 7. Neural Network Comparison

Let me compare all four architectures side-by-side.

### 7.1 Classification Leaderboard

| Model          | AUC-ROC | PR-AUC | Precision | Recall | F1-Score | Time (m) |
|----------------|---------|--------|-----------|--------|----------|----------|
| MLP            | 0.91    | 0.81   | 0.62      | 0.90   | 0.73     | 20       |
| SkipConnection | 0.91    | 0.80   | 0.61      | 0.90   | 0.73     | 11       |
| LSTM           | 0.90    | 0.79   | 0.61      | 0.89   | 0.73     | 43       |
| NaiveBayes     | 0.89    | 0.76   | 0.72      | 0.76   | 0.74     | 0        |

**Winner: MLP** (best PR-ROC)

The simple MLP beat the sophisticated architectures. Skip connections added no value while LSTM took 38 minutes for worse performance.

Naive Bayes deserves respect: 2 seconds for 0.89 AUC-ROC and 0.76 PR-AUC is very good. Its precision (0.72) was highest, making it useful when false positives are costly.

### 7.2 Regression Leaderboard

| Model | RMSE (tickets) | MAE (tickets) | R² | Time (s) |
|-------|----------------|---------------|----|----------|
| **SkipConnection** | **33.89** | 10.23 | 0.6505 | 526.6 |
| **MLP** | 35.86 | **9.84** | **0.6847** | 156.0 |
| LSTM | 42.55 | 12.03 | 0.5350 | 724.0 |

**Winner: MLP** (best MAE and R², fast training)

Skip connections achieved slightly better RMSE (33.89 vs 35.86) but worse R² (0.6505 vs 0.6847). RMSE can be skewed by a few large errors, while R² measures overall fit. I trust R² more here.

MLP wins on speed (156s vs 527s) and explains 68.5% of variance—impressive for a neural network on tabular data.

---

## 8. Two-Stage Combined Performance

The real test: **combining classifiers with regressors** on the full test set (including zero-sales events).

### 8.1 Combining Neural Networks

For each classifier-regressor pair:
1. Classifier predicts which events will have sales
2. For predicted positive events, regressor predicts volume
3. All other events get 0 tickets

```python
for cls_name in ['NaiveBayes', 'MLP', 'SkipConnection', 'LSTM']:
    for reg_name in ['MLP', 'SkipConnection', 'LSTM']:
        cls_pred = classification_predictions[cls_name]
        reg_model = regression_results[reg_name]['model']

        y_pred_combined = np.zeros(len(X_test))
        pos_mask = cls_pred == 1

        if pos_mask.sum() > 0:
            X_pos_scaled = reg_scaler.transform(X_test[pos_mask])
            y_pred_combined[pos_mask] = reg_model.predict(X_pos_scaled)

        # Evaluate on full test set
        rmse_log = np.sqrt(mean_squared_error(y_test_full_log, y_pred_combined))
```

### 8.2 Results: Neural Network Combinations

| Classifier | Regressor | RMSE (log) | MAE (tickets) | RMSE (tickets) |
|------------|-----------|------------|---------------|----------------|
| **NaiveBayes** | **MLP** | **0.7025** | **3.65** | **20.47** |
| LSTM | MLP | 0.7146 | 3.71 | 20.47 |
| NaiveBayes | SkipConnection | 0.7246 | 3.85 | 19.89 |
| NaiveBayes | LSTM | 0.7201 | 4.02 | 23.18 |
| LSTM | SkipConnection | 0.7394 | 3.92 | 19.89 |
| MLP | MLP | 0.7401 | 3.86 | 22.53 |
| SkipConnection | MLP | 0.7442 | 3.87 | 22.17 |
| LSTM | LSTM | 0.7529 | 4.12 | 23.18 |
| MLP | SkipConnection | 0.7657 | 4.12 | 22.60 |
| SkipConnection | SkipConnection | 0.7701 | 4.15 | 22.32 |

**Winner: Naive Bayes classifier + MLP regressor**
- **RMSE (log): 0.7025**
- **MAE: 3.65 tickets**
- **RMSE: 20.47 tickets**

The surprise winner is the simplest combination: fast Naive Bayes classification paired with MLP regression. Why?

**Naive Bayes has the highest precision (0.7165)**. When it predicts sales, it's right 72% of the time. This reduces false positives, meaning the regressor isn't wasting effort on events unlikely to sell.

MLP classifiers have higher recall (88%) but lower precision (64%). They send more events to the regressor, including many false positives. The regressor then predicts positive sales for zero-sale events, inflating the error.

**The lesson:** For two-stage models, **precision matters more than recall** in the classifier. Better to miss some sales events than waste the regressor on false positives.

---

## 9. Trees vs Neural Networks: The Showdown

Now for the moment of truth. How do neural networks compare to Part 4's tree models?

### 9.1 Regression-Only Comparison (Events with Sales)

| Model Type | Best Model | RMSE (tickets) | MAE (tickets) | R² |
|------------|------------|----------------|---------------|----|
| **Trees** | **LightGBM** | **31.90** | **10.26** | **0.6519** |
| Neural Networks | MLP | 35.86 | 9.84 | 0.6847 |

Trees win on RMSE (31.90 vs 35.86 = **12% better**). But neural networks win on MAE (9.84 vs 10.26) and R² (0.6847 vs 0.6519).

Wait—how can NNs have worse RMSE but better MAE and R²? **RMSE penalizes large errors more.** NNs might make a few catastrophic predictions (predicting 100 tickets when actual is 10), inflating RMSE, while making better average predictions (lower MAE).

R² is higher for NNs (0.6847 vs 0.6519), suggesting they explain variance better despite larger outlier errors.

**Verdict:** Trees edge out NNs on regression, but **it's closer than expected**.

### 9.2 Two-Stage Comparison (All Test Data)

| Model Type | Best Combination | RMSE (log) | MAE (tickets) | RMSE (tickets) |
|------------|------------------|------------|---------------|----------------|
| **Trees** | **GB + LightGBM** | **0.7160** | **3.89** | **19.15** |
| Neural Networks | NB + MLP | 0.7025 | **3.65** | 20.47 |

Now it gets interesting. Neural networks achieve **better RMSE_log (0.7025 vs 0.7160)** and **better MAE (3.65 vs 3.89 tickets)**.

But wait—I need to check if Part 4 used the same best two-stage model. Let me look...

Actually, from the notebook outputs, Part 4's best two-stage was **GradientBoosting classifier + RandomForest regressor** with RMSE_log 0.7013 and MAE 3.77 tickets.

Let me recalculate:

| Model Type | Best Combination | RMSE (log) | MAE (tickets) |
|------------|------------------|------------|---------------|
| **Trees** | **GB + RF** | **0.7013** | **3.77** |
| Neural Networks | NB + MLP | 0.7025 | 3.65 |

Trees have slightly better RMSE_log (0.7013 vs 0.7025 = **0.2% better**). Neural networks have better MAE (3.65 vs 3.77 = **3% better**).

**Verdict:** It's a tie. Neural networks and trees achieve essentially the same two-stage performance.

### 9.3 Training Time Comparison

| Model Type | Best Model | Training Time |
|------------|------------|---------------|
| Trees (LightGBM) | Classifier + Regressor | 3.5 seconds |
| Neural Networks (MLP) | Classifier + Regressor | 655.8 seconds (~11 minutes) |

Trees train **187× faster** than neural networks.

For production systems retraining nightly on millions of events, this matters. Neural networks would need GPU acceleration to be practical.

---

## 10. Why Neural Networks Nearly Matched Trees

Three factors enabled NNs to compete:

### 10.1 Proper Regularization

Without dropout, batch normalization, and L2 penalties, neural networks overfit catastrophically. These techniques were essential.

### 10.2 The Two-Stage Approach

Separating classification from regression let each network specialize. The classifier focused on finding the 28.3% of sales events. The regressor focused on predicting volume for a more homogeneous subset.

A single network trying to do both would struggle.

### 10.3 Feature Engineering

The 29 features I engineered in Part 3 were designed for tree models. But they also work for neural networks:
- Log-scaled features reduce skew
- Cyclical encoding (sin/cos) for day-of-week captures periodicity
- Interaction terms (e.g., `days_to_event × get_in`) provide non-linear combinations

Good features make any model better—even the "wrong" model.

---

## 11. When Would Neural Networks Win?

Neural networks nearly tied trees despite being the "wrong tool" for tabular data. When would they win?

### 11.1 Unstructured Features

If I had:
- **Event descriptions** (free text): "Taylor Swift Eras Tour - Opening Night"
- **Venue images** (seating chart photos)
- **Artist bios** (Wikipedia text)

Then neural networks would dominate. CNNs for images, Transformers for text. Trees can't process unstructured data.

### 11.2 Massive Datasets

At 10M+ training samples, neural network capacity becomes an advantage. Trees plateau—you can't add 10,000 trees without overfitting.

Neural networks scale by:
- Adding layers
- Adding neurons per layer
- Training longer

### 11.3 Transfer Learning

Pre-trained models (BERT, ResNet) can be fine-tuned on small datasets. If I had event descriptions, I could:
1. Use a pre-trained BERT model (trained on billions of text examples)
2. Fine-tune on my 5M events
3. Leverage linguistic knowledge learned from Wikipedia, books, etc.

Trees have no transfer learning equivalent. Every dataset starts from scratch.

---

## 12. Lessons Learned

### What Worked

**Naive Bayes + MLP was the best combination.** Simple classifier with high precision, paired with powerful regressor.

**MLP dominated NN architectures.** Skip connections and LSTM added complexity without adding performance.

**Regularization transformed NNs.** Dropout, batch normalization, and L2 penalties were non-negotiable.

**Neural networks nearly matched trees** on two-stage performance (MAE 3.65 vs 3.77 tickets).

### What Surprised Me

**Naive Bayes beat NN classifiers in two-stage.** Its high precision (72%) reduced false positives, helping the regressor.

**LSTM failed completely.** 38 minutes of training for the worst performance. Tabular data has no sequential structure.

**R² told a different story than RMSE.** NNs had better R² (0.6847 vs 0.6519) but worse RMSE (35.86 vs 31.90). A few outlier predictions skewed RMSE.

### The Hard Truth

**For tabular data with <10M rows, gradient boosting wins on speed.** Trees train 187× faster with comparable accuracy.

But neural networks aren't far behind. With GPUs and proper regularization, they can compete. For production systems with unstructured data (text, images), they're the only option.

---

## 13. What's Next

I've tested:
- Linear models (Part 4): MAE 13.31 tickets ❌
- Tree models (Part 4): MAE 3.89 tickets ✓
- Neural networks (Part 5): MAE 3.65 tickets ✓

Both trees and NNs used **default or lightly-tuned hyperparameters**. Could systematic optimization squeeze out better performance?

In **Part 6: Hyperparameter Tuning**, I'll:
- Use RandomizedSearchCV to optimize XGBoost, LightGBM, CatBoost
- Test 50+ hyperparameter combinations per model
- Find the optimal learning rate, depth, regularization
- See if I can push below MAE 3.5 tickets

Then in **Part 7: Ensemble Stacking**, I'll combine the best models. Neural networks and trees make different types of errors—averaging them might reduce variance and improve predictions.

The journey from 13.31 tickets MAE (linear baseline) to 3.65 tickets (current best) was dramatic. Can I reach 3.0 tickets or below?

---

*Code for all architectures available on [GitHub](#). Part 6 dropping soon: The hyperparameter optimization deep dive.*
