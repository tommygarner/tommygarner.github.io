---
layout: single
title: "SeatData.io Part 4: Foundation Models - Which Algorithm Wins?"
date: 2026-01-18
description: "Testing five algorithms head-to-head to find the best baseline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - gradient boosting
  - XGBoost
  - LightGBM
  - model comparison
excerpt: "From linear regression to gradient boosting: discovering which algorithms excel at predicting secondary market ticket sales."
---

## Abstract

With 29 engineered features ready from Part 3, I faced the classic machine learning question: **which algorithm should I use?** Rather than assuming, I ran a systematic comparison of five approaches - from simple linear regression to sophisticated gradient boosting. The results surprised me: tree-based models didn't just win, they dominated by over 40%. But the tight race between XGBoost, LightGBM, and CatBoost revealed something more interesting - they were all finding similar patterns, hinting that combining them might unlock even better performance.

## Key Insights

- **Tree-Based Models Crush Linear Baselines** - Gradient boosting achieved 40%+ better RMSE than Ridge regression
- **LightGBM and XGBoost Nearly Tied** - RMSE difference of only 0.003 despite architectural differences
- **Temporal Features Dominate** - Days-to-event and weekend indicators explain 45% of prediction power

---

## 1. The Algorithm Showdown

I had engineered features. I had cleaned data. Now came the moment of truth: **training my first models**.

But here's the thing about machine learning - there's no universal "best" algorithm. The **No Free Lunch theorem** states that no single algorithm dominates across all problems. What works brilliantly for image classification might fail miserably for ticket sales prediction.

So instead of guessing, I ran an **algorithm showdown**.

### 1.1 The Five Contenders

I selected five algorithms spanning the spectrum from simple to sophisticated:

**1. Ridge Regression**
- **Type:** Linear model with L2 regularization
- **Strength:** Interpretable, fast, establishes baseline
- **Weakness:** Assumes linear relationships (rarely true in real data)

**2. Gradient Boosting (sklearn)**
- **Type:** Sequential tree ensemble
- **Strength:** Handles non-linearity, proven on tabular data
- **Weakness:** Slower to train, can overfit

**3. XGBoost**
- **Type:** Optimized gradient boosting
- **Strength:** Built-in regularization, parallel processing, handles missing values
- **Weakness:** Many hyperparameters to tune

**4. LightGBM**
- **Type:** Leaf-wise gradient boosting
- **Strength:** Fast training, efficient memory use
- **Weakness:** Can overfit on small datasets

**5. CatBoost**
- **Type:** Gradient boosting with ordered boosting
- **Strength:** Handles categorical features natively, resistant to overfitting
- **Weakness:** Slower than LightGBM on large datasets

### 1.2 Evaluation Setup

**Metric:** Root Mean Squared Error (RMSE) on test set
- Lower is better
- Penalizes large errors more than MAE
- Interpretable in original units (log-sales)

**Test Set:** Jan 1-12, 2026 (20,391 events with sales)
- Completely unseen during training
- Represents realistic production scenario

**Naive Baseline:** Predict mean sales = RMSE of 0.505
- This is what we'd get by always predicting the average
- Any model must beat this to be useful

Let the games begin.

---

## 2. Model 1: Ridge Regression - The Linear Baseline

I started with the simplest approach: **Ridge Regression**. This is a linear model that finds the best-fit line (well, hyperplane in 29 dimensions) through the data.

### 2.1 Why Start Linear?

Linear models have three advantages:
1. **Fast:** Train in seconds even on millions of rows
2. **Interpretable:** Each coefficient shows feature impact
3. **Diagnostic:** If linear works well, you might not need complexity

I trained with L2 regularization (alpha=1.0) to prevent overfitting:
```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0, random_state=42)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
```

### 2.2 Results

**Test RMSE: 0.3594**

Better than the naive baseline (0.505), but not spectacular. The model was predicting log-sales with ±0.36 error, which translates to roughly ±40% error in actual tickets.

*Figure 1: Ridge regression actual vs predicted - linear relationship visible but lots of variance*

### 2.3 What Ridge Taught Me

Looking at the top coefficients revealed which features had the strongest **linear** relationships:

- `sales_total_7d_log_scaled`: +0.28 (past sales predict future - makes sense!)
- `days_to_event_scaled`: -0.22 (closer to event = more sales)
- `get_in_log_scaled`: -0.15 (lower prices = more sales)

But Ridge struggled with **non-linear patterns**. The relationship between days-to-event and sales isn't a straight line - sales accelerate as events approach, then drop off. Linear models can't capture this curve.

Time to bring in the trees.

---

## 3. Model 2: Gradient Boosting - Enter the Trees

**Gradient Boosting** works by building trees sequentially, where each new tree corrects the errors of the previous trees. Think of it like a team correcting each other's homework - the first person tries, the second fixes mistakes, the third fixes what the second missed, and so on.

### 3.1 How Gradient Boosting Works

1. Start with a simple prediction (mean sales)
2. Build a small decision tree to predict the errors
3. Add this tree's predictions to the current model
4. Repeat 100 times (or however many estimators you choose)

Each tree is **shallow** (max_depth=3), learning simple patterns. But 100 shallow trees combined create sophisticated decision boundaries.
```python
from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)
gb.fit(X_train, y_train)
```

### 3.2 Results

**Test RMSE: 0.2257**

**Whoa.** That's a **37% improvement** over Ridge regression. Gradient Boosting cut prediction error by more than a third just by capturing non-linearity.

*Figure 2: Gradient Boosting predictions showing much tighter fit to actuals*

### 3.3 First Look at Feature Importance

Gradient Boosting gave me my first glimpse into which features actually mattered:

| Feature | Importance |
|---------|------------|
| days_to_event_scaled | 0.23 |
| sales_total_7d_log_scaled | 0.18 |
| get_in_log_scaled | 0.15 |
| listings_active_log_scaled | 0.12 |
| venue_capacity_log_scaled | 0.08 |

Temporal and historical features dominated. The **days_to_event × bucket** interaction terms I engineered in Part 3? They were showing up in the top 15.

Feature engineering was paying off.

---

## 4. Model 3: XGBoost - The Heavy Hitter

If Gradient Boosting is good, **XGBoost** is Gradient Boosting on steroids. Created for Kaggle competitions, it adds:
- Regularization (L1 and L2) to prevent overfitting
- Parallel processing for faster training
- Native handling of missing values
- Built-in cross-validation

I trained with default hyperparameters to establish the baseline:
```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.3,
    random_state=42,
    tree_method='hist'  # Faster training
)
xgb_model.fit(X_train, y_train)
```

### 4.1 Results

**Test RMSE: 0.2147**

Another improvement! XGBoost beat sklearn's Gradient Boosting by 5%. Not a massive jump, but every bit helps when predicting sales.

**Training time:** 2 minutes 14 seconds

*Figure 3: XGBoost actual vs predicted scatter plot - tightest clustering yet*

### 4.2 Why XGBoost Won

XGBoost's regularization helped prevent overfitting on complex patterns. While sklearn's GB learned to fit the training data perfectly (RMSE 0.08 on train), XGBoost maintained better generalization (train RMSE 0.18).

The gap between train and test RMSE narrowed - a sign of a model that generalizes well.

---

## 5. Model 4: LightGBM - Speed Demon

Microsoft's **LightGBM** takes a different approach to tree building. While XGBoost grows trees **level-by-level** (breadth-first), LightGBM grows **leaf-by-leaf** (depth-first), splitting the leaf that reduces loss the most.

*Figure 4: Level-wise (XGBoost) vs Leaf-wise (LightGBM) tree growth strategies*

This makes LightGBM **faster** - often 3-5x faster than XGBoost on large datasets.
```python
import lightgbm as lgb

lgb_model = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.3,
    random_state=42
)
lgb_model.fit(X_train, y_train)
```

### 5.1 Results

**Test RMSE: 0.2144**

**Wait, what?** LightGBM essentially **tied** XGBoost (difference of 0.0003 RMSE), but trained in **47 seconds** instead of 2 minutes 14 seconds.

That's **2.8x faster** with virtually identical performance.

### 5.2 Speed vs Accuracy Trade-Off

This was my first major insight: For this problem, **LightGBM gives you 99% of XGBoost's performance in 35% of the time**.

In production, where you might retrain weekly on millions of rows, this speed matters. You get nearly the same predictions with dramatically faster iteration cycles.

When would you choose XGBoost over LightGBM?
- Small datasets (where speed doesn't matter)
- When you need every 0.1% of RMSE improvement
- When you want more conservative regularization (LightGBM can overfit faster)

For this project? LightGBM was looking like a winner.

---

## 6. Model 5: CatBoost - The Dark Horse

Yandex's **CatBoost** ("Categorical Boosting") brings two innovations:
1. **Ordered Boosting:** Prevents overfitting by using different data permutations
2. **Native Categorical Handling:** No need to one-hot encode categoricals

Wait - but I already one-hot encoded my categorical features in Part 3! So CatBoost's second advantage didn't apply here. Would it still compete?
```python
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.3,
    random_state=42,
    verbose=0
)
cat_model.fit(X_train, y_train)
```

### 6.1 Results

**Test RMSE: 0.2104**

**CatBoost won.**

Not by much (2% better than LightGBM), but it won. Despite not leveraging its categorical feature advantage, CatBoost's ordered boosting made it slightly more resistant to overfitting.

**Training time:** 1 minute 52 seconds (slower than LightGBM, faster than XGBoost)

### 6.2 The Surprising Finding

I expected XGBoost to dominate based on its Kaggle reputation. Instead, I got:
1. CatBoost: 0.2104
2. LightGBM: 0.2144
3. XGBoost: 0.2147

All three within **2% of each other**.

This told me something important: **All three algorithms were finding similar patterns in the data**. The problem wasn't that one algorithm was better - it was that they were all approximating the same underlying function, just with slightly different biases.

This would become critical in Part 7 when I built ensembles.

---

## 7. Head-to-Head Comparison

Let me put all five models side-by-side:

| Model | RMSE | MAE | R² | Train Time | Key Strength |
|-------|------|-----|----|-----------:|--------------|
| **CatBoost** | **0.2104** | **0.1526** | **0.9683** | 1m 52s | Overfitting resistance |
| LightGBM | 0.2144 | 0.1552 | 0.9671 | 47s | Speed |
| XGBoost | 0.2147 | 0.1558 | 0.9670 | 2m 14s | Regularization control |
| Gradient Boosting | 0.2257 | 0.1621 | 0.9636 | 3m 41s | Interpretability |
| Ridge Regression | 0.3594 | 0.2598 | 0.8906 | 3s | Simplicity |

*Figure 5: Performance comparison bar chart showing tree models clustering together, Ridge far behind*

### 7.1 The Winner

**CatBoost** won the foundation model showdown, but barely. The top three tree models were separated by less than the width of a pencil mark.

What mattered more than which tree model won was that **all tree models crushed linear regression**. The gap from Ridge to CatBoost (0.36 to 0.21 RMSE) was **42% improvement**.

### 7.2 Performance vs Complexity

An R² of 0.9683 means CatBoost explains **96.83% of the variance** in log-transformed sales. That's impressive for a baseline model with default hyperparameters.

But there's room for improvement. That remaining 3.17% of unexplained variance? That's where hyperparameter tuning (Part 6) and ensembles (Part 7) come in.

---

## 8. Feature Importance: What Drives Ticket Sales?

CatBoost revealed which of my 29 engineered features actually mattered:

### 8.1 Top 10 Features

| Rank | Feature | Importance | Insight |
|------|---------|------------|---------|
| 1 | days_to_event_scaled | 0.187 | **Time is everything** |
| 2 | sales_total_7d_log_scaled | 0.156 | Past predicts future |
| 3 | get_in_log_scaled | 0.121 | Price drives demand |
| 4 | listings_active_log_scaled | 0.094 | Supply signals quality |
| 5 | is_weekend | 0.073 | Weekend spike confirmed |
| 6 | venue_capacity_log_scaled | 0.051 | Venue size matters |
| 7 | listings_median_log_scaled | 0.042 | Market positioning |
| 8 | dte_X_bucket_Major_Sports | 0.038 | Interactions working! |
| 9 | inv_per_day_scaled | 0.033 | Burn rate matters |
| 10 | dow_sin | 0.028 | Cyclical encoding helps |

*Figure 6: Feature importance bar chart showing steep drop-off after top 5*

### 8.2 What This Tells Us

**Temporal features dominate.** Days-to-event alone explains 18.7% of predictions. Add `is_weekend` and cyclical encoding (`dow_sin`, `dow_cos`), and temporal features account for nearly **30% of the model**.

This validates the EDA finding from Part 2: **When you buy matters more than what you buy.**

**Historical sales are powerful.** The `sales_total_7d_log_scaled` feature (past week's sales) is the #2 predictor. Events with momentum keep selling.

**Interaction terms are working.** The `dte_X_bucket_Major_Sports` interaction I engineered in Part 3 cracked the top 10. This feature captures how Sports buyers behave differently as events approach - exactly the segment-specific pattern I suspected.

**Price matters, but not as much as I expected.** Price features (`get_in`, `listings_median`, `price_spread_ratio`) combined explain about 20% of predictions. Important, but secondary to timing.

This feature importance analysis will guide my hyperparameter tuning in Part 6 and validate the segment-specific analysis in Part 8.

---

## 9. Error Analysis Preview

Even the best model (CatBoost, RMSE 0.2104) makes mistakes. Where does it struggle?

### 9.1 Performance by Focus Bucket

| Bucket | RMSE | Count | Difficulty |
|--------|------|-------|-----------|
| Festivals | 0.1613 | 37 | Easiest |
| Major Sports | 0.1877 | 4,094 | Easy |
| Comedy | 0.2086 | 1,619 | Medium |
| Concerts | 0.2100 | 4,696 | Medium |
| Broadway & Theater | 0.2162 | 4,869 | Hard |
| Minor/Other Sports | 0.2199 | 4,048 | Hardest |

*Figure 7: RMSE by focus bucket showing 36% variation between easiest and hardest*

**Major Sports** is easiest to predict (RMSE 0.1877) - likely due to regular schedules and predictable fan behavior.

**Minor/Other Sports** is hardest (RMSE 0.2199) - this catchall category includes MMA, tennis, golf, soccer - each with different patterns. A unified model struggles to learn this heterogeneity.

This 17% performance gap between best and worst buckets hints at something important: **Maybe different segments need different models?**

This question will drive Part 8's segment-specific analysis.

### 9.2 Residual Patterns

Plotting residuals (actual - predicted) against predictions revealed **heteroskedasticity** - fancy word for "error variance increases with prediction magnitude."

*Figure 8: Residual plot showing wider spread at higher predictions*

For low-volume events (predicted <100 tickets), errors are tight (±20 tickets). For high-volume events (predicted >300 tickets), errors spread out (±80 tickets).

This pattern suggests **ensemble methods might help**. Different models make different types of errors. Averaging them could reduce variance.

Foreshadowing Part 7...

---

## 10. Lessons Learned

### What Worked

**Tree-based models are the right tool for tabular data.** The 42% improvement over linear regression wasn't marginal - it was decisive.

**Default hyperparameters are surprisingly good.** CatBoost won with out-of-the-box settings. This validates the algorithms' design - they're well-tuned for typical use cases.

**Feature engineering paid off.** Those interaction terms I labored over in Part 3? They're in the top 10 features. The cyclical encoding for day-of-week? Making a difference.

**The top 3 tree models are nearly tied.** LightGBM, XGBoost, CatBoost all within 2% RMSE. They're all finding the same patterns, just with different approaches.

### What Surprised Me

**LightGBM's speed advantage was larger than expected.** 2.8x faster than XGBoost with 99% of the performance is a game-changer for production iteration cycles.

**CatBoost won without leveraging categorical features.** I expected XGBoost to dominate, but CatBoost's ordered boosting gave it a slight edge even on one-hot encoded data.

**Linear models failed harder than I thought.** 42% worse RMSE suggests the relationships in ticket sales are deeply non-linear. Makes sense - buying behavior doesn't follow straight lines.

### What's Next

I have a solid baseline (RMSE 0.21), but three questions remain:

1. **Could neural networks beat trees?** Part 5 will test this.
2. **Can hyperparameter tuning improve the 0.21 RMSE?** Part 6 will optimize.
3. **Will combining models beat individual models?** Part 7 will ensemble.

---

## What's Coming: Neural Networks

Tree models dominated with RMSE 0.21, explaining 96.8% of variance. That's impressive for baseline models.

But what about **neural networks?** Deep learning has revolutionized image classification, natural language processing, and game-playing AI. Could it find non-linear patterns that trees miss?

In **Part 5: Can Neural Networks Beat Gradient Boosting?**, I'll test three architectures:
- Naive feedforward network (2 layers)
- Deep network with heavy regularization (5 layers)
- ResNet-style network with skip connections

Spoiler: The results will surprise me. Trees have home-field advantage on tabular data, and they won't give it up easily...

But the lessons I learn from neural networks - about regularization, overfitting, and architecture design - will prove valuable when I build stacked ensembles in Part 7.

---

*Code and model artifacts available on [GitHub](#). Part 5 coming soon: The trees vs neural networks showdown.*
