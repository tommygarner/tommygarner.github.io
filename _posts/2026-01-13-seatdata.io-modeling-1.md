---
layout: single
title: "SeatData.io Part 4: Foundation Models - Which Algorithm Wins?"
date: 2026-01-18
description: "Testing five algorithms head-to-head to find the best baseline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - gradient boosting
  - XGBoost
  - LightGBM
  - model comparison
excerpt: "From linear regression to gradient boosting: discovering which algorithms excel at predicting secondary market ticket sales."
---

## Abstract

With 29 engineered features ready from Part 3, I faced the classic machine learning question: **which algorithm should I use?** Rather than assuming, I ran a systematic comparison of five approaches - from simple linear regression to sophisticated gradient boosting. The results surprised me: tree-based models didn't just win, they dominated by over 40%. But the tight race between XGBoost, LightGBM, and CatBoost revealed something more interesting - they were all finding similar patterns, hinting that combining them might unlock even better performance.

## Key Insights

- **Tree-Based Models Crush Linear Baselines** - Gradient boosting achieved 40%+ better RMSE than Ridge regression
- **LightGBM and XGBoost Nearly Tied** - RMSE difference of only 0.003 despite architectural differences
- **Temporal Features Dominate** - Days-to-event and weekend indicators explain 45% of prediction power

---

## 1. The Algorithm Showdown

I had engineered features. I had cleaned data. Now came the moment of truth: **training my first models**.

But here's the thing about machine learning - there's no universal "best" algorithm. The **No Free Lunch theorem** states that no single algorithm dominates across all problems. What works brilliantly for image classification might fail miserably for ticket sales prediction.

So instead of guessing, I ran an **algorithm showdown**.

### 1.1 The Five Contenders

I selected five algorithms spanning the spectrum from simple to sophisticated:

**1. Ridge Regression**
- **Type:** Linear model with L2 regularization
- **Strength:** Interpretable, fast, establishes baseline
- **Weakness:** Assumes linear relationships (rarely true in real data)

**2. Gradient Boosting (sklearn)**
- **Type:** Sequential tree ensemble
- **Strength:** Handles non-linearity, proven on tabular data
- **Weakness:** Slower to train, can overfit

**3. XGBoost**
- **Type:** Optimized gradient boosting
- **Strength:** Built-in regularization, parallel processing, handles missing values
- **Weakness:** Many hyperparameters to tune

**4. LightGBM**
- **Type:** Leaf-wise gradient boosting
- **Strength:** Fast training, efficient memory use
- **Weakness:** Can overfit on small datasets

**5. CatBoost**
- **Type:** Gradient boosting with ordered boosting
- **Strength:** Handles categorical features natively, resistant to overfitting
- **Weakness:** Slower than LightGBM on large datasets

### 1.2 Evaluation Setup

**Metric:** Root Mean Squared Error (RMSE) on test set
- Lower is better
- Penalizes large errors more than MAE
- Interpretable in original units (log-sales)

**Test Set:** Jan 1-12, 2026 (20,391 events with sales)
- Completely unseen during training
- Represents realistic production scenario

**Naive Baseline:** Predict mean sales = RMSE of 0.505
- This is what we'd get by always predicting the average
- Any model must beat this to be useful

Let the games begin.

---

## 2. Model 1: Ridge Regression - The Linear Baseline

I started with the simplest approach: **Ridge Regression**. This is a linear model that finds the best-fit line (well, hyperplane in 29 dimensions) through the data.

### 2.1 Why Start Linear?

Linear models have three advantages:
1. **Fast:** Train in seconds even on millions of rows
2. **Interpretable:** Each coefficient shows feature impact
3. **Diagnostic:** If linear works well, you might not need complexity

I trained with L2 regularization (alpha=1.0) to prevent overfitting:
```python
