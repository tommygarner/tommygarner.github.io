---
layout: single
title: "SeatData.io Part 5: Can Neural Networks Beat Gradient Boosting?"
date: 2026-01-21
description: "Testing three neural network architectures against tree-based models on tabular ticket sales data"
author_profile: true
toc: true
toc_sticky: true
tags:
  - neural networks
  - deep learning
  - keras
  - tensorflow
  - model comparison
excerpt: "From naive networks to ResNet-style architectures: exploring whether deep learning can outperform gradient boosting on structured data."
---

## Abstract

Tree-based models dominated Part 4 with RMSE of 0.21, but I had to ask: **could neural networks do better?** Deep learning has revolutionized computer vision and natural language processing. Surely with the right architecture and enough regularization, I could find patterns that gradient boosting missed. Spoiler alert: I was wrong. Neural networks struggled on this tabular data, barely reaching RMSE 0.2136 despite extensive tuning. But the journey taught me crucial lessons about when to use deep learning - and when simpler methods win.

## Key Insights

- **Neural Networks Struggle on Tabular Data** - Best NN achieved RMSE 0.2136 vs tree models at 0.21 (1.7% worse)
- **Regularization is Non-Negotiable** - Without dropout and batch normalization, NNs overfit catastrophically
- **Skip Connections Help But Don't Close Gap** - ResNet-style architecture improved convergence but not final performance

---

## 1. Why Try Neural Networks?

CatBoost won Part 4 with RMSE 0.2104, explaining 96.8% of variance. That's impressive. So why bother with neural networks?

### 1.1 The Neural Network Promise

Neural networks theoretically can:
- **Learn arbitrary feature interactions** without manual engineering
- **Create learned representations** that compress information optimally
- **Model complex non-linearities** through layer composition
- **Scale to massive datasets** where trees become impractical

If trees are finding 96.8% of the pattern, maybe NNs could find that last 3.2%?

### 1.2 The Tabular Data Challenge

But here's the uncomfortable truth: **neural networks usually lose to gradient boosting on tabular data**.

Why?
- Trees excel at **sharp decision boundaries** (if price > $100, then...)
- NNs need **smooth functions** (they interpolate rather than split)
- Tabular data has **heterogeneous features** (price, dates, counts mixed)
- NNs prefer **homogeneous data** (pixels, words, sequences)

*Figure 1: Decision boundary comparison - trees create sharp splits, NNs create smooth curves*

### 1.3 Setting Expectations

I went into this experiment knowing NNs probably wouldn't win. But I wanted to understand **why** they fail on tabular data and **what** they could teach me for ensemble methods later.

As Yann LeCun (deep learning pioneer) said: "If your data is structured, use gradient boosting. If it's unstructured (images, text), use deep learning."

Let's see if I could prove him wrong.

---

## 2. Data Preparation for Neural Networks

Trees don't care about feature scales. XGBoost happily splits on `days_to_event` (range 0-338) and `dow_sin` (range -1 to 1) without complaint.

Neural networks? **They absolutely care.**

### 2.1 The Scaling Problem

Gradient descent (the training algorithm for NNs) updates weights proportionally to feature magnitudes. If one feature ranges from 0-1000 and another from 0-1, the optimizer will:
- Take huge steps for the 0-1000 feature
- Take tiny steps for the 0-1 feature
- Struggle to converge

Solution: **StandardScaler**
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

This transforms every feature to:
- Mean = 0 (center the data)
- Standard deviation = 1 (same scale)

Now all features are on equal footing for gradient descent.

*Figure 2: Feature distributions before (varied scales) vs after (all normalized) StandardScaler*

### 2.2 Time-Based Validation Split

Critical detail: I used **Dec 29-31, 2025** as my validation set for early stopping.

Why not random 10% split? **Temporal leakage.** 

If I randomly split, I might have December 30 in training and December 5 in validation. The model could "cheat" by learning the December 5 event's pattern during training.

Time-based split ensures validation is strictly in the future from all training data:
- Training: Oct 1 - Dec 28
- Validation: Dec 29-31  
- Test: Jan 1-12

This mimics production: predict the future using only the past.
```python
VAL_START = pd.Timestamp('2025-12-29')
train_mask = train_df['snapshot_date'] < VAL_START
val_mask = train_df['snapshot_date'] >= VAL_START

X_train_nn = X_train_scaled[train_mask]
X_val_nn = X_train_scaled[val_mask]
```

With scaling done and validation split properly, I was ready to train.

---

## 3. Architecture 1: Naive Neural Network

I started simple: **Can a basic feedforward network learn ticket sales?**

### 3.1 The Simplest Architecture
```python
from tensorflow.keras import Sequential, layers

naive_nn = Sequential([
    layers.Input(shape=(29,)),  # 29 features
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='linear')  # Output: log-sales
])

naive_nn.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)
```

Just three layers:
- Input → 64 neurons → 32 neurons → 1 output
- ReLU activation (standard choice)
- Adam optimizer (adaptive learning rates)
- MSE loss (same as trees)

No regularization. No tricks. Can the network learn from raw capacity?

### 3.2 Training
```python
history = naive_nn.fit(
    X_train_nn, y_train_nn,
    validation_data=(X_val_nn, y_val_nn),
    epochs=100,
    batch_size=1024,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    ],
    verbose=1
)
```

Early stopping watches validation loss. If it doesn't improve for 10 epochs, training stops and the best weights are restored.

### 3.3 Results

**Test RMSE: 0.2682**

**That's 27% worse than CatBoost!**

Looking at the training curves explained why:

*Figure 3: Naive NN training curves showing classic overfitting - training loss drops to 0.05, validation loss plateaus at 0.23*

The network **memorized the training data** (training RMSE 0.08) but couldn't generalize (validation RMSE 0.27). This is textbook overfitting.

### 3.4 What Went Wrong

Neural networks have **millions of parameters** (64×29 + 32×64 + 32 = 4,000+ weights). With 1.5M training samples, the ratio is okay, but the network was free to learn noise.

Trees avoid this through:
- Limited depth (can't fit arbitrary curves)
- Regularization via min_samples_split
- Built-in ensemble averaging

NNs need **explicit regularization**. Time to fight back.

---

## 4. Architecture 2: Deep Neural Network with Heavy Regularization

I threw the entire regularization toolkit at the overfitting problem.

### 4.1 The Regularization Arsenal

**Batch Normalization:** Normalizes inputs to each layer, stabilizing training
**Dropout:** Randomly drops neurons during training, preventing co-adaptation
**L2 Regularization:** Penalizes large weights
**Learning Rate Scheduling:** Reduces learning rate when validation stops improving
```python
from tensorflow.keras import regularizers

deep_nn = Sequential([
    layers.Input(shape=(29,)),
    layers.BatchNormalization(),
    
    # Layer 1: 256 neurons
    layers.Dense(256, activation='relu', 
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    
    # Layer 2: 128 neurons
    layers.Dense(128, activation='relu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.25),
    
    # Layer 3: 64 neurons
    layers.Dense(64, activation='relu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    
    # Layer 4: 32 neurons
    layers.Dense(32, activation='relu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.15),
    
    # Output
    layers.Dense(1, activation='linear')
])
```

*Figure 4: Deep NN architecture diagram showing 5 layers with BatchNorm and Dropout at each level*

### 4.2 Why Each Technique Matters

**Batch Normalization** prevents internal covariate shift. As weights update, layer inputs change distribution. BatchNorm keeps them stable, allowing higher learning rates.

**Dropout** is like training with random teammates. Each training step randomly drops 30% of neurons (then 25%, 20%, 15% in deeper layers). The network can't rely on any single neuron, forcing robust representations.

Think of it like a sports team where random players sit out each practice. The team learns strategies that work even when individuals are missing.

**L2 Regularization** adds a penalty term to the loss function: `loss = MSE + 0.001 × (sum of squared weights)`. This discourages the network from using large weights, preventing over-reliance on single features.

**Adaptive Learning Rates** via ReduceLROnPlateau:
```python
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.0001
)
```

If validation loss doesn't improve for 5 epochs, cut the learning rate in half. This helps the optimizer find better local minima.

### 4.3 Results

**Test RMSE: 0.2136**

Much better! That's **20% improvement** over the naive network.

But still **1.5% worse than CatBoost** (0.2104).

*Figure 5: Deep NN training curves showing regularization working - gap between train and validation narrows*

The heavy regularization worked. Training loss was 0.18 (not 0.05) and validation loss matched at 0.21. The network was **generalizing**, just not quite as well as trees.

### 4.4 Training Dynamics

The deep NN took **147 epochs** to converge (vs 31 for naive NN). Regularization slows learning but improves final performance.

Training time: 8 minutes (vs 2 minutes for CatBoost, 47 seconds for LightGBM).

NNs are slower to train AND less accurate. Not a great combination.

---

## 5. Architecture 3: ResNet-Style Skip Connections

One more trick: **skip connections**.

### 5.1 The Vanishing Gradient Problem

In deep networks, gradients (signals for weight updates) get smaller as they propagate backward through layers. By layer 5, gradients might be 0.0001 - too small to update weights effectively.

This is the **vanishing gradient problem**, and it limits how deep you can go.

### 5.2 Skip Connections to the Rescue

ResNet (Residual Networks) solved this by adding **highway routes** for gradients:
```python
from tensorflow.keras import Model
from tensorflow.keras.layers import Add

# Input
inputs = layers.Input(shape=(29,))
x = layers.BatchNormalization()(inputs)

# Residual Block 1
x1 = layers.Dense(128, activation='relu')(x)
x1 = layers.BatchNormalization()(x1)
x1 = layers.Dropout(0.3)(x1)

# Residual Block 2
x2 = layers.Dense(128, activation='relu')(x1)
x2 = layers.BatchNormalization()(x2)
x2 = layers.Dropout(0.3)(x2)

# Skip connection: Add input of block 2 to its output
x2 = Add()([x1, x2])  # <-- This is the magic

# Continue...
x3 = layers.Dense(64, activation='relu')(x2)
# ... more layers

resnet_nn = Model(inputs=inputs, outputs=outputs)
```

The `Add()` layer creates a shortcut: "Here's the processed signal AND the original signal from 2 layers ago." Gradients can now flow backward through these shortcuts.

*Figure 6: ResNet architecture diagram showing skip connections as highways around blocks*

Think of it like a highway system. Local roads (normal layers) take you places, but highways (skip connections) let you bypass traffic.

### 5.3 Results

**Test RMSE: 0.2142**

Marginally worse than the deep NN (0.2136) but converged faster (89 epochs vs 147).

Skip connections helped **gradient flow** but didn't improve final predictions. The problem wasn't gradient vanishing - it was that NNs fundamentally struggle on this type of data.

---

## 6. Neural Network Comparison

Let me compare all three architectures:

| Architecture | RMSE | MAE | Epochs | Train Time | Key Feature |
|--------------|------|-----|--------|------------|-------------|
| **Deep NN (Heavy-Reg)** | **0.2136** | **0.1503** | 147 | 8m 14s | Best balance |
| ResNet NN | 0.2142 | 0.1509 | 89 | 6m 32s | Fast convergence |
| Naive NN | 0.2682 | 0.1921 | 31 | 2m 18s | Overfit badly |

**Winner: Deep NN with heavy regularization**

But compare to Part 4's trees:

| Model Type | Best Model | RMSE | Gap to Best NN |
|------------|------------|------|----------------|
| **Trees** | **CatBoost** | **0.2104** | **Baseline** |
| Neural Networks | Deep NN | 0.2136 | +1.5% worse |

Neural networks couldn't beat trees. Not even close.

*Figure 7: Model comparison bar chart - trees cluster at 0.21, NNs spread from 0.21 to 0.27*

---

## 7. Why Neural Networks Struggle on Tabular Data

This wasn't a failure of implementation. I tried multiple architectures, extensive regularization, and proper validation. NNs just aren't the right tool here.

### 7.1 Dataset Size vs Complexity

**1.5M training samples sounds large**, but for neural networks with millions of parameters, it's medium-sized. Trees achieve similar complexity with:
- 100-1000 trees
- Each with depth 6-10
- Total parameters: ~50,000

NNs need 10M+ samples to leverage their capacity fully.

### 7.2 Feature Interactions

Trees create **sharp decision boundaries**:
```
If days_to_event < 7:
    If price < 100:
        Predict 150 tickets
    Else:
        Predict 80 tickets
Else:
    Predict 50 tickets
```

NNs create **smooth interpolations**. They learn continuous functions, not discrete rules. Ticket sales have threshold effects ("7 days out changes behavior"), which trees capture naturally but NNs struggle with.

### 7.3 Tabular vs Grid-Structured Data

NNs excel on **grid-like** data where nearby elements relate:
- Images: Nearby pixels form objects
- Text: Nearby words form sentences
- Time series: Nearby timesteps are correlated

Tabular data features are **heterogeneous**:
- Row 1: [price=$50, days=30, sales=5, ...]
- These features don't "neighbor" each other meaningfully

NNs can't exploit spatial relationships because there aren't any.

### 7.4 When NNs Would Win

If this problem had:
- **10M+ training samples** (more data for complexity)
- **Text features** (event descriptions, performer names)
- **Image features** (venue photos, seating charts)
- **Sequential patterns** (minute-by-minute price changes)

Then NNs would dominate. But for 29 numerical features on 1.5M samples? Trees are the better tool.

---

## 8. What Neural Networks Taught Me

Even though NNs didn't win, they taught valuable lessons:

### 8.1 Feature Engineering Validation

Bad features would cause NNs to fail catastrophically. The fact that NNs reached 0.2136 (only 1.5% worse than trees) validates my Part 3 feature engineering.

If I'd engineered garbage features, NNs would have RMSE > 0.4.

### 8.2 Overfitting Risk

The naive NN's catastrophic overfitting (training RMSE 0.08, test RMSE 0.27) showed how careful you must be with model capacity.

Trees have built-in overfitting protection. NNs require explicit regularization.

### 8.3 Baseline for Ensembles

In Part 7, I'll build stacked ensembles. Having a neural network in the mix (even if weaker individually) will add **diversity** to the ensemble.

Models that make different types of errors combine well. NNs make smooth predictions, trees make sharp predictions - averaging them might improve both.

### 8.4 Hyperparameter Sensitivity

NNs taught me about:
- **Learning rates** (too high = diverge, too low = slow)
- **Batch sizes** (1024 worked well, smaller caused instability)
- **Regularization strength** (dropout 0.3 vs 0.5 mattered)

This sensitivity analysis will inform Part 6's hyperparameter optimization.

---

## 9. The ARIMA Experiment

Before giving up on non-tree models entirely, I tried one more approach: **traditional time-series forecasting**.

### 9.1 ARIMA for Aggregated Sales

From Part 2's EDA, I knew daily sales followed temporal patterns. What if I modeled **aggregate sales** across all events?
```python
from statsmodels.tsa.arima.model import ARIMA

# Aggregate to daily sales
daily_sales = train_df.groupby('snapshot_date')['sales_total_7d_next'].sum()

# Fit ARIMA(5,1,0): 5 lags, 1st-order differencing, no MA terms
arima = ARIMA(daily_sales, order=(5,1,0))
arima_fit = arima.fit()

# Forecast next 12 days
forecast = arima_fit.forecast(steps=12)
```

*Figure 8: ARIMA forecast vs actual daily sales - captures trend but misses event-level variation*

### 9.2 Results

**Aggregate RMSE: 0.3821** (on daily totals)

ARIMA captured the **weekly seasonality** (remember Part 2's finding that Saturdays outsell Sundays?), but failed at **event-level prediction**.

Why? ARIMA predicts **one number per day** (total market sales). I need to predict **sales for 20,000+ individual events** on that day.

An event-level model (trees or NNs) must distinguish:
- Lakers game (high sales) vs minor league baseball (low sales)
- Both happening on the same Saturday

ARIMA can't make these distinctions. It's the wrong granularity.

### 9.3 Lesson Learned

**Match your model to your problem's granularity.**

- Predicting total market sales? ARIMA works.
- Predicting individual event sales? Need event-level features (trees/NNs).

This confirms my approach: Event-level gradient boosting was the right choice.

---

## 10. Lessons Learned

### What Worked

**Regularization transformed NNs** from RMSE 0.27 to 0.21. Dropout, BatchNorm, and L2 were essential.

**Time-based validation** prevented leakage and mimicked production.

**Skip connections** improved convergence speed (147 epochs → 89 epochs).

**Trying multiple architectures** revealed diminishing returns: Naive NN was terrible, Deep NN was okay, ResNet NN was marginally better.

### What Didn't Work

**Neural networks couldn't beat trees** despite extensive tuning. The gap (1.5%) was small but persistent.

**Deeper isn't always better.** The 5-layer Deep NN barely beat the 3-layer Naive NN after regularization.

**Skip connections didn't improve final performance,** only convergence speed.

### The Hard Truth

**For tabular data with <10M rows, gradient boosting wins.** This isn't my failure - it's a well-documented pattern in machine learning competitions. Trees dominate Kaggle tabular challenges.

But NNs aren't useless here. In Part 7, they'll contribute to ensemble diversity.

---

## What's Next

I've now tested:
- Linear models (Part 4): RMSE 0.36 ❌
- Tree models (Part 4): RMSE 0.21 ✓
- Neural networks (Part 5): RMSE 0.2136 ✓-ish

All three approaches used **default or lightly-tuned hyperparameters**. Could systematic optimization squeeze out the last few percentage points?

In **Part 6: Hyperparameter Tuning**, I'll:
- Use RandomizedSearchCV to optimize XGBoost, LightGBM, CatBoost
- Test 50+ hyperparameter combinations per model
- Find the optimal learning rate, depth, regularization
- See if I can push below RMSE 0.20

Then in **Part 7: Ensemble Stacking**, I'll combine the best models. Even though NNs didn't win individually, averaging them with trees might reduce variance and improve predictions.

The journey from 0.505 (naive baseline) to 0.21 (current best) was dramatic. Can I reach 0.20 or below?

---

*Code for all architectures available on [GitHub](#). Part 6 dropping soon: The hyperparameter optimization deep dive.*
