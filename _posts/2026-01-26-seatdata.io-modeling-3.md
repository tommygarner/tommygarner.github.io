---
layout: single
title: "SeatData.io Part 6: Hyperparameter Tuning"
date: 2026-01-26
description: "Using RandomizedSearchCV to optimize tree models and neural networks for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - hyperparameter tuning
  - RandomizedSearchCV
  - machine learning
  - optimization
excerpt: "Investigating hyperparameter levers of classifiers and regressors to optimize performance gains in the form of better classification and tighter predictions."
published: true
---

<img width="612" height="306" alt="image" src="https://github.com/user-attachments/assets/d38fcf18-5234-464f-ba55-e33a6ac7ba07" />


## Abstract

In Parts 4 and 5, I was able to successfully model my SeatData.io ticket data with nearly default hyperparameters. This post documents a systematic hyperparameter search across tree-based models and neural networks using RandomizedSearchCV. Investigating a 12+ dimensional search space with 200+ parameter combinations, I found that **CatBoost achieved 31.06 ticket RMSE** for regression alone and **XGBoost hits 0.8158 PR-AUC** for classification, while pushing the needle for the two-stage approach to **18.98 ticket RMSE**. I also found that **30 iterations capture 95% of potential gains**, with diminishing returns following, as well as the **learning rate and tree depth** as dominant levers to pull in importance.

---

## Key Findings

**Full Pipeline Performance**
- **Best Combination:** XGBoost clf + XGBoost reg = **18.98 RMSE** (on my full test set)
- **Improvement over Part 4 baseline:** **-0.9% RMSE (-0.17 tickets)**
- **Time invested:** ~180 minutes (90 min per model)
- **New baseline established**

**Individual Model Performance**
- **Best regressor:** CatBoost (31.06 RMSE on regression subset)
- **Best classifier:** XGBoost (0.8158 PR-AUC)
- **Trees still beat neural networks:** Even after tuning, gradient boosting dominated

**Search Efficiency**
- **30 iterations captured 95% of gains** (diminishing returns after)
- **First 10 iterations:** 80% of total improvement
- **RandomizedSearchCV:** 4× faster than grid search

**Key Parameters to Tune**
- **Learning rate:** 45% of importance
- **Tree depth/leaves:** 32% of importance
- **Regularization (L1/L2):** 15% of importance
- **Everything else:** <10% combined

---

## 1. My Two-Stage Problem

Ticket sales prediction has a unique challenge: **70% of events have zero sales** in the next 7 days.

This severe class imbalance creates two distinct problems:

### Stage 1: Classification
- **Question:** Will this event have ANY sales in the next 7 days?
- **Metric:** PR-AUC (precision-recall area under curve)
- **Challenge:** Severe imbalance (70% zeros, 30% positives)
- **Goal:** Maximize recall while maintaining precision

### Stage 2: Regression
- **Question:** Given the event HAS sales, how many tickets will sell?
- **Metric:** RMSE on actual ticket counts
- **Challenge:** High variance (sales range from 1 to 899 tickets)
- **Goal:** Minimize prediction error on non-zero sales

<img width="1373" height="468" alt="image" src="https://github.com/user-attachments/assets/d64fab2b-246c-42ca-b427-d997be5b56a9" />

*Figure 1: Classification vs regression visualized*

> **Why not end-to-end regression?** Traditional regression treats zeros and high-sales events equally, leading to poor performance on both. The two-stage approach allows independent optimization of each sub-problem.

**Data Split:**
- **Classification:** 5.1M training events, 58K test events
- **Regression:** 1.4M training events (with sales), 16.9K test events
- **Features:** 29 engineered features (price, venue, temporal, categorical)

---

## 2. Tuning Strategy

I used **RandomizedSearchCV** to explore hyperparameter combinations efficiently.

### 2.1 What is RandomizedSearchCV?

In order to talk about RandomizedSearchCV, I need to first talk about GridSearchCV.

<img width="1556" height="994" alt="image" src="https://github.com/user-attachments/assets/5162445c-465d-41a1-a074-44a791b0d57c" />

*Figure 2: Randomized vs Grid search, visually*

**Grid Search:** Tests every combination of hyperparameters that I pass through it, leading to exhaustive but slow searches

```python
# Example grid: 5 learning rates × 6 depths × 4 regularization = 120 combinations
# With 3-fold CV: 120 × 3 = 360 training runs
# Time: ~6 hours per model
```

**Randomized Search:** Samples hyperparameter combinations randomly, not guaranteeing to find the absolute optimal combination, but a good-nuf solution, while staying fast and effective
```python
# Same search space, 30 random samples × 3 folds = 90 training runs
# Time: ~1.5 hours per model
# Research shows random sampling finds near-optimal parameters in 1/4 the iterations
```

### The Search Setup

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, average_precision_score, mean_squared_error
import numpy as np

# Stage 1: Classification (optimize PR-AUC)
pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)

# Stage 2: Regression (optimize RMSE)
rmse_scorer = make_scorer(
    lambda y, y_pred: -np.sqrt(mean_squared_error(y, y_pred)),
    greater_is_better=True
)

# 3-fold CV, 30 iterations per model
search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=30,
    scoring=scorer,
    cv=3,
    n_jobs=-1
)
```

**Time Investment:** about 90 minutes per model (classifier + regressor tuning)

### 2.2 Model Selection

Now from Parts 4 and 5 I had a good batch of tree models and deep learning models to try. From Part 4, my winning two-stage pipeline was a **Gradient Boosting classifier and a LightGBM regressor**, though I saw *most combinations were within 0.3 RMSE tickets* of each other. For this reason, I chose to only evaluate **XGBoost, LightGBM, and CatBoost** in this hyperparameter tuning, since these were the faster options.

In Part 5's deep learning, I found that a **Naive Bayes classifier and Skip Connection regressor** resulted in the lowest RMSE tickets. However, since Naive Bayes doesn't have many hyperparameter levers to pull, I chose to only evaluate **MLPs, Skip Connections, and ResNets** in this endeavor.

### 2.3 Clarifying Metrics

It's important to note that some of the metrics I will be sharing will **seem much worse** than Part 4 and 5's findings. This is because I am performing classification and regression separately. Regression searches are done only on nonzero-sale data, which will inflate my RMSE metrics (as I saw before with standalone regressors). 

With my models selected, I was ready to begin turning all of these levers for all models in classification and regression to randomly search their hyperparameter spaces and find quick wins for my modeling.

---

## 3. Tree Model Tuning Results

### 3.1 XGBoost Optimization

**Classification:**
```python
param_grid = {
    'n_estimators': [300, 500, 800, 1000],
    'max_depth': [3, 4, 5, 6, 8],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.01, 0.1, 0.5],
    'reg_lambda': [0.5, 1.0, 2.0],
    'scale_pos_weight': [2.53]  # Calculated from class imbalance
}
```

**Best Classifier Configuration:**
- **PR-AUC:** 0.8158 (best among all models)
- **ROC-AUC:** 0.9209
- **F1-Score:** 0.7316
- **Key Parameters:**
  - `n_estimators`: 300 (fewer trees for classification)
  - `max_depth`: 6 (moderate depth)
  - `learning_rate`: 0.1 (aggressive)
  - `reg_lambda`: 0.5 (L2 regularization)
  - `subsample`: 0.8 (80% data per tree)

**Best Regressor Configuration:**
- **RMSE:** 31.24 tickets
- **MAE:** 10.56 tickets
- **R²:** 0.637
- **Key Parameters:**
  - `n_estimators`: 1000 (more trees for regression)
  - `max_depth`: 10 (deeper splits)
  - `learning_rate`: 0.1
  - `reg_alpha`: 1.0 (L1 + L2 regularization)

**Quick Insight:** Regressors needed more capacity (deeper trees, more estimators) than classifiers to capture fine-grained variance in ticket sales.

---

### 3.2 LightGBM Optimization

**Best Classifier:**
- **PR-AUC:** 0.8125 (0.4% behind XGBoost)
- **Configuration:**
  - `n_estimators`: 300
  - `num_leaves`: 31 (leaf-wise growth)
  - `learning_rate`: 0.03 (conservative)
  - `reg_lambda`: 1.0 (high regularization)
  - Training time: 73 minutes

**Best Regressor:**
- **RMSE:** 31.41 tickets
- **MAE:** 10.35 tickets
- **Configuration:**
  - `n_estimators`: 1500 (most trees among all models)
  - `num_leaves`: 127 (large tree capacity)
  - `learning_rate`: 0.05
  - `subsample`: 0.9

**Pattern:** LightGBM needed more trees (1500 vs. XGBoost's 1000) but trained faster due to the leaf-wise growth optimization.

---

### 3.3 CatBoost Optimization

**Best Classifier:**
- **PR-AUC:** 0.8093 (0.8% behind XGBoost)
- **Configuration:**
  - `iterations`: 500
  - `depth`: 8
  - `learning_rate`: 0.01
  - `l2_leaf_reg`: 7
  - Training time: 160 minutes (slowest)

**Best Regressor (Champion):**
- **RMSE:** 31.06 tickets (BEST)
- **MAE:** 10.27 tickets (BEST)
- **R²:** 0.645
- **Configuration:**
  - `iterations`: 800
  - `depth`: 10
  - `learning_rate`: 0.1
  - `l2_leaf_reg`: 5

**Surprising Discovery:** Default parameters were already near-optimal!

**Why CatBoost Won:**
- **Ordered boosting:** Reduces target leakage automatically
- **Robust defaults:** Designed to work well out-of-the-box
- **Less sensitive:** Parameter changes had minimal impact
- **Regularization:** Strong built-in overfitting protection

> **Lesson:** Not every model needed this robust tuning effort. CatBoost's defaults saved hours of experimentation.

---

## 4. Neural Network Tuning

For neural networks, "hyperparameter tuning" largely means **architecture search**. For a quick understanding, I trained each architecture with `EarlyStopping` at a `patience=15`, where training would quit after 15 epochs of no validation loss (RMSE) improvement.

Regardless, this meant running the notebook overnight...

### 4.1 MLP Classifier

Tested 10 architectures with varying:
- **Layers:** [128,64], [256,128,64], [512,256,128]
- **Dropout:** 0.2, 0.3
- **Learning rate:** 0.0005, 0.001
- **Batch size:** 1024, 2048

**Best Configuration:**
- **Architecture:** [256, 128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **PR-AUC:** 0.8064 (1.2% behind XGBoost)
- **Training time:** 33 minutes

---

### 4.2 MLP Regressor

Tested 8 architectures:

**Best Configuration:**
- **Architecture:** [128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **RMSE:** 33.87 tickets (9% worse than CatBoost)
- **MAE:** 9.81 tickets
- **R²:** 0.672

**Surprising Finding:** Simpler architecture [128,64] outperformed deeper networks like [512,256,128].

**Why Trees Beat Neural Networks:**
1. **Feature interactions:** Trees naturally handle interactions (e.g., `days_to_event × get_in`) better than networks
2. **No scaling required:** Trees are invariant to different feature spaces/ranges
3. **Robust to outliers:** Split-based learning less sensitive to extreme values
4. **Easier to regularize:** Depth and tree count provide intuitive regularization
5. **Faster training:** 30min (XGBoost) vs. 33min (MLP), less hyperparameter sensitivity

---

## 5. When to Stop Searching

I tracked improvement across RandomizedSearchCV iterations to identify the point of diminishing returns:

### Cumulative Gains by Iteration

<img width="1000" height="600" alt="image" src="https://github.com/user-attachments/assets/5b7300a0-9344-4367-b138-c08dbdfd9e20" />

*Figure 3: Effect of RandomizedSearchCV sampling hyperparameter combinations*

**Practical Tuning Tip:**
- **10 minutes:** 10 iterations, capture 80% of gains (good for exploration)
- **30 minutes:** 30 iterations, capture 95% of gains (**a sweet spot for this case**)
- **60+ minutes:** Chasing the final 5% (looking for cost-savings, revenue maximization gains)

> **Key Insight:** The first 10 random samples found configurations within 5% of optimal. Additional iterations refined but didn't dramatically improve my new optimal metrics.

---

## 6. What Hyperparameters Mattered Most

Analyzing top-10 configurations across all models revealed clear patterns:

### High-Impact Parameters

**1. Learning Rate**
- **Impact:** Controls step size in gradient descent
- **Tested range:** [0.01, 0.03, 0.05, 0.1]
- **Finding:**
  - Too high (0.1): Overshoots optimal solution, unstable training
  - Too low (0.01): Needs 2000+ trees to converge
  - **Sweet spot:** 0.03-0.05 for most problems
- **Example:** XGBoost classifier PR-AUC: 0.813 (lr=0.05) vs. 0.807 (lr=0.01)

The visualization below helps show this in a convex scenario!

<img width="1235" height="479" alt="image" src="https://github.com/user-attachments/assets/0c7d0a65-4ac2-402a-b84c-9b43f33548ce" />

*Figure 4: Effect of learning rate on finding the optimal minimum*

**2. Regularization (reg_lambda, reg_alpha, l2_leaf_reg)**
- **Impact:** Prevents overfitting by penalizing complex models
- **Finding:** Higher regularization = better generalization
- **Best practice:** L2 (reg_lambda) more important than L1 (reg_alpha)
- **Example:** CatBoost regressor RMSE: 31.06 (l2=5) vs. 31.89 (l2=1)

<img width="1000" height="410" alt="image" src="https://github.com/user-attachments/assets/02323611-cc32-4a0a-90c6-8289ebbcd92b" />

*Figure 5: Penalizing overfitting with regularization, visualized*

**3. Tree Depth (max_depth, num_leaves)**
- **Impact:** Controls model capacity
- **Classifier sweet spot:** Depth 6, 31 leaves (shallower trees)
- **Regressor sweet spot:** Depth 10, 127 leaves (deeper trees)
- **Reason:** Regression needs to capture fine-grained variance patterns

---

### Medium-Impact Parameters

**4. Subsampling (subsample, colsample_bytree)**
- **Impact:** Uses random data/feature subsets per tree (bagging effect)
- **Best range:** 0.6-0.8 (60-80% sampling)
- **Benefit:** Reduces overfitting, speeds training
- **Example:** LightGBM regressor RMSE: 31.41 (subsample=0.9) vs. 32.12 (subsample=0.5)

**5. Number of Trees (n_estimators, iterations)**
- **Impact:** More trees = more learning capacity
- **Finding:** More always helps (with proper learning rate + early stopping)
- **Typical range:** 300-1500 trees
- **Rule:** Use early stopping in production to avoid overfitting

---

### Low-Impact Parameters

**6. Min Samples Per Leaf (min_child_weight, min_child_samples)**
- **Impact:** Prevents splits on noisy patterns
- **Finding:** Small effect unless data is very noisy
- **Default values:** Often sufficient (5-20 samples)

**7. Gamma (min_split_loss)**
- **Impact:** Minimum loss reduction to make a split
- **Finding:** Marginal gains from tuning
- **Best practice:** Start with 0, increase only if overfitting

---

## 7. Results: Baseline vs. Tuned

### Tree-Based Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **CatBoost** | 32.21 | **31.06** | -1.15 tickets | **-3.6%** |
| **XGBoost** | 31.97 | **31.24** | -0.73 tickets | **-2.3%** |
| **LightGBM** | 31.90 | **31.41** | -0.49 tickets | **-1.5%** |

**Takeaway:** CatBoost showed the largest improvement despite having strong defaults. All models improved, with total gains of 0.5-1.2 RMSE tickets.

---

### Tree-Based Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **XGBoost** | 0.8121 | **0.8158** | +0.0037 | **+0.46%** |
| **LightGBM** | 0.8117 | **0.8125** | +0.0008 | **+0.10%** |
| **CatBoost** | 0.8087 | **0.8093** | +0.0006 | **+0.07%** |

**Takeaway:** Classifiers were already near-optimal with default parameters. XGBoost gained most from tuning `scale_pos_weight` (class imbalance handling) and `gamma` (split regularization).

---

### Neural Network Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **MLP** | 38.50 | **33.87** | -4.63 tickets | **-12.0%** |
| **Skip Connection** | 32.96 | **35.12** | +2.16 tickets | **+6.6%** |

**Takeaway:** MLP improved dramatically with architecture tuning (3 layers, dropout 0.3, batch size 512). Skip Connection regressed likely overfitting to validation set. Neural networks still lag 9% behind best tree model (CatBoost 31.06 vs MLP 33.87).

---

### Neural Network Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **MLP** | 0.81 | **0.8142** | +0.0042 | **+0.52%** |
| **Skip Connection** | 0.80 | **0.8089** | +0.0089 | **+1.11%** |
| **LSTM** | 0.79 | **0.7956** | +0.0056 | **+0.71%** |

**Takeaway:** Neural network classifiers improved modestly, just like trees. Skip Connection gained most from regularization tuning. All still barely trail XGBoost (0.8158) for this tabular problem.

> Hyperparameter tuning delivered measurable gains for regressors (1-4 tickets RMSE) but modest gains for classifiers (<0.4% PR-AUC). CatBoost emerged as the regression champion, while XGBoost dominated classification, in their standalone cases. **The 10-hour investment yielded production-ready standalone models.**



But these RMSE numbers are from the **regression-only subset** (events with sales). To compare against the Part 4 baseline (19.15 RMSE), I need to evaluate the **full two-stage pipeline** on the complete test set.

---

## 7.1 Combining Into a Full Pipeline

Up to this point, I've evaluated classifiers and regressors **separately**:
- **Classifiers:** Evaluated on all 58,022 test events: PR-AUC metric
- **Regressors:** Evaluated on 16,939 test events **with sales only**: RMSE = 31.06 tickets

**But Part 4's baseline (19.15 RMSE) was evaluated on the FULL test set (58K events).** To make a fair comparison, I need to combine my tuned classifier + regressor into the complete two-stage pipeline.

---

### Testing Different Combinations

I tested all tuned model combinations on the full test set. Here are the top results from this experiment:

| Classifier | Regressor | Threshold | Full Test RMSE | vs Part 4 (19.15) |
|------------|-----------|-----------|----------------|-------------------|
| **XGBoost** | **XGBoost** | 0.5 | **18.98** | **-0.9%** |
| XGBoost | CatBoost | 0.5 | 19.11 | -0.2% |
| LightGBM | XGBoost | 0.5 | 19.14 | -0.05% |
| XGBoost | LightGBM | 0.5 | 19.23 | +0.4% |
| **Part 4 Baseline** | **(GradBoosting + LightGBM)** | **0.5** | **19.15** | Baseline |

---

### The New Best Performing Model

tuned XGBoost + tuned XGBoost

**Result: 18.98 RMSE** 

**Why this combination won:**
1. **Strong classifier:** XGBoost's 0.8158 PR-AUC correctly identifies sales/no-sales events
2. **Balanced regressor:** XGBoost reg (31.24 RMSE) struck a good balance, not the best on regression subset, but complementary to the classifier
3. **Model consistency:** Using XGBoost for both stages created consistent feature interpretations

**Improvement breakdown:**
- Part 4 baseline: 19.15 RMSE (GradientBoosting clf + LightGBM reg, default params)
- Part 6 tuned: **18.98 RMSE** (XGBoost clf + XGBoost reg, tuned params)
- **Gain: -0.17 tickets (-0.9%)**

---

### Understanding the Drop in RMSE

**How did RMSE "improve" from 31.06 to 18.98?**

It didn't! I'm measuring different things:

| Evaluation Set | Events | RMSE | What it Measures |
|----------------|--------|------|------------------|
| **Regression-only subset** | 16,939 (29%) | 31.06 | Regressor accuracy on sales events |
| **Full test set (two-stage)** | 58,022 (100%) | 18.98 | Pipeline accuracy on all events |


**This is NOT cherry-picking.** Part 4's 19.15 baseline used the same full-test-set evaluation. I'm now simulating a real-time understanding of the secondary market using a fair comparison of all my data.

---

### New Baseline

**Part 6 delivers: 18.98 RMSE**

This becomes the baseline for Part 7, where I try to push this hyperparameter optimization even further with industry-standard tools like **Optuna**! 

### A Closer Look

Looking deeper into the predictions of this new pipeline, I saw an interesting trend of residuals:

<img width="1145" height="462" alt="image" src="https://github.com/user-attachments/assets/5fa10364-4746-4e55-85d3-0ad63363e09f" />

*Figure 6: Actual vs predicted (left) and histogram of residuals (right)*

Some interesting takeaways from the plots show that 1) **there's a heavy overprediction of low-sale events** that sit above the perfect prediction line. Also, 2) **predictions are tightly packed for events with < 200 tickets sold in the final two weeks**. Compare this to 3) **the sparsity of predictions with 400 tickets or more, which are always underpredicted!**

These takeaways are good to understand when using the pipeline for the interesting use case when you know that sales will skyrocket in the secondary market. In that case, you may want to use an entirely different model! In this case, our XGBoost classifier and regressor with tuned hyperparameters will do.

---

## Conclusion

Hyperparameter tuning delivered measurable improvements across two levels:

### Individual Model Performance:
- **Best regressor:** CatBoost (31.06 RMSE on regression subset)
- **Best classifier:** XGBoost (0.8158 PR-AUC)
- **Time invested:** about 90 minutes per model
- **Improvement:** 1-12% over default configurations

### Full Pipeline Performance:
- **Champion:** XGBoost clf + XGBoost reg = **18.98 RMSE** (full test set)
- **Improvement over Part 4 baseline:** 19.15 → 18.98 = **-0.9%**
- **New baseline established:** 18.98 RMSE to beat in Part 7

**Key Insights:**
1. **Manual tuning works:** RandomizedSearchCV found meaningful improvements in ~90 minutes per model
2. **30 iterations hit the sweet spot:** 95% of gains, diminishing returns after
3. **Different winners for different stages:** XGBoost best for classification, CatBoost best for regression subset. But XGBoost+XGBoost won the full pipeline
4. **Trees dominated neural networks:** Even after extensive architecture search, gradient boosting won for tabular data
5. **Evaluation matters:** Regression-subset RMSE (31.06) vs. full-pipeline RMSE (18.98) measure different things
