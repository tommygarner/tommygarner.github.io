---
layout: single
title: "SeatData.io: What the Secondary Market Tells Primary Pricers"
date: 2026-02-15
description: "How secondary ticket market velocity signals can inform dynamic primary pricing, and what a 9-part ML project revealed about demand in live events"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - ticket market
  - forecasting
  - portfolio
excerpt: "Primary pricing teams at Ticketmaster and LiveNation set prices weeks in advance with limited demand visibility. The secondary market is a great proxy. This project built a system to read the secondary market to demand forecast."
published: true
---

  [<i class="fas fa-external-link-alt" aria-hidden="true"></i> View Live
  Dashboard](https://event-explorer.streamlit.app/?embed=true){: .btn .btn--info}

  <style>
    .streamlit-container {
      border: 1px solid #ddd;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      margin: 2rem 0;
    }
    .dashboard-loading {
      text-align: center;
      padding: 2rem;
      color: #666;
      font-style: italic;
    }
  </style>

  <div class="streamlit-container">
    <div id="dashboard-loading" class="dashboard-loading">Loading interactive dashboard...</div>
    <iframe
      id="wedgie-dashboard"
      src="https://event-explorer.streamlit.app/?embed=true"
      width="100%"
      height="1200"
      frameborder="0"
      allowfullscreen="true"
      style="display:none;"
      onload="document.getElementById('dashboard-loading').style.display='none'; this.style.display='block';">
    </iframe>
  </div>

## The Problem With Primary Pricing

When Ticketmaster or LiveNation price a concert, they are making a decision weeks or months before the show with limited real-time demand data. Price too low and they leave revenue on the table. Price too high and tickets stall, the show looks empty, and the artist's team gets unhappy.

StubHub already has a cleaner read on demand.

Secondary market listings and sales velocity update daily. If floor prices are holding firm ten days out and tickets are moving, demand is strong and the primary market likely underpriced the show. If floor prices are compressing at day five with little buyer activity, demand is soft and the primary market overpriced. Secondary market behavior is a leading indicator that most primary pricing teams monitor manually, if at all.

This project built a forecasting system that reads those signals at scale: **predicting 7-day secondary ticket sales for any event, using only the daily market snapshot data that StubHub reports**. The secondary forecast serves as the demand proxy. A primary pricing team using it can ask whether a show is gaining momentum or losing it, and whether they need to act before the final week.

---

## What I Built

I built a forecasting pipeline on four months of daily StubHub snapshot data covering **6 million rows, 114,000 events, and 13,000 venues**. The model predicts how many tickets will sell in the next seven days for any given event on any given day, with a point estimate and a 95% confidence interval.

**Results at a glance:**

| Metric | Value |
|--------|-------|
| Events evaluated (test set) | 58,022 |
| RMSE vs. naive baseline | **50.6% lower** |
| Average miss per event | **10.35 tickets** |
| Training data | 6M+ daily snapshots |
| Model configurations tested | 78 two-stage pipeline combinations |

The naive baseline is what a pricing analyst would rely on without a model: predict the same average sales for every event every day. A 50.6% improvement over that means the model is meaningfully tracking which events are gaining velocity and which are slowing down, not just guessing the market mean.

---

## What the Market Data Shows

Before building any model, I looked at what secondary market data actually reveals. Three findings shaped the rest of the project.

### Floor prices drop about 10% as events approach

Secondary sellers reprice constantly based on real buyer activity. Aggregated across all event types, **floor prices drop roughly 10% in the final week before showtime**. This is sellers responding to weak demand before any official pricing action happens on the primary side.

[![Chart showing floor price declining as days to event approach zero](https://github.com/user-attachments/assets/bd32f380-278b-4d95-a28d-e5e5c4c25e6d)](https://github.com/user-attachments/assets/bd32f380-278b-4d95-a28d-e5e5c4c25e6d)
*Floor price falls predictably as the event date approaches*

For a primary pricing team, a stable secondary floor at day ten is a sign that the original price held up. A falling floor at day fourteen is a signal that demand is softer than expected, and a promotional push might be needed before the window closes.

### Sales volume accelerates in the final week

Sales activity picks up significantly in the last seven days before an event. This is the window where the model is most useful: predictions are still actionable, but time to intervene is limited.

### External shocks hit every category at once

On November 4th, Election Day, secondary sales dropped significantly across every event category with meaningful inventory. I confirmed this with seasonal z-scores, which compare each day to the distribution for that same day of the week. **The drop was statistically anomalous for a Tuesday.**

This is a real limitation of any model trained on historical patterns. Macro shocks from elections, major news events, or unexpected cancellations are not predictable from market signals alone. A production deployment would need external override triggers for days where context clearly breaks the normal pattern.

---

## Business Impact, In Numbers

The model produces a weekly ticket sales forecast with an average miss of **10.35 tickets per event**.

Using the dataset average floor price of about $113, a 10-ticket accuracy window translates to roughly **$1,170 in revenue precision per event snapshot**. At the individual event level that is modest. At scale it is not.

Consider a primary pricing team managing 1,000 shows per week. If the model correctly identifies 50 of those shows as gaining momentum on secondary, where velocity is strong and the original primary price was conservative, and the team holds price or adds a 5% premium on remaining inventory, the incremental revenue per show on a 5,000-seat venue at a $100 average ticket price is **$25,000 per show**. Across 50 shows, that is **$1.25M per week** from a signal that already exists in the secondary market and was previously being ignored or read manually.

The same math runs in the other direction. Identifying 50 shows per week where secondary velocity is stalling allows the team to run planned promotional pricing rather than reactive discounting the day before the show. A controlled 10% discount on 500 tickets is a different outcome than a last-minute fire sale.

The model does not create revenue. It surfaces a signal that the secondary market is already generating, and replaces a gut-feel monitoring process with a consistent trigger.

---

## How the Forecasting System Works

### Part 1: Structuring the Data

Four months of daily StubHub CSV snapshots were loaded into BigQuery and organized into a star schema connecting events, venues, and daily market readings. I also built an automated classification system that labeled 114,000 events into 38 categories using regex patterns, distinguishing "Concert-Pop/A-List" from "Concert-Legacy/Tribute" from "NBA" without manual tagging after the rules were written.

These 38 categories were consolidated into seven modeling buckets: Broadway and Theater, Comedy, Concert, Festivals, Major Sports, Minor/Other Sports, and Other.

[![Entity relationship diagram showing the BigQuery star schema](https://github.com/user-attachments/assets/f8e86cd8-4023-48e2-9ee1-2cea7c3d71fd)](https://github.com/user-attachments/assets/f8e86cd8-4023-48e2-9ee1-2cea7c3d71fd)
*The database schema connecting events, venues, and daily snapshots*

### Part 2: Understanding What Drives Demand

The most useful EDA finding was the pricing tier structure across categories. Three distinct tiers emerged:

- **Premium tier** (Festivals, Theater/Broadway): floor prices around $80
- **Mid tier** (Concerts, Comedy, Other): $50-65
- **Volume tier** (Major and Minor Sports): $30-40

This tier structure matters for primary pricing because a concert is not priced like an NBA game, and the signals that indicate healthy demand look different across tiers. A floor price drop for a sports ticket means something different than the same drop for a Broadway show.

I also found strong day-of-week seasonality. **Saturdays had the highest secondary sales volume, Tuesdays had the most variance**. Any model ignoring this structure would misread the market on off-peak days.

### Part 3: Preparing the Data

The raw sales distribution was heavily skewed. A small number of high-demand events drove thousands of weekly sales while most had zero. I applied a log transformation that reduced skewness from 12.28 to 1.25, which is close to symmetric and made the model's errors more consistent across typical events.

Venue capacity was missing for **42% of events**, and it turned out to matter for predictions. I filled the gaps through a four-step pipeline: fuzzy name matching against a reference spreadsheet, Ticketmaster API lookups, Wikidata SPARQL queries, and finally category-level medians.

[![Side-by-side histogram showing raw vs log-transformed sales distributions](https://github.com/user-attachments/assets/9b506158-f066-46cb-a24f-8123fa640dfd)](https://github.com/user-attachments/assets/9b506158-f066-46cb-a24f-8123fa640dfd)
*Skewness drops from 12.28 to 1.25 after log transformation*

### Part 4: Two-Stage Pipeline

**72% of daily snapshots have zero sales.** A standard regression model predicting into that structure spends most of its capacity learning to predict near zero, and fails on the events that actually matter.

My solution was a two-stage design. A classifier first decides whether any sales will happen at all. Then a separate regressor estimates how many, but only on the events the classifier flagged. This split alone cut prediction error by **40%** compared to single-stage regression. The gain came entirely from architecture, not from tuning.

[![Bar chart comparing RMSE across single-stage vs two-stage pipeline configurations](https://github.com/user-attachments/assets/4284a413-5286-41c8-9cb0-48fb39a9c99b)](https://github.com/user-attachments/assets/4284a413-5286-41c8-9cb0-48fb39a9c99b)
*Two-stage pipelines outperform single-stage regression across all configurations*

### Part 5: Neural Networks vs. Tree Models

I tested three deep learning architectures against tree-based models. The best neural network pipeline matched tree model accuracy (MAE 3.96 vs 3.89) but required **100x the training time**. For structured tabular data with engineered features, neural networks did not justify the extra cost. Tree-based models became the standard going forward.

### Part 6: Hyperparameter Tuning

Random search over 30 hyperparameter combinations captured 95% of the available tuning gain. The two most important parameters were learning rate and tree depth. Everything else contributed less than 10% of total improvement combined. Full-test RMSE dropped from 19.15 to 18.98, a 0.9% improvement.

The residual analysis from this stage also showed something important: **the model consistently underestimates very high-demand events**. For primary pricers, this means the model is most reliable for typical events in the middle of the demand distribution. For breakout events like blockbuster tours or playoff matchups, the model will likely underforecast secondary velocity. Pricing teams should treat a strong positive signal from the model as a minimum estimate, not a ceiling.

[![Actual vs predicted scatter plot and residual distribution](https://github.com/user-attachments/assets/5fa10364-4746-4e55-85d3-0ad63363e09f)](https://github.com/user-attachments/assets/5fa10364-4746-4e55-85d3-0ad63363e09f)
*The model underestimates extreme events*

### Part 7: Bayesian Optimization and Threshold Tuning

I replaced random search with 200-trial Bayesian optimization using Optuna. That moved RMSE from 18.98 to 18.78. But the **largest single gain in the entire project** came from a non-model change: raising the classifier's confidence threshold from 0.50 to 0.90.

At 0.50, the classifier activates the regressor whenever it is at least 50% confident of a sale. At 0.90, the regressor only runs when confidence is at least 90%. Moving to 0.90 pushed RMSE to 18.53, a combined 3.2% gain over Part 6. High-confidence predictions were substantially more accurate, and false positives on quiet events were driving disproportionate error.

For a production pricing system, this finding matters operationally. A tool that only alerts on high-confidence signals generates fewer false alarms and earns more trust from the teams using it. Where a two-stage system decides to fire is often a bigger lever than how well the second stage predicts.

[![Search strategy comparison across grid, random, and Bayesian optimization runs](https://github.com/user-attachments/assets/696d7d1d-dcf1-4aef-b29e-183d564c05c6)](https://github.com/user-attachments/assets/696d7d1d-dcf1-4aef-b29e-183d564c05c6)
*Bayesian optimization converges faster than random search*

### Part 8: One Model vs. Segment-Specific Models

The last experiment tested whether training a dedicated model per event category would outperform the unified model. On the held-out test set, dedicated models improved predictions for four of seven categories. But cross-validation across three time windows showed a different result: the unified model won or tied everywhere.

The test-set gains were specific to that time period. For the Festivals category with only 17,000 training rows, the dedicated model was fitting patterns from the training window rather than learning generalizable signals. One model trained on all 6 million rows generalized better than seven models trained on subsets.

[![Bar chart comparing segment model results vs unified model across categories](https://github.com/user-attachments/assets/4a35a3ad-0d67-4dd7-9e22-eeab31c8c295)](https://github.com/user-attachments/assets/4a35a3ad-0d67-4dd7-9e22-eeab31c8c295)
*Segment models appear better on one test window but do not hold up across time folds*

### Part 9: Testing Semantic Embeddings

The final experiment asked whether the model could benefit from knowing something about the identity of each event. Every feature in the existing set describes market state: price levels, listing counts, days to event, day of week. None of them say anything about who the artist is, which team is playing, or whether the event is a regional act or a major national tour.

I built a pipeline that fetches Wikipedia summaries for 18,000 artists, venues, teams, and events, converts them into 384-dimensional vectors using the `all-MiniLM-L6-v2` sentence transformer, and compresses them to 102 dimensions with PCA. For artists without a Wikipedia page, I used Last.fm bios and genre tags as a fallback source. These compressed vectors were joined onto training and test data by a normalized identifier called a slug.

#### The Slug System

The pipeline connects event records to external sources using normalized identifiers called slugs. Three slug types are used: `artist::` for concert and comedy acts (after stripping venue info and format markers from the event name), `team::` for sports events, and `event::` for everything else. Normalization removes accents, strips punctuation, and collapses whitespace so "O.A.R." and "OAR" resolve to the same slug. Any mismatch between how a slug is derived at collection time versus join time means zero coverage for that entity, so the derivation logic is kept centralized and identical across all scripts.

#### Data Collection and Coverage

Wikipedia is the primary source. For each entity slug, the pipeline fetches the page summary (up to 2,000 characters), checkpointing to `data/wiki_cache.parquet` every 100 entries to allow resuming without re-fetching.

| Entity type | Wikipedia coverage |
|-------------|-------------------|
| Artist | ~61% |
| Team | ~95% |
| Venue | ~69% |
| Event | ~81% |

For the 1,299 artists with no Wikipedia page, Last.fm provided a fallback: bio text, genre tags, and listener count were joined into a single text string and passed through the same sentence transformer. Broadway coverage is only 7.6% in the test set because event names do not slug-match Wikipedia well under this scheme.

#### AI Summary Generation

A secondary output of the enrichment pipeline is 13,804 plain-English summaries generated via GPT-4o-mini (Gemini Flash as fallback) from the same Wikipedia and Last.fm text. These appear as context cards in the Event Explorer dashboard and are not model features. Total generation cost: $1.92 for 7.4M input tokens and 1.3M output tokens.

Four configurations were tested, ranging from no embeddings (baseline) to adding 102 PCA components. The full ablation ran 32 training slots with 60 Optuna trials each.

**The result: embeddings do not improve overall accuracy.**

| Config | RMSE | vs baseline |
|--------|------|------------|
| Baseline (no embeddings) | 19.09 | -- |
| + 10 PCA components | 19.25 | +0.15 |
| + 50 PCA components | 19.31 | +0.22 |
| + 102 PCA components | 19.21 | +0.12 |

All embedding configs were slightly worse than the baseline on the full test set. Even restricting the evaluation to only the 61.9% of test rows that received a real embedding (removing zero-vector rows for unmatched events), the best embedding config scored essentially the same as baseline: 22.90 vs 22.87 RMSE.

The bucket-level breakdown reveals why. Sports events benefit noticeably from embeddings (Minor/Other Sports improved by 0.77 RMSE, Major Sports by 0.36), while Concert events got worse by nearly 1 RMSE point despite 87% embedding coverage.

The tabular market features are not identity-agnostic: they are identity-encoded-at-prediction-time. A $300 get-in price and 15 active listings in a 20,000-seat venue tells you more about likely demand in the next seven days than a Wikipedia biography of the artist. Embeddings describe what an entity is; market features describe how that entity's event is performing right now. Sports are a partial exception: team identity, including market size, standings, and rivalry context, is harder to encode through price and listing signals alone, which is why team embeddings produce a small but consistent improvement.

The classifier stage showed a small benefit from embeddings (PR-AUC 0.819 vs 0.814 baseline), meaning embeddings help identify whether an event will sell at all, but that improvement does not carry through to the regression stage where predicting volume matters.

---

## Where the Model Works Best

The model's error varies substantially by event type. This matters for deciding where to trust automated signals and where to require human review.

| Category | RMSE (tickets) | MAE (tickets) |
|----------|---------------|---------------|
| Comedy | 5.80 | 1.50 |
| Broadway and Theater | 10.81 | 1.50 |
| Concert | 10.70 | 1.96 |
| Festivals | 11.16 | 2.99 |
| Minor/Other Sports | 11.46 | 4.13 |
| **Major Sports** | **48.44** | **19.19** |

Comedy is the most predictable: small venues, consistent audiences, tight price ranges. The model is reliable enough here for automated signals with minimal human review.

Major Sports is a different case. The 48-ticket RMSE is not a modeling failure, it is a feature gap. The model has no information about whether a game is a playoff matchup, a rivalry game, or nationally televised. A regular-season Tuesday night game and a must-win playoff game look identical in the current feature set, but their demand profiles are completely different. Fixing this requires new features like team standings, playoff probability, opponent quality, and broadcast schedule. Architecture changes alone will not help.

For primary pricing teams managing NBA, NFL, or MLB events, the secondary market signal from this model is a reasonable starting point but needs enrichment with game-context features before it can drive automated decisions.

---

## Key Lessons

**1. Data preparation mattered more than algorithm selection.**
Log-transforming the sales target and imputing missing venue sizes improved every model tested. The choice between XGBoost, LightGBM, and CatBoost had smaller effects than getting the inputs right.

**2. The biggest gain in the project was not a model change.**
Moving the classifier threshold from 0.50 to 0.90 delivered more RMSE improvement than months of hyperparameter tuning. In a production pricing system, how conservatively the first stage flags events is as important as how accurately the second stage predicts. Fewer, higher-confidence outputs build more trust than high-volume noisy alerts.

**3. Simpler models trained on all the data beat complex models trained on subsets.**
When one model saw 6 million rows and a category-specific model saw 17,000, the full-data model generalized better even though the smaller model should have had more targeted signal. More data consistently beat more specialization.

**4. Evaluating on multiple time windows prevented bad conclusions.**
Segment-specific models looked promising on a single test window. Cross-validation across three windows showed those gains did not hold up. One time period is not enough evidence to justify a more complex system.

**5. The secondary market moves before the primary market does.**
Floor prices compress and secondary velocity picks up before any official primary pricing action. A primary pricing team monitoring this in real time has an early-warning signal based on actual market behavior rather than manual observation or lagging reports.

**6. Market signals already encode identity for concerts; they do not for sports.**
Artist identity in the concert market is reflected in price levels and listing counts. The model reads those signals directly. For sports, team identity carries additional context that market signals do not capture: franchise history, current season standing, opponent quality. Semantic embeddings from Wikipedia text helped sports predictions but hurt concert predictions, which points to where explicit contextual features would add the most value.

---

## What Would Come Next

The most valuable extensions for a primary pricing use case:

- **Sports context features**: team standings, playoff probability, opponent strength, broadcast schedule. These would likely cut Major Sports RMSE significantly and make the signal actionable for teams managing NBA, NFL, and MLB events. The embedding experiment confirmed that sports predictions benefit from identity-level context; the next step is structured features rather than text embeddings.
- **Category-specific thresholds**: The 0.90 threshold was tuned globally. Comedy and Broadway may work better at a lower threshold since false positives are less costly. Major Sports may need a higher threshold to reduce noise.
- **Primary price data**: Adding the original primary ticket price to the feature set would close the feedback loop. The model would know whether primary pricing was aggressive or conservative before any secondary activity occurred.
- **Improved embedding pipeline**: The current entity embeddings mix two incompatible coordinate systems (Wikipedia embeddings are PCA-projected; Last.fm fallback embeddings are raw-truncated). Re-running the pipeline so all entities go through the same standardization and PCA fit would remove that inconsistency and may improve the sports gains seen in the ablation.

---

Try the live prediction interface at [event-explorer.streamlit.app](https://event-explorer.streamlit.app).*
