---
layout: single
title: "SeatData.io Part 8: Segmenting Event Categories"
date: 2026-02-05
description: "Testing whether segment-specific models outperform a unified approach"
author_profile: true
toc: true
toc_sticky: true
tags:
  - segmentation
  - model architecture
  - cross-validation
  - bootstrap
  - XGBoost
excerpt: "Training separate models for each event category and ran statistical tests to see if they beat the unified model"
published: false
---

<img width="600" height="600" alt="image" src="https://github.com/user-attachments/assets/af03e2bb-cac8-4e91-8285-b24053599f0f" />


## Overview

The unified model from earlier posts predicts 7-day ticket sales across all event categories (NBA games, concerts, theater, comedy shows, etc) using one model. A natural question is whether training separate models for each category would do better, since these categories behave differently.

This will try to answer that question. I used 3-fold time-based cross-validation, 1000-resample bootstrap confidence intervals, and residual distribution tests to evaluate whether segmentation is worth the added complexity.

**Summary:** On the held-out test set, dedicated models improved predictions for several segments. But in 3-fold cross-validation evaluated on the final 14 days of each fold, the unified model won or tied everywhere. The test-set improvements appear to be period-specific and do not generalize.

---

## Setup

### Two-Stage Pipeline

All models use the same architecture: a two-stage pipeline.

- **Stage 1 (Classification):** Predict whether any sales will occur in the next 7 days (binary). I use PR-AUC as the primary metric here since most snapshots have zero sales.
- **Stage 2 (Regression):** Given that sales are predicted to occur, estimate how many. I use a log-transformed target and evaluate in original ticket scale (end-to-end RMSE).

For segment models, I train a separate classifier and regressor on data from that category only. All classifiers and regressors are XGBoost, using the Optuna-tuned hyperparameters found in Part 7 as the starting configuration for the unified model.

### Segments Tested

| Segment | Train Rows | Test Rows |
|---------|------------|-----------|
| Concert | 1,903,137 | 15,818 |
| Broadway & Theater | 1,383,917 | 18,045 |
| Other | 663,217 | 9,225 |
| Major Sports | 543,514 | 5,909 |
| Comedy | 463,750 | 6,272 |
| Minor/Other Sports | 139,755 | 2,596 |
| Festivals | 17,223 | 157 |

### Evaluation

The primary metric is **end-to-end RMSE in tickets**, the average prediction error across test rows where **actual sales occurred** (events with confirmed ticket sales in the test window, ~16,900 rows out of 58,022 total).

> **Comparison note:** This metric differs from the **18.53 full-test-set RMSE** reported in Part 7 (Optuna). That figure covers all 58,022 test events, including the ~71% with zero sales where predicting zero is trivially correct. The numbers here are on the harder active-event subset — rows where tickets actually sold. The unified baseline is the Optuna-tuned XGBoost pipeline from Part 7 (classifier + regressor, threshold 0.90), evaluated segment by segment. Segment-specific models are the same XGBoost architecture retrained on category-only data.

---

## Test Set Results

### How Each Segment Performs

*[Figure 1: Bar chart - Unified RMSE vs Segment RMSE by category, in tickets]*

| Segment | Unified RMSE | Segment RMSE | Improvement |
|---------|-------------|-------------|-------------|
| Major Sports | 53.98 tickets | 53.96 tickets | +0.02 (negligible) |
| Festivals | 22.61 tickets | 14.61 tickets | **+8.00 (+35%)** |
| Concert | 19.34 tickets | 18.21 tickets | **+1.12 (+6%)** |
| Other | 27.30 tickets | 25.77 tickets | **+1.52 (+6%)** |
| Broadway & Theater | 20.71 tickets | 20.68 tickets | +0.04 (negligible) |
| Comedy | 10.12 tickets | 9.24 tickets | **+0.88 (+9%)** |
| Minor/Other Sports | 14.65 tickets | 15.24 tickets | -0.59 (worse) |

On the test set, segment-specific models look appealing for Concert, Festivals, Comedy, and Other. The improvements for Major Sports and Broadway are essentially zero. Minor/Other Sports actually gets worse with a dedicated model.

---

## Statistical Significance: Bootstrap Tests

To check whether these improvements are real or just noise, we ran bootstrap significance tests with 1000 resamples per segment. We report the 95% confidence interval for the RMSE difference and a one-tailed p-value.

*[Figure 2: Forest plot - Bootstrap 95% CI for RMSE improvement by segment]*

| Segment | RMSE Improvement | 95% CI | p-value | Significant? |
|---------|-----------------|--------|---------|--------------|
| Festivals | +8.00 tickets | [+3.94, +12.33] | 0.000 | Yes |
| Other | +1.52 tickets | [+0.36, +2.80] | 0.003 | Yes |
| Concert | +1.12 tickets | [+0.35, +2.10] | 0.001 | Yes |
| Comedy | +0.88 tickets | [+0.41, +1.38] | 0.000 | Yes |
| Broadway | +0.04 tickets | [-0.62, +0.57] | 0.479 | No |
| Major Sports | +0.02 tickets | [-0.63, +0.68] | 0.466 | No |
| Minor/Other Sports | -0.59 tickets | [-1.19, -0.01] | 0.977 | No |

Yes, these were bootstrapped. Four segments show statistically significant improvements with CIs that exclude zero. At face value, this looks like segmentation works.

---

## Residual Tests: Is the Unified Model Systematically Biased?

We tested whether the unified model's prediction errors are distributed differently across segments.

**KS-test** (do residuals differ from a common distribution?): Significant for 6 of 7 segments (p < 0.01).

**Levene's test** (are residual variances equal across segments?): Statistic = 53.19, p < 0.0001.

This confirms the unified model has systematically different error patterns by segment. Concerts, Major Sports, and Festivals all have distinct residual distributions, which is an argument for segmentation in principle.

---

## Cross-Validation Tells a Different Story

Here is the key finding that undermines the test-set results.

The test set is one specific time window. To see whether segmentation holds up across different time periods, we ran 3-fold time-based cross-validation, training on earlier data and evaluating predictions on the final 14 days of each fold - the same prediction window as the test set.

*[Figure 3: CV comparison - Unified RMSE vs Segment RMSE per fold, by segment]*

| Segment | Unified CV RMSE | Segment CV RMSE | Result |
|---------|----------------|----------------|--------|
| Broadway & Theater | 6.75 ± 1.05 | 7.06 ± 0.88 | unified wins |
| Comedy | 10.74 ± 6.70 | 11.29 ± 7.16 | unified wins |
| Concert | 12.77 ± 0.51 | 12.92 ± 0.80 | tied |
| Festivals | 5.97 ± 1.89 | 7.91 ± 3.41 | unified wins |
| Major Sports | 16.13 ± 2.23 | 16.09 ± 2.69 | tied |
| Minor/Other Sports | 11.68 ± 4.37 | 12.23 ± 6.41 | unified wins |
| Other | 11.33 ± 0.17 | 11.30 ± 1.37 | tied |

In every segment, the unified model wins or ties. This directly contradicts the test-set results for Concert, Festivals, Comedy, and Other.

The interpretation: the segment-model improvements on the test set are specific to that time window. They do not hold up across different time periods. The segment models are picking up on patterns that were present in the test period but are not stable features of these categories.

---

## Feature Importance: Are the Segments Actually Different?

We measured how much each segment model's feature importances diverge from the unified model using L1 distance.

*[Figure 4: Feature importance heatmap - Unified vs per-segment top features]*

| Segment | Feature Divergence (L1) |
|---------|------------------------|
| Minor/Other Sports | 0.60 (high) |
| Festivals | 0.63 (high) |
| Major Sports | 0.34 (moderate) |
| Broadway & Theater | 0.33 (moderate) |
| Other | 0.21 (low-moderate) |
| Concert | 0.18 (low) |
| Comedy | 0.13 (low) |

The segments with the most divergent feature importances (Minor/Other Sports, Festivals) are not the same ones that benefit most consistently from segmentation. Festivals shows high divergence and a large test-set improvement, but the cross-validation shows it does worse with a dedicated model. This is consistent with overfitting: with only 17,223 training rows, the Festivals model fits patterns specific to the training window rather than learning stable signals.

The dominant feature in the unified model is `sales_total_7d_log_scaled` at 61.7% importance. When one feature drives most of the prediction, there is limited room for segment-specific patterns to change the outcome.

---

## The Major Sports Problem

Major Sports (NBA, NFL, MLB, NHL) has the highest RMSE at 53.98 tickets, and segmentation does nothing for it (improvement: 0.02 tickets). This is not a model architecture problem. It is a data problem.

High-stakes games - playoff games, rivalry matchups, nationally televised games - have sales volumes that no current feature captures. Without features like team standings, playoff probability, or opponent quality, the model is predicting blind for the most important events.

A dedicated Major Sports model with the same features as the unified model will not fix this. New features are required.

---

## Comedy: The Edge Case

Comedy is the one segment where there is a plausible structural argument for a dedicated model. The test-set improvement is +0.88 tickets (+9%) with a tight bootstrap CI ([+0.41, +1.38], p = 0.000). The feature importance divergence is low (0.13), which means the comedy model is not relying on fundamentally different signals - it is just calibrated differently.

However, cross-validation shows the unified model winning for Comedy (-0.54 tickets). Given the high variance across folds (10.74 ± 6.70 tickets for unified), this is not a reliable signal either way.

Comedy is a marginal case. The practical recommendation is to monitor it: if the segment model consistently outperforms the unified model in production over several months, it is worth maintaining.

---

## Why Segmentation Might Work Later

The core issue is that segmentation introduces more parameters to fit. With limited data, especially for Festivals and Minor/Other Sports, the segment models cannot generalize beyond the training window. A unified model benefits from seeing all events and learns more stable patterns.

With more data - specifically, more time periods and more events per segment - segment models would have a better chance of learning stable, generalizable patterns rather than fitting to the specific characteristics of any one time window.

The residual tests (Levene p < 0.0001) confirm there are real structural differences in how the unified model errors behave across segments. This is a theoretical basis for segmentation. But the cross-validation evidence says we do not yet have enough data to exploit it reliably.

---

## Conclusion

| Question | Answer |
|----------|--------|
| Do segment models beat unified on the test set? | Yes, for 4 of 7 segments |
| Are those improvements statistically significant? | Yes (bootstrapped, p < 0.01 for 4 segments) |
| Do they hold up in cross-validation? | No - unified wins or ties everywhere |
| Should we deploy segment-specific models? | Not yet |
| Why not? | Insufficient data for stable generalization |
| What would change this? | More time periods, more events per segment |
| What will actually help Major Sports? | New features (standings, playoff probability) |

The unified model is the more defensible choice at this stage. The improvements from segmentation appear real on the specific test period but do not generalize across time, which is what matters for production.

---

*[Appendix Figure: Learning curves by segment showing data-starved vs data-sufficient segments]*

*Analysis and model code in `04_segmentation_deep_dive.ipynb`. Part 9 will cover model diagnostics and production readiness.*
