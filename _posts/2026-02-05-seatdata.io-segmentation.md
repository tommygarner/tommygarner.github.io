---
layout: single
title: "SeatData.io Part 8: Segmenting Event Categories"
date: 2026-02-05
description: "Testing whether segment-specific models outperform a unified approach"
author_profile: true
toc: true
toc_sticky: true
tags:
  - segmentation
  - model architecture
  - cross-validation
  - bootstrap
  - confidence intervals
  - XGBoost
excerpt: "Training separate models for each event category and ran statistical tests to see if they beat the unified model"
published: true
---

<img width="600" height="600" alt="image" src="https://github.com/user-attachments/assets/39b1b1bb-114c-4f48-afa3-de0115ded05f" />


## Overview

The unified model from earlier posts predicts 7-day ticket sales across all event categories (NBA games, concerts, theater, comedy shows) using one model. A natural question is whether training separate models for each category would do better, since these categories behave differently.

I first compared segment-specific models against the unified model on the held-out test set, then validated those results with bootstrap confidence intervals, residual distribution tests, and 3-fold time-based cross-validation.

**Summary:** Segment models beat the unified model for several categories on the test set — but cross-validation tells a different story. Across all three folds, the unified model wins or ties everywhere. The test-set improvements appear to be period-specific and do not generalize.

---

## Setup

### The Two-Stage Pipeline

All models use the same architecture I've discussed before.

- **Stage 1 (Classification):** Predict whether any sales will occur in the next 7 days (binary). I use PR-AUC as the primary metric here since most snapshots have zero sales.
- **Stage 2 (Regression):** Given that sales are predicted to occur, estimate how many. I use a log-transformed target and evaluate in original ticket scale (end-to-end RMSE).

For segment models, I train a separate classifier and regressor on data from that category only. All classifiers and regressors are XGBoost, using the Optuna-tuned hyperparameters from Part 7 as the starting configuration for the unified model.

### Segments Tested

| Segment | Train Rows | Test Rows |
|---------|------------|-----------|
| Concert | 1,903,137 | 15,818 |
| Broadway & Theater | 1,383,917 | 18,045 |
| Other | 663,217 | 9,225 |
| Major Sports | 543,514 | 5,909 |
| Comedy | 463,750 | 6,272 |
| Minor/Other Sports | 139,755 | 2,596 |
| Festivals | 17,223 | 157 |

### Evaluation

The primary metric is **end-to-end RMSE in tickets**, the average prediction error across test rows where **actual sales occurred** (events with confirmed ticket sales in the test window, ~16,900 rows out of 58,022 total).

> **Comparison note:** This metric differs from the **18.53 full-test-set RMSE** reported in Part 7 (Optuna). That figure covers all 58,022 test events, including the ~71% with zero sales where predicting zero is trivially correct. The numbers here are on the harder active-event subset, rows where tickets actually sold. The unified baseline is the Optuna-tuned XGBoost pipeline from Part 7 (classifier + regressor, threshold 0.90), evaluated segment by segment. Segment-specific models are the same XGBoost architecture retrained on category-only data.
---

## Test Set Results

### Segment Results

<img width="2388" height="711" alt="image" src="https://github.com/user-attachments/assets/4a35a3ad-0d67-4dd7-9e22-eeab31c8c295" />

*Figure 1: Unified vs segmented XGBoost performance RMSE by category*

| Segment | Unified RMSE | Segment RMSE | Improvement |
|---------|-------------|-------------|-------------|
| Comedy | 9.66 tickets | 8.09 tickets | **+1.57 (+16%)** |
| Minor/Other Sports | 14.54 tickets | 14.86 tickets | -0.32 |
| Concert | 15.24 tickets | 15.24 tickets | +0.00 |
| Festivals | 16.39 tickets | 15.66 tickets | +0.73 (+4%) |
| Broadway & Theater | 17.94 tickets | 17.64 tickets | +0.30 (+2%) |
| Other | 24.46 tickets | 25.31 tickets | -0.85 (worse) |
| Major Sports | 53.97 tickets | 53.09 tickets | +0.88 (+2%) |

On the test set, Comedy clearly benefits from a segment model (+1.57 tickets, +16%). Festivals, Major Sports, and Broadway show small positive improvements. Concert is essentially tied. Minor/Other Sports and Other actually get worse with dedicated models.

This chart shows how disparate errors are between different categories. It is much harder to predict Major Sports since, as I saw in my EDA, the category has the most spread of sales. Interestingly enough, Comedy has the tightest predictions of all categories.

---

## Bootstrap Tests

To check whether these improvements are real or just noise, I ran bootstrap significance tests with 1000 resamples per segment. I report the 95% confidence interval for the RMSE difference and a one-tailed p-value.

<img width="1787" height="594" alt="image" src="https://github.com/user-attachments/assets/4c185556-121c-4163-998c-93d779f6629e" />

*Figure 2: Bootstrap 95% CI and distributions for RMSE improvement by segment*

| Segment | RMSE Improvement | 95% CI | p-value | Significant? |
|---------|-----------------|--------|---------|--------------|
| Comedy | +1.57 tickets | [+0.94, +2.26] | 0.000 | Yes (***) |
| Major Sports | +0.88 tickets | [-0.12, +1.84] | 0.040 | Marginal (**) |
| Festivals | +0.73 tickets | [-2.42, +4.17] | 0.321 | No |
| Broadway & Theater | +0.30 tickets | [-0.87, +1.24] | 0.285 | No |
| Concert | +0.00 tickets | [-0.63, +0.65] | 0.507 | No |
| Minor/Other Sports | -0.32 tickets | [-1.07, +0.35] | 0.808 | No |
| Other | -0.85 tickets | [-2.42, +0.76] | 0.836 | No |

Bootstrapping allowed me to sample with replacement of my data and predictions to get a more holistic view on distributions. Only Comedy shows a statistically significant improvement with a CI that excludes zero. Major Sports is marginal. At face value, the case for segmentation is much weaker than the test-set results first suggested.

---

## Residual Tests

I also tested whether the unified model's prediction errors are distributed differently across segments.

<img width="859" height="452" alt="image" src="https://github.com/user-attachments/assets/10bdf810-2377-4f9c-b8d3-1ecd83cbeb91" />

*Figure 3: Biases of residuals within each segment*

From the above plot, you can see that all predictions seem to underpredict sales volume except for the case of Festivals. Since I had so few Festival events in my data, it was difficult for my XGBoost trees to make generalized splits on the true patterns of the category.

**KS-test** (do residuals differ from a common distribution?): Significant for all segments! (p < 0.01)

<img width="866" height="462" alt="image" src="https://github.com/user-attachments/assets/8458ad51-9457-48d5-9cac-39ca0b155a3c" />

*Figure 4: KS-test visualized*

In fact, all events have different residual distributions than each other. This would suggest that segmentation might be meaningful, however my initial bootstrap results challenge this idea. This might be simply due to the lack of data, unfortunately.

**Levene's test** (are residual variances equal across segments?): Statistic = 53.19, p < 0.0001.

This confirms the unified model has systematically different error patterns by segment. Concerts, Major Sports, and Festivals all have distinct residual distributions, which is an argument for segmentation in principle.

---

## Feature Importance

Before asking whether segment models generalize, it helps to ask whether segments are even learning different things. I measured how much each segment model's feature importances diverge from the unified model using L1 distance.

| Segment | Feature Divergence (L1) |
|---------|------------------------|
| Festivals | 0.63 (high) |
| Minor/Other Sports | 0.60 (high) |
| Major Sports | 0.34 (moderate) |
| Broadway & Theater | 0.33 (moderate) |
| Other | 0.21 (low-moderate) |
| Concert | 0.18 (low) |
| Comedy | 0.13 (low) |

The dominant feature in the unified model is `sales_total_7d_log_scaled` at 61.7% importance. When one feature drives most of the prediction, there is limited room for segment-specific patterns to change the outcome.

Notably, the segments with the most divergent feature importances (Festivals, Minor/Other Sports) are not the ones that benefit most from segmentation. Comedy, the only segment with a statistically significant test-set improvement, has the *lowest* divergence (0.13), meaning its segment model isn't learning fundamentally different signals from the unified model. It's just calibrated differently on a more homogeneous slice of data, where most events behave similar to each other on StubHub.

---

## Cross-Validation

Here is the key finding that undermines the test-set results.

The test set is one specific time window. To see whether segmentation holds up across different time periods, I ran 3-fold time-based cross-validation, training on earlier data and evaluating predictions on the final 14 days of each fold, the same prediction window as the test set.

| Segment | Unified CV RMSE | Segment CV RMSE | Result |
|---------|----------------|----------------|--------|
| Broadway & Theater | 7.64 ± 1.74 | 8.78 ± 0.95 | unified wins |
| Comedy | 10.62 ± 5.23 | 11.86 ± 5.86 | unified wins |
| Concert | 12.38 ± 1.27 | 12.63 ± 1.29 | tied |
| Festivals | 8.20 ± 4.65 | 14.33 ± 10.25 | unified wins |
| Major Sports | 15.28 ± 2.60 | 15.52 ± 2.49 | tied |
| Minor/Other Sports | 11.07 ± 0.79 | 11.91 ± 3.79 | unified wins |
| Other | 10.20 ± 0.10 | 10.96 ± 1.43 | unified wins |

In every segment, the unified model wins or ties. This directly contradicts the test-set results for Comedy, Festivals, Major Sports, and Broadway. Festivals is the starkest reversal: a +0.73 test-set improvement collapses to a -6.13 CV gap (unified 8.20 vs segment 14.33), a clear sign of overfitting on a 17K-row dataset.

The segment-model improvements on the test set are specific to that time window. They do not hold up across different time periods, since the segment models are picking up on patterns present in the test period but not stable features of these categories.

---

## The Major Sports Problem

Major Sports (NBA, NFL, MLB, NHL) has the highest RMSE at 53.97 tickets. The segment model shaves off 0.88 tickets, which is marginal relative to the overall error magnitude, and not statistically reliable (p = 0.040, CI crossing zero). This is not a model architecture problem. It is a data problem.

High-stakes games like playoff games, rivalry matchups, and nationally televised games have sales volumes that no current feature captures. Without features like team standings, playoff probability, or opponent quality, the model is predicting blind for the most important events.

A dedicated Major Sports model with the same features as the unified model will not fix this. New features are required.

---

## Comedy

Comedy is the one segment where there is a plausible argument for a dedicated model. The test-set improvement is +1.57 tickets (+16%) with a tight bootstrap CI ([+0.94, +2.26], p = 0.000) and is the only segment that clears the significance bar cleanly. The feature importance divergence is low (0.13), which means the comedy model is not relying on fundamentally different signals, it is just calibrated differently.

However, cross-validation shows the unified model winning for Comedy by 1.24 tickets (10.62 vs 11.86). Given the high variance across folds (10.62 ± 5.23 tickets for unified), this is not a reliable signal either way.

So, Comedy is a marginal case. The practical recommendation is to monitor it: if the segment model consistently outperforms the unified model in production over several months, it is worth maintaining.

---

## Why Segmentation Might Work Later

The core issue is that segmentation introduces more parameters to fit. With limited data, especially for Festivals and Minor/Other Sports, the segment models cannot generalize beyond the training window. A unified model benefits from seeing all events and learns more stable patterns.

With more data like more time periods and more events per segment, segment models would have a better chance of learning stable, generalizable patterns rather than fitting to the specific characteristics of any one time window.

The residual tests (Levene p < 0.0001) confirm there are real structural differences in how the unified model errors behave across segments. This is a theoretical basis for segmentation. But the cross-validation evidence says we do not yet have enough data to exploit it reliably.

---

## Conclusion

| Question | Answer |
|----------|--------|
| Do segment models beat unified on the test set? | Yes, for 4 of 7 segments |
| Are those improvements statistically significant? | Only Comedy (p < 0.001); Major Sports marginal (p = 0.04) |
| Do they hold up in cross-validation? | No - unified wins or ties everywhere |
| Should we deploy segment-specific models? | Not yet |
| Why not? | Insufficient data for stable generalization |
| What would change this? | More time periods, more events per segment |
| What will actually help Major Sports? | New features (standings, playoff probability) |

The unified model is the more defensible choice at this stage. The improvements from segmentation appear real on the specific test period but do not generalize across time, which is what matters for production.
