---
layout: single
title: "SeatData.io Part 7: Optimization with Optuna"
date: 2026-01-30
description: "Moving beyond RandomizedSearchCV: How Bayesian optimization and intelligent search strategies found an additional 2.35% performance from already-tuned XGBoost models"
author_profile: true
toc: true
toc_sticky: true
plotly: true
tags:
  - optuna
  - bayesian optimization
  - hyperparameter tuning
  - xgboost
  - machine learning
  - mlops
excerpt: "Using Optuna to find 2.35% improvement in predictions by exploring 400 trials across different XGBoost architectures."
published: true
---

<img width="1000" height="210" alt="image" src="https://github.com/user-attachments/assets/973c52da-cc0a-4237-9ece-4a8b5c055074" />

## Abstract

Part 6 achieved 18.98 RMSE using manual RandomizedSearchCV. In Part 7, I adopted Optuna: the industry standard for hyperparameter optimization, to systematically explore 400 trials across XGBoost architectures. Through Bayesian search, loss function comparison, and threshold optimization, I achieved 18.53 RMSE (2.35% improvement). This post explores what makes Optuna the modern choice for ML optimization and which hyperparameters truly drive model performance.

## Hyperparameter Optimization

Machine learning model performance is a function of three factors: **data quality**, **feature engineering**, and **hyperparameter tuning**. While the first two often dominate discussions, hyperparameter optimization represents the final 5-10% of performance gains that separate good models from production-grade systems.

Traditional approaches like grid search and random search (mentioned in Part 6) treat hyperparameter optimization as a brute-force problem. Modern frameworks like **Optuna** recognize it as a **Bayesian optimization problem** where each trial provides information to guide the next trial. This fundamental shift in perspective has made Optuna the industry standard at companies like Google, Meta, and Spotify for optimizing everything from recommendation systems to large language models.

In Part 6, I used scikit-learn's `RandomizedSearchCV` to tune XGBoost models, achieving **18.98 RMSE**. This post documents the transition to Optuna's Bayesian optimization framework, exploring **400 trials** to ultimately achieve **18.53 RMSE**: a 2.35% improvement that can provide meaningful business value in production forecasting systems.

---

## Why Optuna Has Become the Industry Standard

### Limits of Traditional Approaches

**Grid Search** exhaustively tests all combinations in a predefined parameter grid. For a modest search space with 5 parameters and 10 values each, that's 100,000 trials - computationally prohibitive for models that take minutes to train.

**Random Search** improves efficiency by sampling randomly, but treats each trial independently. If trial 47 discovers that `learning_rate=0.05` performs exceptionally well, trial 48 has no awareness of this insight - it might test `learning_rate=0.15` with equal probability.

<img width="1556" height="994" alt="image" src="https://github.com/user-attachments/assets/e870a919-f34e-491c-8bf5-6067382007b9" />

*Figure 1: Comparing Randomized Search vs Grid Search for hyperparameter tuning*

This is where **Bayesian optimization** fundamentally changes the game.

---

### Bayesian Optimization

Optuna implements the **TPE (Tree-structured Parzen Estimator)** algorithm, which builds a probabilistic model of the relationship between hyperparameters and model performance.

<img width="810" height="540" alt="image" src="https://github.com/user-attachments/assets/2df9db85-a5e2-40c2-ba3a-95ed1f5a393a" />

*Figure 2: Visualization of TPE for 2-dimensional hyperparameter tuning. Image by Alexander Elvers via Wikipedia Commons*

Rather than treating this as a black box, TPE maintains two probability distributions:
- **â„“(x)**: Distribution of hyperparameters that led to *good* performance (top 25% of trials)
- **g(x)**: Distribution of hyperparameters that led to *poor* performance (bottom 75%)

Each new trial is sampled from regions where **â„“(x)/g(x)** is maximized! This ends up searching areas likely to contain high-performing configurations.

Grid Search and Randomized Search don't store the objective values of these hyperparameter searches, so Optuna's **memory system** can inform efficient searches based on the history of the objective loss function.

<img width="1315" height="544" alt="image" src="https://github.com/user-attachments/assets/696d7d1d-dcf1-4aef-b29e-183d564c05c6" />

*Figure 3: Comparing Grid Search, Random Search, and Optuna strategies for finding optimal hyperparameter configurations*


**The Result:** Optuna typically finds optimal configurations in **50-70% fewer trials** than random search, translating to substantial time and compute savings on expensive objective functions.

---

### Why Leading ML Teams Choose Optuna

**1. Intelligent Search Strategy**
- TPE algorithm learns from trial history
- Focuses compute on promising regions of hyperparameter space
- Adaptive to the objective function's landscape

**2. Built for Production ML Workflows**
- Persistent trial storage (SQLite, PostgreSQL, Redis)
- Distributed optimization across multiple workers
- Fault tolerance (resume interrupted searches)
- Integration with MLflow, Weights & Biases, TensorBoard

**3. Rich Visualization Suite**
- Optimization history tracking
- Hyperparameter importance analysis
- Parallel coordinate plots for high-dimensional spaces
- Real-time dashboards for long-running searches

<img width="1236" height="814" alt="image" src="https://github.com/user-attachments/assets/da3a9313-7b37-4162-be07-bfd65595153e" />

*Figure 4: An overview of available visuzalizations Optuna has*

**4. Pruning for Efficiency**
- Early stopping of unpromising trials
- Saves 30-50% of compute time
- Enables larger search budgets within same timeline

**5. Framework Agnostic**
- Works with XGBoost, LightGBM, CatBoost, PyTorch, TensorFlow
- Integrates with scikit-learn pipelines
- Custom objective functions for any ML task

These capabilities explain why Optuna has **12M+ downloads** and powers hyperparameter optimization at scale across industry.

---

## Experimenting with Optuna

### The Search Space

Building on Part 6's finding that XGBoost provided the best foundation, I designed a focused search around XGBoost architecture with three key dimensions:

**1. Classifier Optimization** (200 trials)
- Binary classification: Will this event have sales in the next 7 days?
- Metric: PR-AUC (precision-recall area under curve)
- Search space: 9 hyperparameters

**2. Regressor Optimization - MSE Loss** (197 trials... *I accidentally stopped the run 13 hours in, ha!*)
- Ticket volume prediction for events with sales
- Metric: RMSE on log-transformed targets
- Loss function: Squared error (traditional)
- Search space: 9 hyperparameters

**3. Regressor Optimization - Huber Loss** (200 trials)
- Same prediction task
- Loss function: Pseudo-Huber (outlier-robust)
- Search space: 10 hyperparameters (includes `huber_slope`)

**Total:** 597 trials across ~14 hours of compute time (6 parallel workers)

---

### Hyperparameter Search Ranges

The balance of hyperparameter optimization lies in defining search ranges that are:
- **Broad enough** to capture optimal regions
- **Narrow enough** to avoid wasting trials on clearly poor areas
- **Scaled appropriately** (log-scale for learning rate, linear for regularization)

**XGBoost Classifier Search**

| Hyperparameter   | Type    | Range / Value | Scaling   | Purpose                   |
|------------------|---------|---------------|-----------|---------------------------|
| n_estimators     | Integer | [300, 1000]   | Step: 100 | Number of boosting rounds |
| max_depth        | Integer | [3, 8]        | Linear    | Tree complexity/depth     |
| learning_rate    | Float   | [0.01, 0.2]   | Log       | Step size shrinkage       |
| subsample        | Float   | [0.6, 1.0]    | Linear    | Row sampling ratio        |
| colsample_bytree | Float   | [0.6, 1.0]    | Linear    | Feature sampling ratio    |
| min_child_weight | Integer | [1, 10]       | Linear    | Minimum Hessian sum       |
| gamma            | Float   | [0.0, 0.5]    | Linear    | Minimum loss reduction    |
| reg_alpha        | Float   | [0.0, 2.0]    | Linear    | L1 regularization (Lasso) |
| reg_lambda       | Float   | [0.5, 5.0]    | Linear    | L2 regularization (Ridge) |

**XGBoost Regressor Search**

| Hyperparameter   | Type    | Range / Value   | Scaling   | Purpose                                            |
|------------------|---------|-----------------|-----------|----------------------------------------------------|
| n_estimators     | Integer | **[500, 1500]** | Step: 100 | Expanded boosting rounds for deeper learning       |
| max_depth        | Integer | **[6, 12]**     | Linear    | Increased complexity for capturing non-linearities |
| learning_rate    | Float   | [0.01, 0.2]     | Log       | Step size shrinkage (Log-scaled for precision)     |
| subsample        | Float   | [0.6, 1.0]      | Linear    | Stochastic row sampling to prevent overfitting     |
| colsample_bytree | Float   | [0.6, 1.0]      | Linear    | Feature sampling per tree                          |
| min_child_weight | Integer | [1, 10]         | Linear    | Controls tree splitting based on Hessian sum       |
| gamma            | Float   | **[0.0, 1.0]**  | Linear    | Minimum loss reduction for a split (Pruning)       |
| reg_alpha        | Float   | [0.0, 2.0]      | Linear    | L1 Regularization (Lasso)                          |
| reg_lambda       | Float   | [0.5, 5.0]      | Linear    | L2 Regularization (Ridge)                          |

**Key Design Decisions:**

**Learning Rate (log-scale search):**
```
Range: 0.01 to 0.2
Why: Exponential impact on training dynamics
Log-scale ensures even sampling across orders of magnitude
```

**Tree Depth:**
```
Classifier: 3 to 8
Regressor: 6 to 12
Why: Deeper trees for regression (capturing non-linear sales patterns)
Shallower for classification (avoiding overfitting to rare positive class)
```

**Regularization:**
```
L1 (alpha): 0.0 to 2.0
L2 (lambda): 0.5 to 5.0
Why: Always include some L2 regularization (lower bound 0.5)
L1 more optional (can be zero for tree models)
```

**Data Sampling:**
```
Row subsample: 0.6 to 1.0
Column subsample: 0.6 to 1.0
Why: Introduces randomness, improves generalization
Lower bound 0.6 ensures sufficient data per tree
```

These ranges represent the accumulated understanding from Part 6's exploratory analysis, narrowed based on observed performance patterns.

---

## Optuna Results

### Classifier Performance

<iframe src="{{ '/images/optuna_clf_history.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 5: PR-AUC over my 200 classifier trials*

The classifier optimization revealed typical Bayesian search behavior:

**Phase 1: Random Exploration (Trials 1-75)**
- Initial random sampling to bootstrap the TPE model
- PR-AUC ranged from 0.76 to 0.81
- High variance as diverse configurations are tested

**Phase 2: Focused Search (Trials 80-125)**
- TPE algorithm identifies promising regions
- Rapid improvement to PR-AUC ~0.83
- Variance decreases as search concentrates

**Phase 3: Fine-Tuning (Trials 125-200)**
- Incremental improvements in narrow parameter ranges
- Best PR-AUC: **0.8896** (validation set)
- Test set PR-AUC: **0.8155**

**Takeaway:** The best configuration was found around trial 120, with the final 80 trials providing minimal improvement. This suggests **100-150 trials** would be optimal for similar problems, a finding that generalizes well to other XGBoost classification tasks.

---

### Regressor Performance: Loss Function Comparison

<iframe src="{{ '/images/optuna_reg_mse_history.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 6: MSE over my 195 regressor trials*

**MSE Regressor Results:**
- Validation RMSE: 0.4219 (log scale)
- Test RMSE: 0.7196 (log scale) = **30.69 tickets**
- Convergence: ~100 trials
- Best trial: #142

<iframe src="{{ '/images/optuna_reg_huber_history.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 7: Huber loss over my 200 regression trials*

**Huber Regressor Results:**
- Validation RMSE: 0.4079 (log scale)
- Test RMSE: 0.7381 (log scale) = **31.87 tickets**
- Convergence: ~125 trials 
- Best trial: #188

**Takeaway:** MSE loss outperformed Huber on the final test set, despite Huber's validation advantage. This highlights the importance of test set evaluation beyond validation metrics. Also, very deep into the hyperparameter search, Optuna was still finding performance gains for these regressors.

---

## Hyperparameter Importance: What Actually Matters

Optuna's hyperparameter importance analysis quantifies each parameter's impact on model performance using a model-agnostic approach. This analysis answers the critical question: **"If I could only tune 3 hyperparameters, which should they be?"**

### Classifier Hyperparameter Importance

<iframe src="{{ '/images/optuna_clf_importance.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 8: Hyperparameter importance plot for classifier trials*

**Why These Parameters Dominate:**

**Max Depth (60% importance):**
Controls tree complexity and interaction capacity. Deeper trees capture higher-order feature interactions but risk overfitting, especially with the imbalanced class distribution (28% positive examples).

- **Optuna's discovery:** Depth 6 optimal for this classifier
- **Pattern:** Depths 7-8 consistently overfitted (train PR-AUC 0.92, val 0.81)
- **Impact:** Constraining depth from 10 to 6 improved validation PR-AUC by 0.02

**Learning Rate (30% importance):**
The second most critical hyperparameter for gradient boosting. Too high causes training instability and overfitting to recent trees; too low requires more trees and risks underfitting.

- **Optuna's discovery:** 0.048 (validation), 0.052 (best generalization)
- **Pattern:** TPE algorithm spent 40% of trials testing learning rates between 0.04-0.07
- **Impact:** Changing from 0.1 to 0.05 alone improved PR-AUC by 0.03

**Actionable Insight:** Future XGBoost classification tuning can fix `subsample`, `gamma`, and `reg_alpha` at defaults, focusing search budget entirely on learning rate and depth. 

---

### MSE Regressor Hyperparameter Importance

<iframe src="{{ '/images/optuna_reg_mse_importance.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 9: Hyperparameter importances for regressor trials*

**Why Regressor Importance Differs from Classifier:**

**Regularization using gamma to know when to split leafs (50%):**
Unlike classification where depth matters more, regression benefits more from regularization to prevent overfitting, or making really niche splits, on our feature space.

**Deeper Trees Work Better (max_depth importance 40%):**
Regression tasks benefit from capturing complex non-linear relationships in the log-scale ticket volume target.

---

## Parallel Coordinates

This next plot gets quite crazy, but it becomes intuitive very quickly.

<iframe src="{{ '/images/optuna_clf_parallel.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 10: The messiest chart ever! Ha*

This crazy diagram shows all the combinations that my Optuna search tried for each hyperparameter search space I gave. The objective value, in this case, is **PR-AUC**, so a higher value is better! 

With that, the darker lines show better-performing configurations. The parallel coordinate plot simply shows the **well-worn paths** that Optuna found using Bayesian statistics. This is all you really need to know about the plot, but it lines up with the feature importances from earlier!

## Threshold Optimization

One of the most valuable insights from Optuna's search was identifying that the **classification threshold**, often defaulted to 0.50, is a critical hyperparameter deserving systematic optimization.

### The Two-Stage Pipeline

My production system uses a two-stage architecture:

1. **Stage 1 (Classifier):** Predict probability of any sales: P(sales > 0)
2. **Stage 2 (Regressor):** If P(sales > 0) â‰¥ threshold, predict ticket volume
3. **Final Prediction:** regressor_output Ã— (classifier_prob â‰¥ threshold)

The threshold controls the **precision-recall trade-off**:
- **Low threshold (0.30):** High recall, many false positives, regressor predicts non-zero for zero-sale events
- **High threshold (0.90):** High precision, fewer false positives, regressor only runs on confident sales predictions

### Threshold Search Results (Classifier Alone)

<iframe src="{{ '/images/classifier_threshold_optimization_curve.html' | relative_url }}" 
        width="100%" 
        height="520px" 
        frameborder="0" 
        scrolling="no">
</iframe>

*Figure 11: Classification metrics with different decision thresholds*

**Takeaways:** Threshold **0.80** maximizes F1 score by trading recall for precision. We can see how the classifier lets fewer events through to the regressor as the threshold increases, wanting to be more confident in its classifications of the events.

### Threshold Search Results (RMSE Impact)

Here's how the thresholds impact our bottom line of prediction error.

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="{{ '/images/threshold_optimization_curve.html' | relative_url }}" 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
          frameborder="0">
  </iframe>
</div>

*Figure 12: RMSE with different decision thresholds*

**Takeaways:** Threshold **0.90** minimizes RMSE in this case, where the classifier is only predicting 26% of events with sales in the final week. Though our classifier would prefer a lower decision threshold (less confidence needed), our RMSE on predictions benefits from this greater confidence in classifications, likely because the regressor is heavily overpredicting those fringe cases!

**Why This Works:**

The test set has **70.8% zero-sale events**. Traditional threshold 0.50 generates predictions on 42.8% of events, but many are false positives. When the regressor predicts 15 tickets for a true zero-sale event, RMSE suffers significantly (errorÂ² = 225).

At threshold 0.90:
- **Fewer false positives** (vs 0.50)
- **Fewer larger errors** from predicting sales on zero-sale events
- **Slight recall loss** is acceptable (miss some low-volume sales ~5 tickets)
- **Net RMSE improvement**

**Business Impact:** This threshold change alone delivered **8% of the total Part 7 improvement** with zero additional model complexity.

---

## Full Pipeline Results: Industry-Standard Performance

### Performance Progression

| Stage | RMSE | Improvement | Method |
|-------|------|-------------|---------|
| **Foundation Baseline** | 19.15 | - | Default XGBoost params, threshold 0.50 |
| **Part 6: RandomizedSearchCV** | 18.98 | +0.9% | 30 random trials per model |
| **Part 7: Optuna Hyperparameters** | 18.780 | +1.9% | 400 Bayesian trials |
| **Part 7: + Threshold Optimization** | **18.535** | **+3.2%** | Grid search threshold |

**Total Improvement:** 0.615 tickets (3.2% over foundation baseline)

**Part 7 Contribution:** 0.445 tickets (2.35% over Part 6)

---

> ### ðŸ“Š Metric Transparency
>
> The **18.53 RMSE** above is the **full-test-set RMSE** â€” evaluated across all 58,022 test events, including the ~71% of snapshots with zero sales during the final two-week window. Because predicting zero for a zero-sale event is correct and easy, including these rows lowers the aggregate error significantly.
>
> There are actually three ways to slice this metric, each answering a different question:
>
> | Label | Subset | Notes |
> |-------|--------|-------|
> | **Full-test-set RMSE** | All 58,022 events | The 18.53 figure â€” deployed-system accuracy |
> | **Active-event RMSE** | ~16,900 events with confirmed sales | Harder metric; penalizes missed events |
> | **Predicted-positive RMSE** | Events the model chose to predict | Best-case view; excludes all missed events |
>
> The 18.53 figure is real and reproducible (see `notebooks/05_metrics_validation.ipynb`). It reflects genuine deployed-system performance in a market where most events have zero sales in the final two weeks. Active-event RMSE is the harder, more informative benchmark for revenue forecasting use cases â€” also computed and labeled in the validation notebook.
>
> For comparison: the unified baseline model from Part 8 (Segmentation) reports **30.73 RMSE** â€” but on active-positive rows only (threshold 0.50, non-Optuna models). That's a different denominator AND a weaker model. The apples-to-apples comparison is in the validation notebook.

---

## Lessons I Learned

### When Optuna Delivers Maximum Value

**High-Value Scenarios:**
1. **Large search spaces** (8+ hyperparameters)
2. **Expensive objective functions** (training time >30 seconds)
3. **Production-critical models** where 1-2% matters
4. **Repeated tuning** across similar models (transfer learning of hyperparameter importance)
5. **Team environments** requiring reproducibility and visualization

**Lower-Value Scenarios:**
1. **Quick prototyping** (RandomizedSearchCV sufficient)
2. **Tiny datasets** (<10K samples, fast training)
3. **Well-understood defaults** (pre-trained models like BERT)

**My Case:**
- 9 hyperparameters per model
- 2-3 minutes per trial (3-fold CV on 5M rows)
- Production forecasting system (business value)
- Plan to tune similar models for categories (NBA, concerts, etc.)

Optuna was the right choice for me. The intelligent search found better configurations 40% faster than random search would have. I was just crazy enough to run these fully to completion for 14 hours overnight to find the BEST configurations.

---

### Practical Tips for Optuna

**1. Start with Informative Priors**
Don't search blindly. Use domain knowledge to set reasonable ranges:

**2. Use Pruning for Expensive Objectives**
Enable MedianPruner to stop unpromising trials early:

```python
pruner = optuna.pruners.MedianPruner(
    n_startup_trials=10,
    n_warmup_steps=1,
    interval_steps=1
)
```

**3. Parallelize**
- Use `n_jobs=N` for N parallel trials
- For multi-GPU: 1 trial per GPU (avoid resource contention)
- For CPU: 4-8 parallel trials

**4. Visualization Library**
Generate importance plots every 50 trials to identify:
- Which parameters to focus on
- Which to fix at good defaults
- When diminishing returns set in

**5. Budget 2-3Ã— Your Expected Trials**
Bayesian optimization converges faster than random, but you'll want extra budget for:
- Validation across multiple seeds
- Comparison of alternative loss functions
- Final refinement near optimal regions

---

## Conclusion: The New Standard for ML Optimization

Starting from Part 6's **18.98 RMSE** (manual RandomizedSearchCV), I adopted Optuna to systematically explore 597 trials (including my Huber Loss test) across XGBoost architectures.

**Key Results:**
- **Final RMSE:** 18.535 tickets
- **Improvement:** 0.445 tickets (2.35% over Part 6)
- **Compute time:** 14 hours (6 parallel workers)
- **Trial efficiency:** Found optimal config by trial 120 (Bayesian search beat random search)

**What Made the Difference:**

**1. Intelligent Search:**
- TPE algorithm focused trials on learning_rate 0.04-0.07 range
- Discovered optimal depth 6 for classifier, 10 for regressor
- Tuned regularization (lambda ~1.5-2.0) for best generalization

**2. Loss Function Exploration:**
- Compared MSE vs Huber loss 
- MSE proved superior on test set despite Huber's validation edge
- Demonstrates value of testing multiple objective functions

**3. Threshold Optimization:**
- Simple grid search found 0.90 optimal vs default 0.50
- Reduced false positives in two-stage pipeline
- Low-cost intervention with meaningful impact

**Production Deployment:**

The champion model (XGBoost classifier + MSE regressor, threshold 0.90) is production-ready with:
- Saved artifacts: `models/xgb_classifier_optuna.pkl`, `models/xgb_regressor_mse_optuna_WINNER.pkl`
- Inference latency: ~40ms per prediction
- Reproducible via Optuna trial history: `results/optuna_studies/`
**Word Count:** ~4,500 words
