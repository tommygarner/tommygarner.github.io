---
layout: single
title: "SeatData.io Part 2: Exploratory Data Analysis"
date: 2026-01-08
description: "Visualizing 6 Million rows of Secondary Market Data to Understand my SeatData.io Snapshots and Resale Trends"
author_profile: true
toc: true
toc_sticky: true
tags:
  - EDA
  - data visualization
  - python
  - matplotlib
  - market analysis
excerpt: "Investigating the patterns of 114,000+ events to identify market-specific sales curves and validate data integrity before modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/72b9abfb-60b3-4d37-b81e-2257279bfb76" />

## Abstract
Now it was time to explore my data. Following the construction of my BigQuery warehouse in Part 1, I conducted an Exploratory Data Analysis (EDA) on over 5.7 million records to understand the volatility of the secondary ticket market. This article explores the relationship between lead time and price floors, the distinct sales curves of different event categories, and observed shocks, like Election Day, that impact discretionary spend on tickets. By understanding these signals, I validated feature engineering choices required for later demand forecasting.

## Key Insights


---

## 1. Data Integrity
After performing a bulk load of my data mart `mart_event_snapshot_panel` into my VS Code environment using the `google.cloud` Python library and `bigquery.Client`, I could begin to investigate my data and SeatData.io's aggregated reporting detail. 

At a high level, I now had a dataframe with:

-  **5.78 million** rows
-  **114,000** events
-  **13,000** venues
-  **103** days of collected snapshots

### 1.1 Missing Values

I wanted to understand the difference between missing values and meaningful zeros. Practically, a missing zero means there really was no sales for that event that day, whereas a zero in `venue_capacity` makes no sense and is a true missing value. 

In SQL, you can find how much information is missing within a single column by using the `COUNTIF(feature IS NULL)` function, dividing the result by `COUNT(*)`. `COUNTIF()` results a 1 if the logical statement is true, so the result gave me the `pct_missing` of a specific feature.

Below are some of the key features I evaluated for missingness: 

| Column             | Missing Count | Missing % |
|--------------------|---------------|-----------|
| venue_capacity     | 2419297       | 41.81     |
| listings_median    | 621851        | 10.75     |
| price_spread_ratio | 621851        | 10.75     |
| get_in (null or 0) | 445768        | 7.70      |
| listings_active    | 105775        | 1.83      |
| event_date         | 206           | 0.00      |
| days_to_event      | 206           | 0.00      |

The biggest red flag in this analysis was missing almost half of my data's `venue_capacity` feature. In another section, I will highlight my strategy for handling these missing values in an analytical way and not just disregarding the variable. 

Otherwise, calculated values such as the `price_spread_ratio` (calculated using `get_in`/`listings_median`) are missing values because its dependent feature, the median listing price, is also missing values. `days_to_event`, in a similar vein, is aligned with the `event_date` missing value because the calculation is derived from the variable.

Next, I wanted to see if there's any bias towards specific `focus_buckets` when it comes to data completedness.

<img width="1391" height="690" alt="image" src="https://github.com/user-attachments/assets/85ec3c28-fa8d-4895-bcfb-6b944ad46c6b" />

*Figure 1: Missingness by `focus_bucket`*

An observation from this bar chart faceted by `focus_bucket` is that these Major (and Minor) Sports venues have more complete data than any other category. Otherwise, there is not a signficiant difference in missingness for these important features. 

### 1.2 Outliers

Now, I wanted to understand the distributions of my features to validate my data quality and begin brainstorming feature engineering steps.

Using the `.describe()` function, I can easily evaluate a table of each numerical fields' distribution to both understand sparsity and skew.

| Statistic | sales_1d  | sales_3d  | sales_7d  | sales_30d | **get_in**    | venue_cap | listings_active | **listings_med** | **days_to_event** | **price_spread_ratio** |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------|--------------|---------------|--------------------|
| count     | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,680,284       | 5,164,208    | 5,785,853     | 5,164,208          |
| mean      | 0.39      | 1.18      | 2.60      | 8.54      | $113.61   | 7,467     | 386.7           | 656.1        | 102.2         | 0.59               |
| std       | 2.65      | 6.73      | 12.85     | 34.81     | $2,354.34 | 13,408    | 909.8           | 95,018.9     | 587.8         | 8.54               |
| min       | 0.0       | 0.0       | 0.0       | 0.0       | $0.00     | 100       | 0.0             | 0.9          | 0.0           | 0.00               |
| 25%       | 0.0       | 0.0       | 0.0       | 0.0       | $37.80    | 1,500     | 6.0             | 82.7         | 29.0          | 0.35               |
| 50%       | 0.0       | 0.0       | 0.0       | 0.0       | $59.78    | 2,500     | 44.0            | 127.9        | 72.0          | 0.54               |
| 75%       | 0.0       | 0.0       | 1.0       | 3.0       | $97.71    | 5,000     | 304.0           | 216.1        | 132.0         | 0.76               |
| **max**       | 241.0     | 530.0     | 1,056.0   | 2,803.0   | **$929,999**  | 235,000   | 18,267.0        | **58,228,300**   | **65,608**        | **10,657.3**           |

The most revealing maximum values I saw were a `get_in` price minimum greater than $900,000, a maximum median listing price more than $50 million, and `days_to_event` totaling out to 180 years!

These highly inflate my data, especially aggregated figures. So, I wanted to isolate these observed maximum values and fields to understand what went wrong. So, I used the `APPROX_QUANTILES` on those three variables to collect the 50th, 75th, 90th, 95th, and 99th percentiles, since I wanted to find how many other cases had similar extreme values.

|         Feature |   50th |   75th |   90th |   95th |   99th |
|----------------:|-------:|-------:|-------:|-------:|-------:|
|  `get_in` price |  59.79 |  97.71 | 168.25 | 261.67 | 900.0  |
|`listings_median`| 127.83 | 216.00 | 414.95 | 665.55 | 1760.0 |
|  `days_to_event`|  72.00 | 132.00 | 199.00 | 249.00 | 338.0  |

<img width="1790" height="530" alt="image" src="https://github.com/user-attachments/assets/0a9ddf75-f8de-4c94-955f-3d867e860c36" />

*Figure 2: Distribution of these variables, clipped at 99th percentile*

Some takeaways from the table and visual show me the following:

-  `get_in` above $800 is absurd and nonsensical, so it might make sense to clip the rows/cases and even events where this is the case, such as the $900K case
-  `listings_median` above $1,760 also might need to be removed, especially once the data shows tens of thousands of dollars for an average listing price per event, much more when we see a $50 million + datapoint
-  `days_to_event` shows a more normally skewed distribution, which implies we can't clip the 99th percentile because these values beyond 338 days still seem meaningful with 500+ occurrences

So, I **manually checked events** to investigate the top outliers. I began with evaluating events by their median `days_to_event` in descending order to investigate the top outliers. A LOT of these event snapshots have placeholder dates (2099-12-31) for some events. This is evident from 2028 up to 2099 for test events. After that, there's seems to be a shift from scheduled major events, like the College Football Playoff National Championship Game in 2028 (which aren't yet released), to plausible concerts scheduled a year in advance, like Laura Pausini in November 2027 in Amsterdam. 2027 appears to be the maximum year to filter my dataset by, and beyond this, seems like fake event placeholders.

Then, I looked at `get_in` prices in descending order. Surprisingly, many of these `get_in` prices don't come as sole instances, but with many snapshot samples. I evaluated the median `get_in` price floor, therefore, and found many of the top 100 events with a median `get_in` over $5,000. These high price minimums make me think of individuals super-inflating the market who KNOW this inventory will never sell, hoping a consumer misclicks and sends them a fortune. These listings drown out my true data as noise and suggests I need to clip the events with `get_in` values above the 99th percentiles to rule it out.

Similarly, `listings_median` prices in descending order showed the same trend. So, this convinced me to also remove the noise above the 99th percentile in this field.

### 1.3 Data Completeness

Next, I wanted to explore the time-series aspect of my collected snapshots. Was I collecting similar CSVs across these 100 days? What is the trend of these key reported fields over time? So, I plotted both the 1-day sale totals and the percentage of various fields that were not NULL against my `snapshot_dates`.

<img width="1386" height="790" alt="image" src="https://github.com/user-attachments/assets/a4fac5c0-b540-4b8c-981a-3ceb7bcf38d5" />

*Figure 3: Sales, prices, capacity and inventory over time*

Sales tends to hover between 50-62K transactions on StubHub per day, so there seem to be no abnormal trends in the top chart. Below it, `venue_capacity` is consistently 60% meaningful, which will require some feature engineering and imputation as stated before. The inventory is mostly recorded for every day, and the average price is ~90% filled for my data. 

### 1.4 Aggregated Price

Now, I want to evaluate price in the entire secondary market to see how the minimum floor moves in regards to my time snapshots.

<img width="1186" height="589" alt="image" src="https://github.com/user-attachments/assets/ad94850f-82e7-4a2e-84d1-d4b8742b630e" />

*Figure 4: `get_in` price movement in the secondary market*

While `get_in` prices fluctate for every event every day with new listings, the aggregated median reveals an upward trend throughout the month of November, raising roughly 18% from $56 to $66. Generally, the price to get into any event in the resale market is rising from my observed data. There seems to be a decrease in minimum ticket prices throughout December, and again increasing to the mid-$60s at the beginning of January.

### 1.5 Aggregated Sales

<img width="1187" height="589" alt="image" src="https://github.com/user-attachments/assets/731fc538-2030-4541-bdcc-77d5d97c3c19" />

*Figure 5: 1-Day sale movement in the secondary market*

Observing the aggregated sales in my snapshots also gave me some insight into the data. My first observation was that daily transaction totals are quite spiky and can range greatly day-to-day. There is a major minimum in my observed snapshots on November 4th. After considering causes to this observation, especially since days before and after are more correlated to each other, imply that Election Day created a shock in the secondary market. This is probably because many folks are headed to the polls and considering their ballot choices instead of searching StubHub for potential events to attend. 

However, intuition isn't proof. To validate my assumption, I needed to differentiate this day from the others as statistically significant. 

#### Seasonality

This summer, I took a course in Time-Series Forecasting, which looked at aggregated statistics like sales and broke up data into components, such as Trend and Seasonality. I didn't see clear Trend in either direction with this data, so I wanted to look more closely at Seasonality by day of the week to determine any underlying patterns. 

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/73f449eb-8882-4a5a-bab9-22a62da47914" />

*Figure 6: Boxplot of 1-day sale totals per day of the week*

Reordering this plot to show Sunday first reveals an underlying trend in day-of-the-week seasonality, where more tickets are purchased on StubHub as the week progresses. Saturday's true mean sits roughly 5,000 tickets more than Sunday's. Thursday has more dependable sales on the ticketing platform with its tighter quartile bands. 

In fact, Tuesdays statistically have the most spread on average, where sales swing by +/- 5,000 tickets, or nearly 25% of the average. I measured this with the coeffient of variation, essentially dividing standard deviation by the mean sales for each day. 

So, now that we've determined there is some day-of-the-week seasonality in sales, I wanted to see if the November 4th day, and any others, were statistically significant anomalies than other days of the week. Would this Tuesday be different than the others?

#### Anomaly Detection

I couldn't use standard Z-scores to determine my outliers because I already inferred some seasonality. So, I transitioned to seasonal Z-scores. The Z-score is a measurement that describes an observed value's relationship to its mean in terms of standard deviations. The assumption behind it relies on Maximum Likelihood Estimate (MLE) parameters, in this case, $\mu$ and $\sigma$. $$Z = \frac{x - \mu}{\sigma}$$ is the formula for calculating a Z-score for a particular observation, and below highlights how to statistically determine the likelihood of observing that value. 

<img width="700" height="456" alt="image" src="https://github.com/user-attachments/assets/2de529c6-e4c2-4a10-8896-de0a8b908072" />

*Figure 7: Understanding the normal distribution and z-scores*

However, I couldn't compare the lowest sale total on November 4th to just any day, because it would clearly be an outlier, but also it compares apples to oranges. Instead, I calculated $\mu$ and $\sigma$ of each day of the week to compare November 4th (and others) to similar days of the week. 

With that, I discovered 5 days that had Z-scores (in absolute value) above 2.0.

<img width="1389" height="990" alt="image" src="https://github.com/user-attachments/assets/6d3926d1-34b9-4feb-aa44-288004a6aac8" />

*Figure 8: (Top) Outliers in sale totals per day, (Bottom) Plotting z-score outliers*

After this analysis, I found that October 22nd, November 4th, January 3rd, 5th, and 8th all statistically moved from their expected daily baseline, as shown in green. From there, I can draw more assumptions as to why these sale figures are outliers, but it's great to know this in the background before I start feature engineering and trying to predict these sale totals.

### 1.6 Aggregated Inventory

<img width="1187" height="589" alt="image" src="https://github.com/user-attachments/assets/98eca7f6-b3ff-4c72-a161-aefb0a5ca7fe" />


## 2. Categorical Differences

Next, I wanted to understand how my regex + Claude Code categorization segmented my data and find unique trends to different categories of events.

### 2.1 Distribution of Data by Category

Sure, in the last post I mentioned the splits of events in each category after implementing the NLP categorization. But now, I wanted to see how wide and long my data was for each bucket.

<img width="1389" height="690" alt="image" src="https://github.com/user-attachments/assets/8d06f493-cc0f-4509-aabc-9d23b6119ac9" />

*Figure 4: Comparing events and their records per category*

The gap between the amount of rows and unique events is the main thing to look at in this bar chart. Specifically, Major Sports has, what seems like, a lack of variety in its snapshots, revealing more rows per event than average. Sports events listed on StubHub must be listed longer than other categories, so I expect to have many "full" snapshots in this category. On the other hand, the Other Events category has more variety of events than rows, so Other Events will be thinner, as ticket inventory is posted for fewer days, making it harder to forecast later on.

And, to kick a man while he's down, I had little records for Festivals altogether, making the category look puny here. 

### 2.2 Pricing by Category

Next, I evaluated the price minimum trends of each category. With so few samples in Festivals, I decided to compare median `get_in` prices to reduce the noise in my data and visualization.

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/58f527ed-6f5b-45a7-814c-03051b60c318" />

*Figure 5: Comparing median `get_in` prices between categories*

From this chart, I determined three tiers of pricing for these events:

1. **High pricing for Festivals and Theater.** These types of events make up more unique experiences compared to other markets. This chart suggests there's a premium for even getting through the doors/gates of a Broadway show or Festival. The average festival is a higher-quality production than a sporting event or concert, with many artists involved and major infrastructure. Also, most festival tickets are **standardized**, so the price floor should be similar to the average, since there's no true seating in a festival. Broadway shows are also high production events and expensive to put on. To offset these costs, primary markets must set prices high for these categories, and the secondary market doesn't want to take huge losses, which is likely why I saw this premium tier in the data at around **$80**. 
2. **Mid tier includes Concerts, Comedy, and Other Events.** These represent your average occasion event, with good production and targeted marketing costs. Comedy and Concerts are **semantically similar** to each other, so you'd expect pricing to be in the same ballpark, whereas **Other Events is my catchall category**, revealing a possible "true average". Getting the worst seat in the house can sometimes be **standing room only or could be specific seating**, which is reasonable when understanding the tier ranges from $50-65.
3. **Low minimum ticket price for Sports.** Sports is the cheapest event to attend as shown by this chart, likely because 1) these **events happen frequently**, with some home teams playing 80+ games at home (MLB) in a given season, and 2) venues can hold **greater capacity**, so there are more seats to fill. These nosebleed seats are around **$30-40** to find.

### 2.2 Sales by Category

Next I wanted to look at sales by category, especially over time, to understand the volume of transactions in the secondary market and their volatility.

<img width="1390" height="590" alt="image" src="https://github.com/user-attachments/assets/2c9f81fd-f50f-4fc0-b70f-5a3183ae27d0" />

*Figure 5: 1-Day sale totals per category over time*

Here is where I saw the heartbeat of StubHub: **Major Sports and Concerts are the crutch of the platform,** driving the majority of daily transactions. To look at the spread of `sales_total_1d` even closer, I looked at a boxplot distribution as well.

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/338b4756-c8c9-4c5a-a3de-8ce9be5ba665" />

*Figure 6: Distribution of 1-day sale totals per category*

---

## What's Next?
In **Part 3**, I will detail the feature engineering process, including how I handled the sparse "zero-sale" days and the creation of lag features to capture momentum.

[[Link to Part 1: Database Engineering](https://tommygarner.github.io/seatdata-io-database-engineering/)]
