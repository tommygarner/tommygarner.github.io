---
layout: single
title: "SeatData.io Part 2: Exploratory Data Analysis"
date: 2026-01-08
description: "Visualizing 6 Million rows of Secondary Market Data to Understand my SeatData.io Snapshots and Resale Trends"
author_profile: true
toc: true
toc_sticky: true
tags:
  - EDA
  - data visualization
  - python
  - matplotlib
  - market analysis
excerpt: "Investigating the patterns of 114,000+ events to identify market-specific sales curves and validate data integrity before modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/72b9abfb-60b3-4d37-b81e-2257279bfb76" />

## Abstract
Now it was time to explore my data. Following the construction of my BigQuery warehouse in Part 1, I conducted an Exploratory Data Analysis (EDA) on over 5.7 million records to understand the volatility of the secondary ticket market. This article explores the relationship between lead time and price floors, the distinct sales curves of different event categories, and observed shocks, like Election Day, that impact discretionary spend on tickets. By understanding these signals, I validated feature engineering choices required for later demand forecasting.

## Key Insights


---

## 1. Data Integrity
After performing a bulk load of my data mart `mart_event_snapshot_panel` into my VS Code environment using the `google.cloud` Python library and `bigquery.Client`, I could begin to investigate my data and SeatData.io's aggregated reporting detail. 

At a high level, I now had a dataframe with:

-  **5.78 million** rows
-  **114,000** events
-  **13,000** venues
-  **103** days of collected snapshots

### 1.1 Missing Values

I wanted to understand the difference between missing values and meaningful zeros. Practically, a missing zero means there really was no sales for that event that day, whereas a zero in `venue_capacity` makes no sense and is a true missing value. 

In SQL, you can find how much information is missing within a single column by using the `COUNTIF(feature IS NULL)` function, dividing the result by `COUNT(*)`. `COUNTIF()` results a 1 if the logical statement is true, so the result gave me the `pct_missing` of a specific feature.

Below are some of the key features I evaluated for missingness: 

| Column             | Missing Count | Missing % |
|--------------------|---------------|-----------|
| venue_capacity     | 2419297       | 41.81     |
| listings_median    | 621851        | 10.75     |
| price_spread_ratio | 621851        | 10.75     |
| get_in (null or 0) | 445768        | 7.70      |
| listings_active    | 105775        | 1.83      |
| event_date         | 206           | 0.00      |
| days_to_event      | 206           | 0.00      |

The biggest red flag in this analysis was missing almost half of my data's `venue_capacity` feature. In another section, I will highlight my strategy for handling these missing values in an analytical way and not just disregarding the variable. 

Otherwise, calculated values such as the `price_spread_ratio` (calculated using `get_in`/`listings_median`) are missing values because its dependent feature, the median listing price, is also missing values. `days_to_event`, in a similar vein, is aligned with the `event_date` missing value because the calculation is derived from the variable.

Next, I wanted to see if there's any bias towards specific `focus_buckets` when it comes to data completedness.

<img width="1391" height="690" alt="image" src="https://github.com/user-attachments/assets/3db3462d-05e9-4091-a609-8b49d931c608" />

*Figure 1: Missingness by `focus_bucket`*

An observation from this bar chart faceted by `focus_bucket` is that these Major (and Minor) Sports venues have more complete data than any other category. Otherwise, there is not a signficiant difference in missingness for these important features. 

### 1.2 Outliers

Now, I wanted to understand the distributions of my features to validate my data quality and begin brainstorming feature engineering steps.

Using the `.describe()` function, I can easily evaluate a table of each numerical fields' distribution to both understand sparsity and skew.

| Statistic | sales_1d  | sales_3d  | sales_7d  | sales_30d | **get_in**    | venue_cap | listings_active | **listings_med** | **days_to_event** | **price_spread_ratio** |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------|--------------|---------------|--------------------|
| count     | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,680,284       | 5,164,208    | 5,785,853     | 5,164,208          |
| mean      | 0.39      | 1.18      | 2.60      | 8.54      | $113.61   | 7,467     | 386.7           | 656.1        | 102.2         | 0.59               |
| std       | 2.65      | 6.73      | 12.85     | 34.81     | $2,354.34 | 13,408    | 909.8           | 95,018.9     | 587.8         | 8.54               |
| min       | 0.0       | 0.0       | 0.0       | 0.0       | $0.00     | 100       | 0.0             | 0.9          | 0.0           | 0.00               |
| 25%       | 0.0       | 0.0       | 0.0       | 0.0       | $37.80    | 1,500     | 6.0             | 82.7         | 29.0          | 0.35               |
| 50%       | 0.0       | 0.0       | 0.0       | 0.0       | $59.78    | 2,500     | 44.0            | 127.9        | 72.0          | 0.54               |
| 75%       | 0.0       | 0.0       | 1.0       | 3.0       | $97.71    | 5,000     | 304.0           | 216.1        | 132.0         | 0.76               |
| **max**       | 241.0     | 530.0     | 1,056.0   | 2,803.0   | **$929,999**  | 235,000   | 18,267.0        | **58,228,300**   | **65,608**        | **10,657.3**           |

The most revealing maximum values I saw were a `get_in` price minimum greater than $900,000, a maximum median listing price more than $50 million, and `days_to_event` totaling out to 180 years!

These highly inflate my data, especially aggregated figures. So, I wanted to isolate these observed maximum values and fields to understand what went wrong. So, I used the `APPROX_QUANTILES` on those three variables to collect the 50th, 75th, 90th, 95th, and 99th percentiles, since I wanted to find how many other cases had similar extreme values.

|         Feature |   50th |   75th |   90th |   95th |   99th |
|----------------:|-------:|-------:|-------:|-------:|-------:|
|  `get_in` price |  59.79 |  97.71 | 168.25 | 261.67 | 900.0  |
|`listings_median`| 127.83 | 216.00 | 414.95 | 665.55 | 1760.0 |
|  `days_to_event`|  72.00 | 132.00 | 199.00 | 249.00 | 338.0  |

<img width="1790" height="530" alt="image" src="https://github.com/user-attachments/assets/0a9ddf75-f8de-4c94-955f-3d867e860c36" />

*Figure 2: Distribution of these variables, clipped at 99th percentile*

Some takeaways from the table and visual show me the following:

-  `get_in` above $800 is absurd and nonsensical, so it might make sense to clip the rows/cases and even events where this is the case, such as the $900K case
-  `listings_median` above $1,760 also might need to be removed, especially once the data shows tens of thousands of dollars for an average listing price per event, much more when we see a $50 million + datapoint
-  `days_to_event` shows a more normally skewed distribution, which implies we can't clip the 99th percentile because these values beyond 338 days still seem meaningful with 500+ occurrences

So, I **manually checked events** to investigate the top outliers. I began with evaluating events by their median `days_to_event` in descending order to investigate the top outliers. A LOT of these event snapshots have placeholder dates (2099-12-31) for some events. This is evident from 2028 up to 2099 for test events. After that, there's seems to be a shift from scheduled major events, like the College Football Playoff National Championship Game in 2028 (which aren't yet released), to plausible concerts scheduled a year in advance, like Laura Pausini in November 2027 in Amsterdam. 2027 appears to be the maximum year to filter my dataset by, and beyond this, seems like fake event placeholders.

Then, I looked at `get_in` prices in descending order. Surprisingly, many of these `get_in` prices don't come as sole instances, but with many snapshot samples. I evaluated the median `get_in` price floor, therefore, and found many of the top 100 events with a median `get_in` over $5,000. These high price minimums make me think of individuals super-inflating the market who KNOW this inventory will never sell, hoping a consumer misclicks and sends them a fortune. These listings drown out my true data as noise and suggests I need to clip the events with `get_in` values above the 99th percentiles to rule it out.

Similarly, `listings_median` prices in descending order showed the same trend. So, this convinced me to also remove the noise above the 99th percentile in this field.

### 1.3 Data Completeness

Next, I wanted to explore the time-series aspect of my collected snapshots. Was I collecting similar CSVs across these 100 days? What is the trend of these key reported fields over time? So, I plotted both the 1-day sale totals and the percentage of various fields that were not NULL against my `snapshot_dates`.

<img width="1386" height="790" alt="image" src="https://github.com/user-attachments/assets/a4fac5c0-b540-4b8c-981a-3ceb7bcf38d5" />

*Figure 3: Sales, prices, capacity and inventory over time*

Sales tends to hover between 50-62K transactions on StubHub per day, so there seem to be no abnormal trends in the top chart. Below it, `venue_capacity` is consistently 60% meaningful, which will require some feature engineering and imputation as stated before. The inventory is mostly recorded for every day, and the average price is ~90% filled for my data. 

## 2. Categorical Differences

Next, I wanted to understand how my regex categorization script would help me segment my data and find unique trends to different categories of events.

### 2.1 `get_in` Pricing by Category

First, I evaluated the price minimum trends of each category. With so few samples in Festivals, I decided to compare median `get_in` prices to reduce the noise in my data and visualization.

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/5d2c4816-bb8e-448c-8b33-8db631d9c386" />

*Figure 4: Comparing median `get_in` prices between categories*

From this chart, I determined three tiers of pricing for these events:

1. **High pricing for Festivals.** Though this category contains less than 1% of my total data, this `get_in` average is somewhat plausible in the resale market. The average festival is a higher-quality production than a sporting event or concert, with many artists involved and major infrastructure. Most festival tickets are **standardized**, so the price floor should be similar to the average, since there's no true seating in a festival. Therefore, festivals **double** this `get_in` price from the next tier at **$144** for the average ticket to "get in" for resale.
2. **Mid tier includes Broadway, Concerts, Comedy, and Other Events.** These represent your average occasion event, with good production and targeted marketing costs. Comedy, Broadway, and Concerts are all **semantically similar** to each other, so you'd expect pricing to be in the same ballpark, whereas **Other Events is our catchall category**, revealing a possible "true average". Getting the worst seat in the house can sometimes be **standing room only or could be specific seating**, which is reasonable when understanding the tier ranges from $60-80.
3. **Low minimum ticket price for Sports.** Sports is the cheapest event to attend as shown by this chart, likely because 1) these **events happen frequently**, with some home teams playing 80+ games at home (MLB) in a given season, and 2) venues can hold **greater capacity**, so there are more seats to fill. These nosebleed seats are around **$35-50** to find.

### 

---

## What's Next?
In **Part 3**, I will detail the feature engineering process, including how I handled the sparse "zero-sale" days and the creation of lag features to capture momentum.

[Link to Part 1: Database Engineering]([URL_TO_PART_1](https://tommygarner.github.io/seatdata-io-database-engineering/))
