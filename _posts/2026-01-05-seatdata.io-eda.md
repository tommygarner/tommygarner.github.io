---
layout: single
title: "SeatData.io Part 3: Feature Engineering for Demand Forecasting"
date: 2026-01-11
description: "Transforming raw ticket data into model-ready features through scaling, encoding, and domain-driven interaction terms"
author_profile: true
toc: true
toc_sticky: true
tags:
  - feature engineering
  - data transformation
  - machine learning
  - python
  - pandas
excerpt: "From raw snapshots to prediction-ready features: the critical transformations that set up successful modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/6d70d229-4242-46f3-8d6c-9e5fae2ce4ed" />

## Abstract

After validating data quality in Part 2's EDA, I faced a critical question: how do I transform 5.78 million raw snapshots into features that models can actually learn from? Feature engineering bridges the gap between raw data and predictive models. Through outlier handling, log transformations, cyclical encoding, and domain-driven interaction terms, I engineered **30 features** that capture the temporal/seasonal dynamics, pricing psychology, and segment-specific behaviors of the secondary ticket market. The final dataset: **5.1M training rows** and **58K test rows** from 114K unique events.

## Key Insights

- **Outlier Clipping Before Transformation** - Extreme prices ($929K get-in!) must be clipped before log transforms to prevent distribution distortion
- **Log Transformations for Skewed Distributions** - Sales and prices follow power-law distributions requiring mathematical normalization
- **Cyclical Encoding for Temporal Patterns** - Days of the week are circular, not linear; sine/cosine encoding preserves this structure
- **Interaction Terms for Segment-Specific Behavior** - Sports fans buy differently than concert-goers as events approach

---

## 1. Outlier Handling from EDA

My EDA in Part 2 revealed several data quality issues that needed addressing before any feature engineering could begin. This section covers the dropping, clipping, and filtering I applied.

### 1.1 Price Outliers: The $929,999 Get-In Ticket

The raw price distributions were dominated by absurd values:
- `get_in` max: **$929,999** (vs 99th percentile of $900)
- `listings_median` max: **$58,228,300** (vs 99th percentile of $1,760)

I don't believe these were data entry errorsâ€”they were likely real meme listings or ultra-premium VIP packages. But they represented <1% of the data while dominating 99% of the variance, and they had to go.

**Solution: Clip at 99th Percentile**

```python
# EDA-derived thresholds (99th percentile)
CLIP_THRESHOLDS = {
    'get_in': 900,
    'listings_median': 1760
}

model_df['get_in'] = model_df['get_in'].clip(lower=0, upper=900)
model_df['listings_median'] = model_df['listings_median'].clip(lower=0, upper=1760)
```

**Impact on dataset:**
- `get_in` > $900: 38,740 rows clipped (0.67% of data)
- `listings_median` > $1,760: 51,778 rows clipped (0.90% of data)

Combined, less than 1.6% of rows were affectedâ€”a small price to pay for well-behaved distributions.

<img width="1389" height="1025" alt="image" src="https://github.com/user-attachments/assets/e05ed90e-e911-49e6-ac70-950a49bcd471" />

*Figure 1: Before/after clipping visualization showing compressed tails*

### 1.2 Days-to-Event Outliers

I found placeholder listings as well as legitimate listings for shows 2+ years in advance. These extreme lead times added noise without predictive value.

**Solution: Filter in SQL**

```sql
WHERE days_to_event < 1000
  AND event_date < '2028-01-01'
```

This focused my analysis on events happening within the next yearâ€”the window where pricing decisions actually matter.

### 1.3 Why Clip Before Transform?

The order matters. If I log-transformed first, then clipped:
- log($929,999) = 13.74
- log($900) = 6.80

The "clipped" value would still be 2x the intended ceiling in log space. By clipping in raw space first, log transformation uses that 99th percentile as the ceiling and compresses values into a manageable range.

---

## 2. Log Transformations

With outliers handled, I applied log transformations to all skewed numeric features. This is the single most impactful transformation in the pipeline.

### 2.1 Why Log Transform?

My EDA showed severe right-skew in nearly every numeric feature:

| Feature | Raw Skew | Log Skew |
|---------|----------|----------|
| get_in | 9.68 | 0.72 |
| listings_active | 4.89 | 0.12 |
| listings_median | 176.70 | 0.89 |
| sales_total_7d | 12.28 | 1.25 |
| venue_capacity | 3.87 | 0.32 |

Skew > |1| indicates a distribution that will cause problems for most models. Log transformation compresses these distributions toward normality.

### 2.2 Applying Log Transforms

I used `np.log1p(x)` = log(1 + x) to handle zeros gracefully:

```python
log_features = ['get_in', 'listings_median', 'listings_active',
                'sales_total_7d', 'venue_capacity']

for col in log_features:
    model_df[f'{col}_log'] = np.log1p(model_df[col].clip(lower=0).fillna(0))
```

### 2.3 Visual Validation

After transformation, the distributions became approximately normalâ€”much better for gradient-based optimization in neural networks.

<!-- IMAGE: Raw vs log-transformed violin plots showing before/after normalization (02_claude cell 12) -->
<img width="1200" alt="Raw vs log distributions" src="PLACEHOLDER_URL" />

*Figure 2: Raw vs log-transformed distributions showing dramatically reduced skew*

---

## 3. Missing Value Imputation

Missing data is inevitable in real-world datasets. My approach: preserve as much signal as possible through intelligent imputation.

### 3.1 Venue Capacity (41.8% Missing)

This was the biggest challenge. From Part 2's EDA, **41.8% of venue_capacity values were missing** (2.42M rows). Dropping these rows would eliminate nearly half my training data.

**Multi-Stage Imputation Pipeline:**

**Stage 1: TouringData.org Lookup**
```python
# Match venue names to external capacity database
venue_capacity_map = master_docs.groupby(["join_name", "join_city"])["Capacity"].max().to_dict()
```
- Reduced missing from 41.8% â†’ 39.9%

**Stage 2: Fuzzy Matching by City**
```python
from thefuzz import process, fuzz
match = process.extractOne(venue, candidates, scorer=fuzz.token_sort_ratio, score_cutoff=85)
```
- Used `thefuzz` library with 85% similarity threshold
- Matched venues with slight name variations (e.g., "Madison Square Garden" vs "MSG")

**Stage 3: Keyword-Based Imputation**
```python
venue_defaults = {
    "stadium": 60000,
    "arena": 15000,
    "amphitheatre": 10000,
    "theatre": 2500,
    "theater": 2500,
    "club": 500,
}
```
- Reduced missing from 39.9% â†’ 24.4%

**Stage 4: Focus Bucket Median Fallback**
```python
bucket_medians = model_df.groupby("focus_bucket")["venue_capacity"].median()
model_df["venue_capacity"] = model_df["venue_capacity"].fillna(
    model_df["focus_bucket"].map(bucket_medians)
)
```
- Final fallback: use median capacity for each event category
- Reduced missing from 24.4% â†’ **0%**

Why this works: Major Sports venues average ~30,000 capacity, Broadway theaters ~1,500, Concerts ~8,000. Using bucket-specific medians preserves these segment differences.

### 3.2 Listings Median

```python
# Event-level median first, then global median
global_listings_median = model_df['listings_median'].median()
model_df['listings_median'] = model_df.groupby('event_id_stubhub')['listings_median'].transform(
    lambda s: s.fillna(s.median())
)
model_df['listings_median'] = model_df['listings_median'].fillna(global_listings_median)
```

### 3.3 Lag Features (Fill with Current Values)

For lag features where previous values don't exist (first snapshot of an event):

```python
model_df['sales_total_7d_prev'] = model_df['sales_total_7d_prev'].fillna(model_df['sales_total_7d'])
model_df['listings_active_prev'] = model_df['listings_active_prev'].fillna(model_df['listings_active'])
model_df['sales_total_change_7d'] = model_df['sales_total_change_7d'].fillna(0)
model_df['listings_active_change_7d'] = model_df['listings_active_change_7d'].fillna(0)
```

### 3.4 Other Fills

```python
model_df['listings_active'] = model_df['listings_active'].fillna(0)
model_df['price_spread_ratio'] = model_df['price_spread_ratio'].fillna(0).clip(upper=1.0)
model_df['inv_per_day'] = model_df['inv_per_day'].fillna(0)
```

**Final result: 0 missing values** in the feature set (down from 105K in `listings_active_prev`).

---

## 4. Cyclical Day-of-Week Encoding

My EDA showed that **Saturdays have 5,000 more daily sales than Sundays**. Day-of-week clearly mattersâ€”but encoding it properly is tricky.

### 4.1 The Problem with Integer Encoding

If I encoded Monday=1, Tuesday=2...Sunday=7, the model thinks Sunday (7) is "far" from Monday (1). But in reality, Sunday and Monday are adjacent! The week is a **circle**, not a line.

### 4.2 Sine/Cosine Encoding

Think of the week as a clock face. Each day occupies a position on the circle:
- Sunday: 0Â° (BigQuery DAYOFWEEK returns 1 for Sunday)
- Monday: 51.4Â°
- Tuesday: 102.8Â°
- ...
- Saturday: 308.6Â°

I project this onto two axes:

```python
# BigQuery DAYOFWEEK: 1=Sunday, 7=Saturday, so adjust to 0-6
dow_adjusted = (model_df['event_dow'] - 1)

model_df['dow_sin'] = np.sin(2 * np.pi * dow_adjusted / 7)
model_df['dow_cos'] = np.cos(2 * np.pi * dow_adjusted / 7)
```

Now Sunday (day 0) and Saturday (day 6) have similar sine/cosine values, preserving their adjacency.

### 4.3 Weekend Flag

I also created a simple binary indicator:

```python
model_df['is_weekend'] = model_df['event_dow'].isin([1, 7]).astype(int)  # Sunday=1, Saturday=7
```

*Figure 3: Cyclical encoding visualizationâ€”days mapped to unit circle*

---

## 5. Anomaly Day Flags

My EDA discovered **5 days with Z-scores above 2.0** in absolute valueâ€”significant deviations from normal sales patterns.

### 5.1 Identified Anomalies

```python
ANOMALY_DATES = [
    '2025-10-22',  # Unexplained Wednesday dip
    '2025-11-04',  # Election Day
    '2026-01-03',  # Post-holiday slump
    '2026-01-05',  # Post-holiday slump
    '2026-01-08'   # Post-holiday slump
]
```

### 5.2 Creating the Flag

```python
model_df['snapshot_date'] = pd.to_datetime(model_df['snapshot_date'])
anomaly_dates = pd.to_datetime(ANOMALY_DATES)

model_df['is_anomaly_day'] = model_df['snapshot_date'].isin(anomaly_dates).astype(int)
```

**Result:** 282,262 rows flagged (4.88% of data)

This gives models an explicit signal: "this day is weird, adjust expectations accordingly."

---

## 6. Price Tier Feature

My EDA revealed **3 distinct pricing tiers** in the resale market. I encoded this as a categorical feature.

### 6.1 EDA-Derived Tiers

```python
PRICE_TIERS = {
    'Premium': ['Festivals', 'Broadway & Theater'],
    'Mid': ['Concerts', 'Comedy', 'Other Events'],
    'Low': ['Major Sports', 'Minor/Other Sports']
}

def get_price_tier(bucket):
    for tier, buckets in PRICE_TIERS.items():
        if bucket in buckets:
            return tier
    return 'Mid'

model_df['price_tier'] = model_df['focus_bucket'].apply(get_price_tier)
```

### 6.2 Distribution

| Price Tier | Focus Buckets | Row Count |
|------------|---------------|-----------|
| Premium | Festivals, Broadway & Theater | 1,596,732 |
| Mid | Concerts, Comedy, Other | 3,414,061 |
| Low | Major Sports, Minor/Other Sports | 770,280 |

This categorical feature lets models learn "Premium tier events behave differently" without complex price thresholds.

---

## 7. Interaction Terms

This is where feature engineering gets powerful. My EDA revealed that **different segments exhibit different temporal patterns**. A unified model struggles to learn these nuances.

### 7.1 The Problem

Sports fans buy on predictable schedules tied to game announcements. Concert-goers buy based on price drops and artist popularity. A single `days_to_event` coefficient can't capture both behaviors.

### 7.2 Days-to-Event Ã— Focus Bucket

I created interaction terms by multiplying `days_to_event` with each bucket dummy:

```python
# Create one-hot encoding for focus_bucket
bucket_dummies = pd.get_dummies(model_df['focus_bucket'], prefix='bucket')

# Create interaction terms
for col in bucket_dummies.columns:
    interaction_name = f'dte_X_{col}'
    model_df[interaction_name] = model_df['days_to_event'] * bucket_dummies[col]
```

**What this does:**
- For a Major Sports event: `dte_X_bucket_Major_Sports` = days_to_event value
- For a Concert event: `dte_X_bucket_Major_Sports` = 0

This allows models to learn **segment-specific time sensitivity**.

### 7.3 Final Week Flags per Category

```python
model_df['is_final_week'] = (model_df['days_to_event'] <= 7).astype(int)

for bucket in model_df['focus_bucket'].unique():
    safe_name = bucket.replace(' ', '_').replace('/', '_').replace('&', 'and')
    model_df[f'final_week_{safe_name}'] = (
        (model_df['days_to_event'] <= 7) &
        (model_df['focus_bucket'] == bucket)
    ).astype(int)
```

<!-- IMAGE: Days-to-event slopes by category showing different buying patterns (02_claude cell 36) -->
<img width="1200" alt="Interaction effects by category" src="PLACEHOLDER_URL" />

*Figure 4: Interaction effect visualizationâ€”different slopes by category*

---

## 8. Two-Stage Target Variables

With features engineered, I finally defined what I was predicting. The **72.4% zero-sales rate** demanded a two-stage approach.

### 8.1 The Zero-Inflated Problem

Looking at my `sales_next_7d` distribution:
- **4,187,699 rows (72.4%)** had zero sales in the following week
- Only **1,594,374 rows (27.6%)** had any sales at all

A single regression model struggles with this distributionâ€”it's trying to simultaneously predict "no sales" and "how many sales."

<figure>
  <img width="575" height="428" alt="Target distribution" src="https://github.com/user-attachments/assets/8d0eb7e1-c9b2-4c1f-b842-09356ab9a76c" />
  <figcaption>Figure 5: Target distribution showing 72.4% zeros</figcaption>
</figure>

### 8.2 The Two-Stage Solution

**Stage 1 (Binary Classification):** Will there be ANY sales?
```python
model_df['target_sales_binary'] = (model_df['sales_next_7d'] > 0).astype(int)
```

**Stage 2 (Log Regression):** For events WITH sales, how many?
```python
model_df['target_sales_log'] = np.log1p(model_df['sales_next_7d'])
```

### 8.3 Why Log Transform the Target?

Raw sales ranged from 1 to 2,803 tickets. Taking the logarithm:
- Compresses the scale: 2,803 â†’ 7.94
- Penalizes errors proportionally: being off by 10 tickets matters more for low-volume events
- Creates a more Gaussian distribution for regression

<figure>
  <img width="564" height="426" alt="Log-transformed target" src="https://github.com/user-attachments/assets/7975f7e5-730d-4f30-a4b0-ddeb3a449afe" />
  <figcaption>Figure 6: Log-transformed target distribution</figcaption>
</figure>

---

## 9. Temporal Train/Test Split

This is **critical** for preventing data leakage. Random splits on time-series data allow future information to leak into training.

### 9.1 Date Range

```python
print(f"Min: {model_df['snapshot_date'].min()}")  # 2025-10-01
print(f"Max: {model_df['snapshot_date'].max()}")  # 2026-01-12
```

### 9.2 Split Logic

```python
TRAIN_CUTOFF = pd.Timestamp('2026-01-01')

train_mask = model_df['snapshot_date'] < TRAIN_CUTOFF
test_mask = (
    (model_df['snapshot_date'] >= TRAIN_CUTOFF) &
    (model_df['days_to_event'] <= 14) &
    (model_df['days_to_event'] > 0)
)
```

**Result:**
- **Train:** 5,114,513 rows (all snapshots before Jan 1, 2026)
- **Test:** 58,022 rows (Jan 1-12, 2026, final 14 days before events)

### 9.3 Event Overlap Analysis

```python
train_events = set(model_df[train_mask]['event_id_stubhub'])
test_events = set(model_df[test_mask]['event_id_stubhub'])
overlapping = train_events & test_events

print(f"Overlapping events: {len(overlapping)} ({100*len(overlapping)/len(test_events):.2f}% of test)")
# 8,228 events (92.56% of test)
```

This overlap is expected and desirableâ€”we're testing on the **final snapshots** of events we've seen historically, simulating real deployment where we predict upcoming sales for known events.

*Figure 7: Temporal split visualization showing train/test boundary*

---

## 10. Feature Scaling

With all features engineered and the train/test split defined, I applied scaling as the **final** transformation step.

### 10.1 Why Scale at the End?

1. **Prevent leakage:** Scaler statistics must come from training data only
2. **Clean pipeline:** All derived features exist before scaling
3. **Neural networks need it:** Gradient descent converges poorly with unscaled features

### 10.2 RobustScaler (Fit on Train Only)

I used `RobustScaler` instead of `StandardScaler` because it's resistant to remaining outliers (uses median/IQR instead of mean/std):

```python
from sklearn.preprocessing import RobustScaler

features_to_scale = [
    'get_in_log', 'listings_median_log', 'listings_active_log',
    'venue_capacity_log', 'sales_total_7d_log',
    'days_to_event', 'inv_per_day', 'price_spread_ratio',
    'sales_total_change_7d', 'listings_active_change_7d'
]

scaler = RobustScaler()

# Fit on training data ONLY
train_data = model_df[model_df['split'] == 'train'][features_to_scale]
scaler.fit(train_data)

# Transform both train and test
scaled_features = scaler.transform(model_df[features_to_scale])
scaled_df = pd.DataFrame(
    scaled_features,
    columns=[f'{c}_scaled' for c in features_to_scale],
    index=model_df.index
)
model_df = pd.concat([model_df, scaled_df], axis=1)
```

### 10.3 Scaled Feature Ranges

After scaling, features are centered around 0 with reasonable ranges:

| Feature | Mean | Std | Min | Max |
|---------|------|-----|-----|-----|
| get_in_log_scaled | -0.19 | 1.47 | -4.43 | 2.92 |
| listings_median_log_scaled | 0.09 | 0.89 | -4.80 | 2.97 |
| listings_active_log_scaled | 0.01 | 0.62 | -0.96 | 1.58 |
| venue_capacity_log_scaled | 0.06 | 0.95 | -2.25 | 3.18 |
| sales_total_7d_log_scaled | 0.70 | 1.30 | 0.00 | 10.05 |

---

## 11. Correlation Analysis

Before modeling, I validated my engineered features by checking correlations with the target.

### 11.1 Feature-Target Correlations

```python
target_corr = corr_matrix['target_sales_log'].sort_values(ascending=False)
```

| Feature | Correlation |
|---------|-------------|
| sales_total_7d_log | +0.843 |
| listings_active_log | +0.495 |
| venue_capacity_log | +0.406 |
| inv_per_day | +0.246 |
| sales_total_change_7d | +0.169 |
| listings_median_log | +0.155 |
| get_in_log | +0.101 |
| days_to_event | -0.085 |
| price_spread_ratio | -0.145 |

**Key insight:** Past sales (`sales_total_7d_log` at 0.843) is by far the strongest predictorâ€”momentum matters most in ticket sales.

### 11.2 Multicollinearity Check

```python
high_corr_pairs = [
    (i, j, corr) for i, j, corr in ...
    if abs(corr) > 0.9
]
```

Found: `get_in_log` and `listings_median_log` correlated at 0.87. Not unexpected (floor and median prices move together), but not redundant enough to drop.

*Figure 8: Correlation heatmap of final feature set*

---

## 12. Export to BigQuery

With feature engineering complete, I pushed the processed data back to BigQuery for modeling.

### 12.1 Column Sanitization

BigQuery only allows letters, numbers, and underscores in column names:

```python
import re

def clean_bq_col_name(name):
    return re.sub(r'[^a-zA-Z0-9_]', '_', name)

export_df.columns = [clean_bq_col_name(c) for c in export_df.columns]
```

### 12.2 Upload Configuration

```python
from google.cloud import bigquery

job_config = bigquery.LoadJobConfig(
    write_disposition="WRITE_TRUNCATE",
    time_partitioning=bigquery.TimePartitioning(
        field="event_date",
        type_=bigquery.TimePartitioningType.DAY
    ),
    clustering_fields=['focus_bucket', 'event_id_stubhub']
)
```

### 12.3 Final Tables

```python
TRAIN_TABLE = "secondary-tickets.seatdata.model_train_final"
TEST_TABLE = "secondary-tickets.seatdata.model_test_final"

# Upload train
job = client.load_table_from_dataframe(df_train, TRAIN_TABLE, job_config=job_config)
job.result()
print(f"âœ… Train Upload Complete: {len(df_train):,} rows")

# Upload test
job = client.load_table_from_dataframe(df_test, TEST_TABLE, job_config=job_config)
job.result()
print(f"âœ… Test Upload Complete: {len(df_test):,} rows")
```

**Output:**
```
âœ… Train Upload Complete: 5,114,513 rows
âœ… Test Upload Complete: 58,022 rows
ðŸŽ‰ DONE. Proceed to Modeling.
```

### 12.4 Final Feature Count

**30 features ready for modeling:**
- 10 scaled log features (prices, inventory, sales, venue capacity)
- 5 temporal features (dow_sin, dow_cos, is_weekend, is_anomaly_day, is_final_week)
- 7 bucket dummies (one-hot encoded focus_bucket)
- 7 interaction terms (dte Ã— bucket)
- 1 price tier categorical

---

## What's Next

With 30 engineered features stored in BigQuery, my data was finally ready for modeling. But which algorithm would work best for ticket sales prediction?

In **Part 4: Foundation Models**, I put five different approaches head-to-head:
- Ridge Regression (linear baseline)
- Gradient Boosting (sequential trees)
- XGBoost (optimized gradient boosting)
- LightGBM (fast gradient boosting)
- CatBoost (categorical-aware boosting)

The winner? Tree-based models dominated. But the gap between them was surprisingly small, setting up the ensemble methods in Part 7...

---
