---
layout: single
title: "SeatData.io Part 2: Exploratory Data Analysis"
date: 2026-01-08
description: "Visualizing 6 Million rows of Secondary Market Data to Understand my SeatData.io Snapshots and Resale Trends"
author_profile: true
toc: true
toc_sticky: true
tags:
  - EDA
  - data visualization
  - python
  - matplotlib
  - market analysis
excerpt: "Investigating the patterns of 114,000+ events to identify market-specific sales curves and validate data integrity before modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/72b9abfb-60b3-4d37-b81e-2257279bfb76" />

## Abstract
Now it was time to **explore my data**. Following the construction of my BigQuery warehouse in Part 1, I conducted an Exploratory Data Analysis (EDA) on over 5.7 million records to understand **ticket prices, volume, and inventory** in the secondary market. This article explores the relationship between **lead time** and those features, as well as differences in **market categories**. I also investigate **shocks and anomalies** in my data and draw conclusions to why they impact discretionary spend on tickets. By understanding these signals, I **validated feature engineering choices** required for later demand forecasting.

## Key Insights

-  **Statistical Anomaly Detection**
-  **Price Floor Segmentation by Category**
-  **Volume Deviation and Sales Analysis**

---

## 1. Data Integrity
After performing a bulk load of my data mart `mart_event_snapshot_panel` into my VS Code environment using the `google.cloud` Python library and `bigquery.Client`, I could begin to investigate my data and SeatData.io's aggregated reporting detail. 

At a high level, I now had a dataframe with:

-  **5.78 million** rows
-  **114,000** events
-  **13,000** venues
-  **103** days of collected snapshots

### 1.1 Missing Values

I wanted to understand the **difference between missing values and meaningful zeros**. Practically, a zero in sale totals means there really was no sales for that event, whereas a zero in `venue_capacity` makes no sense and is a true missing value. 

In SQL, you can find how much information is missing within a single column by using the `COUNTIF(feature IS NULL)` function, dividing the result by `COUNT(*)`. `COUNTIF()` results a 1 if the logical statement is true, so the result gave me the **`pct_missing`** of a specific feature.

Below are some of the key features I evaluated for missingness: 

| Column             | Missing Count | Missing % |
|--------------------|---------------|-----------|
| **venue_capacity**     | **2419297**       | **41.81**     |
| listings_median    | 621851        | 10.75     |
| price_spread_ratio | 621851        | 10.75     |
| get_in (null or 0) | 445768        | 7.70      |
| listings_active    | 105775        | 1.83      |
| event_date         | 206           | 0.00      |
| days_to_event      | 206           | 0.00      |

The biggest red flag in this analysis was missing **almost half** of my data's `venue_capacity` feature. In another section, I will highlight my strategy for handling these missing values in an analytical way and not just disregarding the variable. 

Otherwise, calculated values such as the `price_spread_ratio` (calculated using `get_in`/`listings_median`) are missing values because its **dependent feature**, the median listing price, is **also missing values**. `days_to_event`, in a similar vein, is aligned with the `event_date` missing value because the calculation is **derived** from the variable.

Next, I wanted to see if there's any **bias** towards specific `focus_buckets` when it comes to data completedness.

<img width="1391" height="690" alt="image" src="https://github.com/user-attachments/assets/85ec3c28-fa8d-4895-bcfb-6b944ad46c6b" />

*Figure 1: Missingness by `focus_bucket`*

An observation from this bar chart faceted by `focus_bucket` is that these **Major (and Minor) Sports venues have more complete data than any other category**. There still remains a similar relationships of missing values in median listing prices and number of listings. Otherwise, there is not a signficiant difference in missingness for these important features. 

### 1.2 Outliers

Now, I wanted to understand the **distributions of my features** to validate my data quality and begin brainstorming feature engineering steps.

Using the **`.describe()`** function, I can easily evaluate a table of each numerical fields' distribution to both understand sparsity and skew.

| Statistic | sales_1d  | sales_3d  | sales_7d  | sales_30d | **get_in**    | venue_cap | listings_active | **listings_med** | **days_to_event** | **price_spread_ratio** |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------|--------------|---------------|--------------------|
| count     | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,680,284       | 5,164,208    | 5,785,853     | 5,164,208          |
| mean      | 0.39      | 1.18      | 2.60      | 8.54      | $113.61   | 7,467     | 386.7           | 656.1        | 102.2         | 0.59               |
| std       | 2.65      | 6.73      | 12.85     | 34.81     | $2,354.34 | 13,408    | 909.8           | 95,018.9     | 587.8         | 8.54               |
| min       | 0.0       | 0.0       | 0.0       | 0.0       | $0.00     | 100       | 0.0             | 0.9          | 0.0           | 0.00               |
| 25%       | 0.0       | 0.0       | 0.0       | 0.0       | $37.80    | 1,500     | 6.0             | 82.7         | 29.0          | 0.35               |
| 50%       | 0.0       | 0.0       | 0.0       | 0.0       | $59.78    | 2,500     | 44.0            | 127.9        | 72.0          | 0.54               |
| 75%       | 0.0       | 0.0       | 1.0       | 3.0       | $97.71    | 5,000     | 304.0           | 216.1        | 132.0         | 0.76               |
| **max**       | 241.0     | 530.0     | 1,056.0   | 2,803.0   | **$929,999**  | 235,000   | 18,267.0        | **58,228,300**   | **65,608**        | **10,657.3**           |

The most revealing maximum values I saw were a **`get_in` price minimum greater than $900,000**, a **maximum median listing price more than $50 million**, and `days_to_event` totaling out to **180 years**!

These highly inflate my data, especially aggregated figures. So, I wanted to isolate these observed maximum values and fields to understand what went wrong. So, I used **`APPROX_QUANTILES`** on those three variables to collect the 50th, 75th, 90th, 95th, and 99th **percentiles**, since I wanted to find how many other cases had similar extreme values.

|         Feature |   50th |   75th |   90th |   95th |   99th |
|----------------:|-------:|-------:|-------:|-------:|-------:|
|  `get_in` price |  59.79 |  97.71 | 168.25 | 261.67 | 900.0  |
|`listings_median`| 127.83 | 216.00 | 414.95 | 665.55 | 1760.0 |
|  `days_to_event`|  72.00 | 132.00 | 199.00 | 249.00 | 338.0  |

<img width="1790" height="530" alt="image" src="https://github.com/user-attachments/assets/0a9ddf75-f8de-4c94-955f-3d867e860c36" />

*Figure 2: Distribution of these variables, clipped at 99th percentile*

Some takeaways from the table and visual show me the following:

-  **`get_in` above $800** is absurd and nonsensical, so it might make sense to **clip** the rows/cases and even events where this is the case, such as the $900K case
-  **`listings_median` above $1,760** also might need to be removed, especially once the data shows tens of thousands of dollars for an average listing price per event, much more when we see a $50 million + datapoint
-  **`days_to_event` shows a more normally skewed distribution**, which implies we can't clip the 99th percentile because **these values beyond 338 days still seem meaningful with 500+ occurrences**

So, I **manually checked events** to investigate the top outliers. I began with evaluating events by their median `days_to_event` in descending order to investigate the top outliers. 

A LOT of these event snapshots have **placeholder dates (2099-12-31)**. This is evident from 2028 up to 2099 for test events. 

After that, there's seems to be a shift from scheduled major events, like the College Football Playoff National Championship Game in 2028 (**which aren't yet released**), to plausible concerts scheduled a **year in advance**, like Laura Pausini in November 2027 in Amsterdam. **2027** appears to be the maximum year to filter my dataset by, and beyond this, seems like fake event placeholders.

Then, I looked at **`get_in` prices** in descending order. Surprisingly, many of these `get_in` prices don't come as sole instances, but with **many snapshot samples**. I evaluated the median `get_in` price floor, therefore, and found many of the top 100 events with a median `get_in` over $5,000. 

These high price minimums make me think of individuals super-inflating the market who **KNOW** this inventory will never sell, hoping a consumer misclicks and sends them a fortune. These listings drown out my true data as **noise** and suggests I need to clip the events with `get_in` values above the 99th percentiles to rule it out.

Similarly, `listings_median` prices in descending order showed the **same trend**. So, this convinced me to also **remove the noise above the 99th percentile** in this field.

### 1.3 Data Completeness

Next, I wanted to explore the time-series aspect of my collected snapshots. Was I collecting **similar CSVs** across these 100 days? What is the trend of these key reported fields over time? 

So, I plotted both the 1-day sale totals and the percentage of various fields that were not NULL against my `snapshot_dates`.

<img width="1386" height="790" alt="image" src="https://github.com/user-attachments/assets/a4fac5c0-b540-4b8c-981a-3ceb7bcf38d5" />

*Figure 3: Sales, prices, capacity and inventory over time*

Sales tends to hover **between 50-62K transactions** on StubHub per day, so there seem to be no abnormal trends in the top chart. 

Below it, `venue_capacity` is **consistently 60% meaningful**, which will require some **feature engineering and imputation** as stated before. The inventory is mostly recorded for every day, and the average price is ~90% filled for my data. 

### 1.4 Aggregated Price

Now, I want to evaluate price in the entire secondary market to see **how the minimum floor moves in regards to my time snapshots**.

<img width="1186" height="589" alt="image" src="https://github.com/user-attachments/assets/ad94850f-82e7-4a2e-84d1-d4b8742b630e" />

*Figure 4: `get_in` price movement in the secondary market*

While `get_in` prices fluctate for every event every day with new listings, the aggregated median reveals an **upward trend** throughout the month of November, raising roughly **18%** from $56 to $66. 

Generally, the price to get into any event in the resale market is rising from my observed data. There seems to be a **decrease** in minimum ticket prices throughout December, and **again increasing** to the mid-$60s at the beginning of January.

### 1.5 Aggregated Sales

<img width="1187" height="589" alt="image" src="https://github.com/user-attachments/assets/731fc538-2030-4541-bdcc-77d5d97c3c19" />

*Figure 5: 1-Day sale movement in the secondary market*

Observing the aggregated sales in my snapshots also gave me some insight into the data. My first observation was that daily transaction totals are **quite spiky** and can range greatly day-to-day. 

There is a **major minimum** in my observed snapshots on **November 4th**. After considering causes to this observation, especially since days before and after are more correlated to each other, I discoverd that that **Election Day** must have created a shock in the secondary market. This is probably because many folks are **headed to the polls** and considering their ballot choices instead of searching StubHub for potential events to attend. 

However, intuition isn't proof. To validate my assumption, I needed to **differentiate this day from the others as statistically significant**. 

#### Seasonality

This summer, I took a course in **Time-Series Forecasting** for my graduate degree, which looked at aggregated statistics like sales and **broke up data into components**, such as Trend and Seasonality. 

I didn't see clear Trend in either direction with this data, so I wanted to look more closely at Seasonality **by day of the week** to determine any underlying patterns. 

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/73f449eb-8882-4a5a-bab9-22a62da47914" />

*Figure 6: Boxplot of 1-day sale totals per day of the week*

Reordering this plot to show Sunday first reveals an underlying trend in day-of-the-week seasonality, where **more tickets are purchased on StubHub as the week progresses**. Saturday's true mean sits roughly **5,000 tickets more than Sunday's**. Thursday has more **dependable sales** on the ticketing platform with its tighter quartile bands. 

In fact, Tuesdays statistically have the **most spread on average**, where sales swing by **+/- 5,000 tickets**, or nearly 25% of the average. I measured this with the **coeffient of variation**, essentially dividing standard deviation by the mean sales for each day. 

So, now that I've determined there is some **day-of-the-week seasonality** in sales, I wanted to see if the November 4th day, and any others, were **statistically significant anomalies** than other days of the week. Would this Tuesday be different than the others?

#### Anomaly Detection

I couldn't use **standard Z-scores** to determine my outliers because I already inferred some seasonality. So, I transitioned to **seasonal Z-scores**. 

The Z-score is a measurement that describes an **observed value's relationship to its mean in terms of standard deviations**. The assumption behind it relies on **Maximum Likelihood Estimate** (MLE) parameters, in this case, **$$\mu$$ and $$\sigma$$**. 

**$$Z = \frac{x - \mu}{\sigma}$$** is the formula for calculating a Z-score for a particular observation, and below highlights how to statistically determine the **likelihood** of observing that value. 

<img width="788" height="444" alt="image" src="https://github.com/user-attachments/assets/81797192-0563-4d97-9bb3-3c2c10be76b1" />

*Figure 7: Understanding the normal distribution and z-scores*

However, I couldn't compare the lowest sale total on November 4th to just any day, because it would clearly be an outlier. Instead, I calculated **$$\mu$$ and $$\sigma$$** of **each day of the week** to compare November 4th (and others) to similar days of the week. 

In a standard normal distribution, **95% of all datapoints fall within two standard deviations** of the mean, or $$\lvert Z \rvert \le 2$$. By using this logic, I found all days with Z-scores beyond -2.0, ocurring less than 2.5% of the time (negatively).

<img width="1389" height="990" alt="image" src="https://github.com/user-attachments/assets/6d3926d1-34b9-4feb-aa44-288004a6aac8" />

*Figure 8: (Top) Outliers in sale totals per day, (Bottom) Plotting z-score outliers*

With that, I discovered **5 days** that had Z-scores (in absolute value) above 2.0.

-  **October 22nd**: This day has **no major cultural event**, and a random Wednesday in October is quite puzzling to understand reasons why there were significantly less sales on this day
-  **November 4th**: Confirming my hypothesis, a dip in this magnitude is **not normal for a Tuesday**, resulting in a statistically significant deviation to Tuesday means. Now is the Election Day hypothesis correct? That will take more data to determine
-  **January 3rd, 5th, and 8th**: These days are all close to each other, which might suggest a **market slump** in purchasing tickets. Maybe this could be a result from post-holiday spending

From there, I can draw more assumptions as to why these sale figures are outliers, but it's great to know this in the background before I start feature engineering and trying to predict these sale totals.

### 1.6 Aggregated Inventory

<img width="1187" height="589" alt="image" src="https://github.com/user-attachments/assets/98eca7f6-b3ff-4c72-a161-aefb0a5ca7fe" />

*Figure 9: Ticket listing movement in the secondary market*

Finally, I wanted to look at ticket listings on StubHub, which revealed an interesting trend. Total active listings climbed in early October, fell drastically throughout the end of the month, and then experienced a **sharp one day correction**, before showing a more normal decreasing and increasing trend from November to January. 

I had **Perplexity use its Research feature** to investigate different forums and news articles as to why there was such a sharp increase in ticket listings from October 31st to November 1st on the platform. Perplexity is a great tool for this use-case because it can **search through unstructured data sources** like text and news threads with the most citations per output of any other LLM. 

One of the most convincing arguments it found was the **Halloween** case, which fell on a Friday last year, and made up one of the year's most significant event weekends. This **explains the downhill trend during the late-October listings** in the graph above as those Halloween-adjacent events expired in the end of the month. 

But, to add more substance to this inventory shock, I **compared expiring and new listings** between October 31st and November 1st. On the 31st, only **40K events expired**, so it wasn't so much that events expired on this day, or else I'd have seen an aggressive cliff in the data. This was a gradual expiration of events without influx of new listings.

On the other hand, the 1st had **over 1 million events added**, with over **60%** of these newly added events being **Concerts**! So, this presumes that November 1st served as a mark for the start of an aggressive holiday sales push for major concert tours. 

In my Time-Series course, we'd call this correction **non-stationarity**. Inventory experiences **structural breaks** based on the calendar, and it would be **hard for any model to predict this November 1st shock** without additional context. 

So, to address this, I will later consider **differencing** (delta changes day-to-day) and **category-specific lag features** rather than a simple inventory count as a regressor.

## 2. Categorical Differences

Since I just discovered that the massive November 1st shock was driven almost entirely by Concerts, it became clear that **I couldn't treat the market as uniform**. I needed to break these categories apart to understand their **unique behaviors**.

### 2.1 Distribution of Data by Category

Sure, in the last post I mentioned the splits of events in each category after implementing the NLP categorization. But now, I wanted to see how **wide and long** my data was for each bucket.

<img width="1389" height="690" alt="image" src="https://github.com/user-attachments/assets/8d06f493-cc0f-4509-aabc-9d23b6119ac9" />

*Figure 10: Comparing events and their records per category*

The **gap** between the amount of rows and unique events is the main thing to look at in this bar chart. 

Specifically, Major Sports has, what seems like, a **lack of variety** in its snapshots, revealing **more rows per event** than the average ratio. Sports events listed on StubHub must be **listed longer** than other categories, so I expect to have **many "full" snapshots** in this category. 

On the other hand, the Other Events category has **more variety of events than rows**, so Other Events will be **thinner**, as ticket inventory is **posted for fewer days**, making it harder to forecast later on.

And, to kick a man while he's down, I had **little records** for Festivals altogether, making the category look puny here. 

### 2.2 Pricing by Category

Next, I evaluated the **price minimum trends** of each category. With so few samples in Festivals, I decided to compare **median** `get_in` prices to **reduce the noise** in my data and visualization.

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/58f527ed-6f5b-45a7-814c-03051b60c318" />

*Figure 11: Comparing median `get_in` prices between categories*

From this chart, I determined three tiers of pricing for these events:

1. **High pricing for Festivals and Theater.** These types of events make up **more unique experiences** compared to other markets. This chart suggests there's a **premium** for even getting through the doors/gates of a Broadway show or Festival.
  -  The average festival is a **higher-quality production** than a sporting event or concert, with many artists involved and major infrastructure
  -  Also, most festival tickets are **standardized**, so the price **floor should be similar to the average**, since there's no true seating in a festival
  -  Broadway shows are **also high production events** and expensive to put on
  -  To offset these costs, **primary markets must set prices high** for these categories, and the secondary market doesn't want to take huge losses, which is likely why I saw this premium tier in the data at around **$80**.

2. **Mid tier includes Concerts, Comedy, and Other Events.** These represent your average occasion event, with good production and targeted marketing costs.
  -  Comedy and Concerts are **semantically similar** to each other, so you'd expect pricing to be in the same ballpark
  -  **Other Events is my catchall category**, revealing a possible "true average"
  -  Getting the worst seat in the house can sometimes be **standing room only or could be specific seating**, which is reasonable when understanding the tier ranges from **$50-65**

3. **Low minimum ticket price for Sports.** Sports is the cheapest event to attend as shown by this chart, likely because:
  -  These **events happen frequently**, with some home teams playing 80+ games at home (MLB) in a given season
  -  Venues can hold **greater capacity**, so there are more seats to fill
  -  These nosebleed seats are around **$30-40** to find

### 2.3 Sales by Category

Next I wanted to look at sales by category, **especially over time**, to understand the volume of transactions in the secondary market and their volatility by market.

<img width="1390" height="590" alt="image" src="https://github.com/user-attachments/assets/2c9f81fd-f50f-4fc0-b70f-5a3183ae27d0" />

*Figure 12: 1-Day sale totals per category over time*

This view confirms that **Major Sports and Concerts** are the backbone of StubHub, driving the majority of daily transactions on the platform.

Festivals, there at the bottom if you squint, sell around **50 tickets per day on average** on StubHub. A very rare breed compared to other types of tickets.

Also, **remember that November 4th anomaly dip**? Well, all categories except Festivals dip **simulataneously**, confirming this shock affected every category *with substantial inventory*.

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/338b4756-c8c9-4c5a-a3de-8ce9be5ba665" />

*Figure 13: Distribution of 1-day sale totals per category*

Looking at the boxplot now, I was able to **compare the volatility** of 1-day sales in each `focus_bucket`. 

Concerts had the **most variation** between a busy and non-busy day, and the interquartile range (14K to 4K daily sales) suggest this market is **highly dependent on its events and announcements**. 

Sports is similarly structured but with **less intensity**. Otherwise, the remaining categories have few transactions and little variation.

Considering both visuals, it makes sense that **a singular model would fail** if used in each market segment, since the resale patterns of Festivals are much more stable than that of Concerts, for example. Here is where I **put segmented modeling in my back pocket** for later.

### 2.4 Inventory by Category

<img width="1389" height="590" alt="image" src="https://github.com/user-attachments/assets/c9839dca-ec24-487a-833f-0cd836e6921b" />

*Figure 14: Inventory per category over time*

When breaking down `listings_active` by category, I saw how **dramatically the Concerts category grew on November 1st**, showing the main driver of increased secondary inventory I saw previously. 

Otherwise, inventory on StubHub per category seems relatively **stable and large**, considering I'm talking about **millions of tickets** listed on the site daily.

Festivals, again, sits there at the bottom with an **average of almost 15K ticket listings per day**. This category is by far the most sparse in the **number of listings and transactions**.

## 3. Days to Event

Next, I wanted to look more closely at these numerical features in my data **as events draw nearer**. How do buyers and sellers react to an **expiration of inventory**?

### 3.1 Price as Event Draws Closer

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/bd32f380-278b-4d95-a28d-e5e5c4c25e6d" />

*Figure 15: Minimum ticket prices as `days_to_event` draws near*

This shows an expected outcome. Tickets are **expiring inventory with no salvage value**. A ticket to a previous game won't work for another game, and rarely has marketable value, especially if digital.

So, I'd expect sellers to want to get rid of this inventory by any means, **even at a loss**. So, generally, there's about a **10% decrease in ticket floors** in the resale market as the `event_date` draws closer, *especially if you're willing to wait until the day of*!

### 3.2 Sales as Event Draws Closer

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/78abf1af-36f5-480e-9d9f-3f088c60e3e4" />

*Figure 16: 1-Day sales as events draw near*

Looking at sold tickets, the **final week** of an event sees more inventory sold than any other `days_to_event` bin, *though the length in days aren't standardized across these buckets*. 

However, I can again see the **sparsity of my data** in this plot, since the y-axis only **caps at 3.5 tickets sold daily**. Still, many people are flocking to StubHub in the final week for tickets to that event in the upcoming weekend.

### 3.3 Inventory as Event Draws Closer

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/8b3a93cb-6379-40b2-82b8-98592c3a1d03" />

*Figure 17: Active listings in relation to `days_to_event`*

In the inverse direction is inventory, which makes sense as well. **As more tickets are bought in the resale market, more listings come off**. Intuitively, this logic holds up, but with a closer look, there are **more tickets coming off the market than being purchased off the market**. 

**Delisting** tickets might happen because **sellers haven't found buyers**. Instead of eating the ticket cost, maybe sellers **change plans** to attend the event or **pass the tickets off** to their friends. Another reason could be to **list their tickets on another site**, or maybe those tickets **sold last-minute on another platform already**.

## 4 Categorical Differences and Days to Event

Finally, I wanted to evaluate any differences as the days to event draw near **among my categories**.

### 4.1 Price as Event Draws Closer by Category

<img width="788" height="490" alt="image" src="https://github.com/user-attachments/assets/e14d3dbb-dfaf-4abb-b808-ffffd7e0ef44" />

*Figure 18: Ticket price floor as `days_to_event` decreases*

Almost counter-intuitively, **price remains stable** for almost every category **except Festivals**. This is interesting to note as a ticket-buyer because those Festivals are events I previously identified as premiums. So, in theory, if you want to attend the upcoming Sips and Sounds Festival in Austin, TX, it might be more beneficial to wait until February to buy your ticket. 

Practically, for this data, Festivals are quite sparse in sample size, **especially 2+ months out, with only 203 unique events and 10K snapshots** this far out. So, there's **not enough evidence** to show that this is a true market trend simply because of my lack of datapoints.

Otherwise, **Concerts and Sports** naturally decline in `get_in` price over the course of the ticket listing. This is good to know if you wanted to see your favorite artist and hoped to **save a couple bucks** next time!


### 4.2 Sales as Event Draws Closer by Category

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/eb7cf948-608d-4898-aa29-f8ef90e8ce6b" />

*Figure 19: 1-Day sales as `days_to_event` decreases*

As events draw nearer, there are generally **more buyers** in the resale market. These are likely those **last-minute decisions** that I make to go to an event this upcoming weekend, for example.

Sports, both Major and Minor, make up the majority of last-minute transactions on StubHub. Since prices for these categories **generally decline** over the lifetime of the event, maybe those fringe listings are **sniped** on the market. Could be both as a way to get in the door for dirt cheap and also to get expiring inventory off your hands at some salvage value.

### 4.3 Inventory as Event Draws Closer by Category

<img width="988" height="590" alt="image" src="https://github.com/user-attachments/assets/d78bf33e-8ee8-41d0-8805-b84de83096da" />

*Figure 20: Listings as `days_to_event` decreases*

Finally, a look at inventory as the event draws closer shows a **dropoff**, which could go hand-in-hand with the activity in those final 2 weeks that I discussed above. 

Specifically for Sports, there's an interesting relationship as inventory seems to **increase until the final week** of the event. Considering I saw price decrease over the lifecycle of events, it seems that **prices drop because sellers are flooding this market**, driving the price down in competition to get tickets off their hands.

Another explanation is **the season ticketholder** who can no longer attend the game and needs to pawn off their ticket! Plans come up, and so posting inventory 2 weeks in advance might be a line of thought a seller takes that gives them enough time to sell.

All other categories do not fluctuate as drastically as do Sports, and show **minor decline** as the event draws closer. 

## Takeaways

-  I will need to perform some **heavy data munging** as I found massive **outliers** (put in as placeholders) that skewed my averages
-  **Data imputation** may be necessary, especially for **`venue_capacity`** which is missing in almost half of my records
-  **3 tiers of pricing** exist in the resale market:
    - **Premium** (Festivals and Theater)
    - **Mid** (Concerts and Comedy)
    - **Low** (Sports) tiers
-  **Tuesdays are the most volatile** days of the week in resale volume, while **Saturdays have the highest sales volume**
-  I found **5 sales volume shocks** that were **statistically significant** from similar days of the week using **anomaly detection and seasonal z-scores**
-  **Concerts flooded the resale market on November 1st**, marking the start of a potential holiday sales/promo push
-  Prices typically **drop by roughly 10%** as the event approaches
-  **`days_to_event`** is an interesting variable, especially per market, which leads me to believe that modeling will require an **interaction term** that takes into effect the **context of each segment**

---

## What's Next?
In **Part 3**, I will detail the feature engineering process, including how I handled the sparse "zero-sale" days and the creation of lag features to capture momentum.

[Link to Part 1: Database Engineering](https://tommygarner.github.io/seatdata-io-database-engineering/)
