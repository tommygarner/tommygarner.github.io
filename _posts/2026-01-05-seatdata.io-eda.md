---
layout: single
title: "SeatData.io Part 2: Exploratory Data Analysis"
date: 2026-01-08
description: "Visualizing 6 Million rows of Secondary Market Data to Understand my SeatData.io Snapshots and Resale Trends"
author_profile: true
toc: true
toc_sticky: true
tags:
  - EDA
  - data visualization
  - python
  - matplotlib
  - market analysis
excerpt: "Investigating the patterns of 114,000+ events to identify market-specific sales curves and validate data integrity before modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/72b9abfb-60b3-4d37-b81e-2257279bfb76" />

## Abstract
Now it was time to explore my data. Following the construction of my BigQuery warehouse in Part 1, I conducted an Exploratory Data Analysis (EDA) on over 5.7 million records to understand the volatility of the secondary ticket market. This article explores the relationship between lead time and price floors, the distinct sales curves of different event categories, and observed shocks, like Election Day, that impact discretionary spend on tickets. By understanding these signals, I validated feature engineering choices required for later demand forecasting.

## Key Insights


---

## 1. Data Integrity
After performing a bulk load of my data mart `mart_event_snapshot_panel` into my VS Code environment using the `google.cloud` Python library and `bigquery.Client`, I could begin to investigate my data and SeatData.io's aggregated reporting detail. 

At a high level, I now had a dataframe with:

-  **5.78 million** rows
-  **114,000** events
-  **13,000** venues
-  **103** days of collected snapshots

### 1.1 Missing Values

I wanted to understand the difference between missing values and meaningful zeros. Practically, a missing zero means there really was no sales for that event that day, whereas a zero in `venue_capacity` makes no sense and is a true missing value. 

In SQL, you can find how much information is missing within a single column by using the `COUNTIF(feature IS NULL)` function, dividing the result by `COUNT(*)`. `COUNTIF()` results a 1 if the logical statement is true, so the result gave me the `pct_missing` of a specific feature.

Below are some of the key features I evaluated for missingness: 

| Column             | Missing Count | Missing % |
|--------------------|---------------|-----------|
| venue_capacity     | 2419297       | 41.81     |
| listings_median    | 621851        | 10.75     |
| price_spread_ratio | 621851        | 10.75     |
| get_in (null or 0) | 445768        | 7.70      |
| listings_active    | 105775        | 1.83      |
| event_date         | 206           | 0.00      |
| days_to_event      | 206           | 0.00      |

The biggest red flag in this analysis was missing almost half of my data's `venue_capacity` feature. In another section, I will highlight my strategy for handling these missing values in an analytical way and not just disregarding the variable. 

Otherwise, calculated values such as the `price_spread_ratio` (calculated using `get_in`/`listings_median`) are missing values because its dependent feature, the median listing price, is also missing values. `days_to_event`, in a similar vein, is aligned with the `event_date` missing value because the calculation is derived from the variable.

Next, I wanted to see if there's any bias towards specific `focus_buckets` when it comes to data completedness.

<img width="1391" height="690" alt="image" src="https://github.com/user-attachments/assets/3db3462d-05e9-4091-a609-8b49d931c608" />

*Figure 1: Missingness by `focus_bucket`*

An observation from this bar chart faceted by `focus_bucket` is that these Major (and Minor) Sports venues have more complete data than any other category. Otherwise, there is not a signficiant difference in missingness for these important features. 

### 1.2 Outliers

Now, I wanted to understand the distributions of my features to validate my data quality and begin brainstorming feature engineering steps.

Using the `.describe()` function, I can easily evaluate a table of each numerical fields' distribution to both understand sparsity and skew.

| Statistic | sales_1d  | sales_3d  | sales_7d  | sales_30d | get_in    | venue_cap | listings_active | listings_med | days_to_event | price_spread_ratio |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------|--------------|---------------|--------------------|
| count     | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,680,284       | 5,164,208    | 5,785,853     | 5,164,208          |
| mean      | 0.39      | 1.18      | 2.60      | 8.54      | $113.61   | 7,467     | 386.7           | 656.1        | 102.2         | 0.59               |
| std       | 2.65      | 6.73      | 12.85     | 34.81     | $2,354.34 | 13,408    | 909.8           | 95,018.9     | 587.8         | 8.54               |
| min       | 0.0       | 0.0       | 0.0       | 0.0       | $0.00     | 100       | 0.0             | 0.9          | 0.0           | 0.00               |
| 25%       | 0.0       | 0.0       | 0.0       | 0.0       | $37.80    | 1,500     | 6.0             | 82.7         | 29.0          | 0.35               |
| 50%       | 0.0       | 0.0       | 0.0       | 0.0       | $59.78    | 2,500     | 44.0            | 127.9        | 72.0          | 0.54               |
| 75%       | 0.0       | 0.0       | 1.0       | 3.0       | $97.71    | 5,000     | 304.0           | 216.1        | 132.0         | 0.76               |
| max       | 241.0     | 530.0     | 1,056.0   | 2,803.0   | **$929,999**  | 235,000   | 18,267.0        | **58,228,300**   | **65,608**        | **10,657.3**           |

The most revealing maximum values I saw were a `get_in` price minimum greater than $900,000, a maximum median listing price more than $50 million, and `days_to_event` totaling out to 180 years!

These highly inflate my data, especially aggregated figures. So, I wanted to isolate these observed maximum values and fields to understand what went wrong. So, I used the `APPROX_QUANTILES` on those three variables to collect the 50th, 75th, 90th, 95th, and 99th percentiles, since I wanted to find how many other cases had similar extreme values.

|         Feature |   50th |   75th |   90th |   95th |   99th |
|----------------:|-------:|-------:|-------:|-------:|-------:|
|    Get-In Price |  59.79 |  97.71 | 168.25 | 261.67 | 900.0  |
| Listings Median | 127.83 | 216.00 | 414.95 | 665.55 | 1760.0 |
|   Days to Event |  72.00 | 132.00 | 199.00 | 249.00 | 338.0  |

<img width="1790" height="530" alt="image" src="https://github.com/user-attachments/assets/0a9ddf75-f8de-4c94-955f-3d867e860c36" />
*Figure 2: Distribution of these variables, clipped at 99th percentile*

Some takeaways from the table and visual show me the following:

-  `get_in` above $800 is absurd and nonsensical, so it might make sense to clip the rows/cases and even events where this is the case, such as the $900K case
-  `listings_median` above $1,760 also might need to be removed, especially once the data shows tens of thousands of dollars for an average listing price per event, much more when we see a $50 million + datapoint
-  `days_to_event` shows a more normally skewed distribution, which implies we can't clip the 99th percentile because these values beyond 338 days still seem meaningful with 500+ occurrences

So, I **manually checked events** to investigate the top outliers. I began with evaluating events by their median `days_to_event` in descending order to investigate the top outliers. A LOT of these event snapshots have placeholder dates (2099-12-31) for some events. This is evident from 2028 up to 2099 for test events. After that, there's seems to be a shift from scheduled major events, like the College Football Playoff National Championship Game in 2028 (which aren't yet released), to plausible concerts scheduled a year in advance, like Laura Pausini in November 2027 in Amsterdam. 2027 appears to be the maximum year to filter my dataset by, and beyond this, seems like fake event placeholders.

Then, I looked at `get_in` prices in descending order. Surprisingly, many of these `get_in` prices don't come as sole instances, but with many snapshot samples. I evaluated the median `get_in` price floor, therefore, and found many of the top 100 events with a median `get_in` over $5,000. These high price minimums make me think of individuals super-inflating the market who KNOW this inventory will never sell, hoping a consumer misclicks and sends them a fortune. These listings drown out my true data as noise and suggests I need to clip the events with `get_in` values above the 99th percentiles to rule it out.

Similarly, `listings_median` prices in descending order showed the same trend. So, this convinced me to also remove the noise above the 99th percentile in this field.

## 2. Market Integrity & Technical Anomalies
Real-world data is never perfect. My first step was a "Daily Pulse" check to ensure the ingestion was stable.

### 2.1 Handling the "October 4th" Gap
As mentioned in the previous post, SeatData.io had a technical failure on October 4th, resulting in a total loss of data for that day. 
* *The Solution:* In this section, discuss how you handled the gap (e.g., using a 3-day rolling average or simply acknowledging the discontinuity in your time-series plots).

### 2.2 Volume Over Time
[INSERT CHART: Daily Snapshot Count]
This visualization confirms the "backfill" period I mentioned earlier and the steady growth of the database as I moved toward 6 million rows.

## 3. The Anatomy of a Ticket Sale
The most critical feature in this project is the relationship between **Price** and **Lead Time**.

### 3.1 Price Spread Ratio vs. Days to Event
I engineered the `price_spread_ratio` to see how the "Get-In" (lowest) price relates to the "Median" price. 
* **Observation:** Do we see the ratio tighten as the event date approaches? 
* **The "Fire Sale" Effect:** Visualizing how resellers drop prices in the final 48 hours to avoid a $0 salvage value.

[INSERT CHART: Scatter plot/Hexbin of Price Spread vs. Days to Event]

## 4. Segmenting the Market
Not all tickets are created equal. Using the `focus_buckets` from the warehouse, I compared the "average sales curve" across different genres.

| Bucket | Volatility | Sales Velocity |
|--------|------------|----------------|
| **Major Sports** | High | Spikes near game day |
| **Broadway** | Low | Consistent & Steady |
| **Concerts** | Medium | Highly dependent on artist tier |

[INSERT CHART: Faceted Bar Chart of Sales Volume by Bucket]

## 5. Outlier Detection: The "Taylor Swift" Effect
In ticketing, outliers aren't just errors; they are often the most important data points. However, a single $10,000 Eras Tour ticket can skew an entire model's RMSE. 
* **The Strategy:** I used [Z-score/Interquartile Range/Log Transformation] to identify these "Mega-Events." 
* **The Decision:** Discuss whether you capped these values or used a specific loss function (like Huber Loss) in future steps to stay robust against these outliers.

## 6. Conclusion: Preparing for Feature Engineering
This EDA confirmed that the "one-size-fits-all" modeling approach would fail. The clear differences between **Festivals** and **Comedy** shows suggest that my future models must be category-aware. 

With a clean, validated understanding of the market signals, I am now ready to move into the **Feature Engineering** phase, where I transform these raw observations into the mathematical inputs for my prediction engine.

---

## What's Next?
In **Part 3**, I will detail the feature engineering process, including how I handled the sparse "zero-sale" days and the creation of lag features to capture momentum.

[Link to Part 1: Database Engineering]([URL_TO_PART_1](https://tommygarner.github.io/seatdata-io-database-engineering/))
