---
layout: single
title: "SeatData.io Part 2: Exploratory Data Analysis"
date: 2026-01-05
description: "Visualizing 6 Million rows of Secondary Market Data to Understand my SeatData.io Snapshots and Resale Trends"
author_profile: true
toc: true
toc_sticky: true
tags:
  - EDA
  - data visualization
  - python
  - matplotlib
  - market analysis
excerpt: "Investigating the patterns of 114,000+ events to identify market-specific sales curves and validate data integrity before modeling."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/72b9abfb-60b3-4d37-b81e-2257279bfb76" />

## Abstract
Now it was time to explore my data. Following the construction of my BigQuery warehouse in Part 1, I conducted an Exploratory Data Analysis (EDA) on over 5.7 million records to understand the volatility of the secondary ticket market. This article explores the relationship between lead time and price floors, the distinct sales curves of different event categories, and observed shocks, like Election Day, that impact discretionary spend on tickets. By understanding these signals, I validated feature engineering choices required for later demand forecasting.

## Key Insights


---

## 1. Data Integrity
After performing a bulk load of my data mart `mart_event_snapshot_panel` into my VS Code environment using the `google.cloud` Python library and `bigquery.Client`, I could begin to investigate my data and SeatData.io's aggregated reporting detail. 

At a high level, I now had a dataframe with:

-  **5.78 million** rows
-  **114,000** events
-  **13,000** venues
-  **103** days of collected snapshots

### 1.1 Missing Values

I wanted to understand the difference between missing values and meaningful zeros. Practically, a missing zero means there really was no sales for that event that day, whereas a zero in `venue_capacity` makes no sense and is a true missing value. 

In SQL, you can find how much information is missing within a single column by using the `COUNTIF(feature IS NULL)` function, dividing the result by `COUNT(*)`. `COUNTIF()` results a 1 if the logical statement is true, so the result gave me the `pct_missing` of a specific feature.

Below are some of the key features I evaluated for missingness: 

| Column             | Missing Count | Missing % |
|--------------------|---------------|-----------|
| venue_capacity     | 2419297       | 41.81     |
| listings_median    | 621851        | 10.75     |
| price_spread_ratio | 621851        | 10.75     |
| get_in (null or 0) | 445768        | 7.70      |
| listings_active    | 105775        | 1.83      |
| event_date         | 206           | 0.00      |
| days_to_event      | 206           | 0.00      |

The biggest red flag in this analysis was missing almost half of my data's `venue_capacity` feature. In another section, I will highlight my strategy for handling these missing values in an analytical way and not just disregarding the variable. 

Otherwise, calculated values such as the `price_spread_ratio` (calculated using `get_in`/`listings_median`) are missing values because its dependent feature, the median listing price, is also missing values. `days_to_event`, in a similar vein, is aligned with the `event_date` missing value because the calculation is derived from the variable.

Next, I wanted to see if there's any bias towards specific `focus_buckets` when it comes to data completedness.

<img width="1391" height="690" alt="image" src="https://github.com/user-attachments/assets/3db3462d-05e9-4091-a609-8b49d931c608" />

*Figure 1: Missingness by `focus_bucket`*

An observation from this bar chart faceted by `focus_bucket` is that these Major (and Minor) Sports venues have more complete data than any other category. Otherwise, there is not a signficiant difference in missingness for these important features. 

### 1.2 Outliers

Now, I wanted to understand the distributions of my features to validate my data quality and begin brainstorming feature engineering steps.

Using the `.describe()` function, I can easily evaluate a table of each numerical fields' distribution to both understand sparsity and skew.

| Statistic | sales_1d  | sales_3d  | sales_7d  | sales_30d | get_in    | venue_cap | listings_active | listings_med | days_to_event | price_spread_ratio |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------|--------------|---------------|--------------------|
| count     | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,786,059 | 5,680,284       | 5,164,208    | 5,785,853     | 5,164,208          |
| mean      | 0.39      | 1.18      | 2.60      | 8.54      | $113.61   | 7,467     | 386.7           | 656.1        | 102.2         | 0.59               |
| std       | 2.65      | 6.73      | 12.85     | 34.81     | $2,354.34 | 13,408    | 909.8           | 95,018.9     | 587.8         | 8.54               |
| min       | 0.0       | 0.0       | 0.0       | 0.0       | $0.00     | 100       | 0.0             | 0.9          | 0.0           | 0.00               |
| 25%       | 0.0       | 0.0       | 0.0       | 0.0       | $37.80    | 1,500     | 6.0             | 82.7         | 29.0          | 0.35               |
| 50%       | 0.0       | 0.0       | 0.0       | 0.0       | $59.78    | 2,500     | 44.0            | 127.9        | 72.0          | 0.54               |
| 75%       | 0.0       | 0.0       | 1.0       | 3.0       | $97.71    | 5,000     | 304.0           | 216.1        | 132.0         | 0.76               |
| max       | 241.0     | 530.0     | 1,056.0   | 2,803.0   | $929,999  | 235,000   | 18,267.0        | 58,228,300   | 65,608        | 10,657.3           |

## 2. Market Integrity & Technical Anomalies
Real-world data is never perfect. My first step was a "Daily Pulse" check to ensure the ingestion was stable.

### 2.1 Handling the "October 4th" Gap
As mentioned in the previous post, SeatData.io had a technical failure on October 4th, resulting in a total loss of data for that day. 
* *The Solution:* In this section, discuss how you handled the gap (e.g., using a 3-day rolling average or simply acknowledging the discontinuity in your time-series plots).

### 2.2 Volume Over Time
[INSERT CHART: Daily Snapshot Count]
This visualization confirms the "backfill" period I mentioned earlier and the steady growth of the database as I moved toward 6 million rows.

## 3. The Anatomy of a Ticket Sale
The most critical feature in this project is the relationship between **Price** and **Lead Time**.

### 3.1 Price Spread Ratio vs. Days to Event
I engineered the `price_spread_ratio` to see how the "Get-In" (lowest) price relates to the "Median" price. 
* **Observation:** Do we see the ratio tighten as the event date approaches? 
* **The "Fire Sale" Effect:** Visualizing how resellers drop prices in the final 48 hours to avoid a $0 salvage value.

[INSERT CHART: Scatter plot/Hexbin of Price Spread vs. Days to Event]

## 4. Segmenting the Market
Not all tickets are created equal. Using the `focus_buckets` from the warehouse, I compared the "average sales curve" across different genres.

| Bucket | Volatility | Sales Velocity |
|--------|------------|----------------|
| **Major Sports** | High | Spikes near game day |
| **Broadway** | Low | Consistent & Steady |
| **Concerts** | Medium | Highly dependent on artist tier |

[INSERT CHART: Faceted Bar Chart of Sales Volume by Bucket]

## 5. Outlier Detection: The "Taylor Swift" Effect
In ticketing, outliers aren't just errors; they are often the most important data points. However, a single $10,000 Eras Tour ticket can skew an entire model's RMSE. 
* **The Strategy:** I used [Z-score/Interquartile Range/Log Transformation] to identify these "Mega-Events." 
* **The Decision:** Discuss whether you capped these values or used a specific loss function (like Huber Loss) in future steps to stay robust against these outliers.

## 6. Conclusion: Preparing for Feature Engineering
This EDA confirmed that the "one-size-fits-all" modeling approach would fail. The clear differences between **Festivals** and **Comedy** shows suggest that my future models must be category-aware. 

With a clean, validated understanding of the market signals, I am now ready to move into the **Feature Engineering** phase, where I transform these raw observations into the mathematical inputs for my prediction engine.

---

## What's Next?
In **Part 3**, I will detail the feature engineering process, including how I handled the sparse "zero-sale" days and the creation of lag features to capture momentum.

[Link to Part 1: Database Engineering]([URL_TO_PART_1](https://tommygarner.github.io/seatdata-io-database-engineering/))
