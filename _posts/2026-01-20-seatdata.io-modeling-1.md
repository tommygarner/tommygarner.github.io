---
layout: single
title: "SeatData.io Part 4: Foundation Models"
date: 2026-01-20
description: "Testing five algorithms head-to-head to find the best baseline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - machine learning
  - random forest
  - gradient boosting
  - XGBoost
  - LightGBM
  - CatBoost
  - scikit-learn
excerpt: "Exploring tree-based learning algorithms to predict secondary market ticket sales."
---

<img width="612" height="367" alt="image" src="https://github.com/user-attachments/assets/95e263d7-269b-4080-be57-4bb0d4116057" />

## Abstract

With 30 engineered features ready from Part 3, I faced the classic machine learning question: **which model should I use?** Rather than assuming, I ran a systematic comparison of five approaches, from simple linear regression to gradient boosting. The rule-based models resulted in a tight race between XGBoost, LightGBM, CatBoost, and RandomForest. Yet, these algorithms were all finding similar patterns, hinting that combining them might unlock even better performance.

## Key Insights

- **Tree-Based Models Dominate** - Gradient boosting achieved significantly better predictions than simpler approaches
- **All Tree Models Cluster Together** - MAE difference of only ~0.3 tickets between top performers
- **Temporal Features Dominate** - Days-to-event and past sales explain over 35% of prediction power
- **Business Impact** - Best models predict within ~10 tickets MAE on average

---

## 1. Choosing the Best Model

I had engineered features. I had cleaned data. Now came the fun part of messing with models for my data.

However, it's not a one-size-fits-all approach. So, I wanted to throw a lot of models at my data to see which fits my sales patterns best. So, I decided to start with trees.

### 1.1 Setting up a baseline

I first wanted to go to the basics and break down a Classifier and Regressor before jumping into trees and even more modeling approaches. So, I went back to the Logistic and Linear Regression.

### 1.2 Logistic Regression

Before I could predict how many tickets would sell, I had to see if a model could even distinguish if a single ticket would sell for an event. Since my data was so imbalanced, this would help keep the nonzero sale events out.

I ended up implementing logistic regression to get a baseline for these classifications.

<img width="686" height="386" alt="image" src="https://github.com/user-attachments/assets/876fb7eb-6805-41ca-868b-725fc301b4f0" />

*Figure 1: Logistic Regression visualized*

Logistic Regression maps out the probability that a specific observation belongs to the positive class based on the feature space. This is the most basic of classifiers that is trying to find a linear boundary separating nonzero-sale from zero-sale events. Below shows how I performed the Logistic Regression.

```python
lr_clf = LogisticRegression(
    class_weight='balanced',  # Handle class imbalance
    max_iter=1000, solver='lbfgs', random_state=42, n_jobs=-1
)
```

Some hyperparameters are involved in this configuration that I can go ahead and explain, though you might see similar ones in later code.

- `class_weight='balanced'` ironically indicates that my data has a class imbalance between zero-sale and nonzero-sale events. This helps logistic regression favor that minority class because, without this, my model can guess no sales for every event and still be 72% accurate (and 0% useful)
- `max_iter=1000` indicates how many runs the solver will perform to converge on the best weights for classifications
- `solver='lbfgs'` is the default optimization algorithm (or loss function) and handles ridge regression to prevent overfitting
- `n_jobs=-1` is simply telling the model to use all available CPU cores on my computer to train the model

**Results**:
- **PR-AUC: 0.8023**

This is a tricky interpretation, but I'll try to make it simple. Since my data is made up of 72% zero-sale events, I'm focused on that 28% of the data with sales in the final two weeks. A random model would predict a PR-AUC of 0.28, or the fraction of positive samples in my data.

Therefore, this Logistic Regression is pretty accurate. Looking more closely at Precision and Recall, this classifier found 83.7% of all events that have sales in the final two weeks, and when the model does flag an event in this way, it's correct 68.17% of the time. Not bad for a baseline!

### 1.3 Linear Regression

Next, I wanted to look only at those nonzero-sale events and try to predict their volume using Linear Regression. 

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/b57ddaac-c0e8-4bdd-bd25-17ccb15f1e8f" />

*Figure 2: Linear Regression function*

Linear Regression takes into account the Maximum Likelihood Estimates of coefficients for the regression line using all 30 of my features. Using Ordinary Least Squares, I can find the best fit line that minimizes the error term. Below shows some more code!

```python
lin_reg = LinearRegression(n_jobs=-1)
lin_reg.fit(X_train_reg, y_train_reg)
```

Sci-kit learn's package makes this code super simple to work with, so let's take a look at how well the model did.

**Results**:
- **RMSE (tickets): 77.65**
- **MAE (tickets): 13.31**

This model was pretty off. Underpredicting big-sale events likely penalized this error term to its detriment, even though a simple Linear Regression can explain about 66% of the variance found in my data. This would become an easy baseline to top.

So with that, I want to introduce some of the strongest models in all of my testing.

## 2 The Trees

### 2.1 What is tree-based modeling?

<img width="950" height="348" alt="image" src="https://github.com/user-attachments/assets/6474a015-4e03-424b-8562-db84cd6b222f" />

*Figure 3: How trees work visually*

Trees operate as rule-based splits of the data. From the above image, this can mean splitting your data in half based on a condition for a specific feature. The goal of this split is to achieve purity in each class resulting from the split.

So, for example, this method could work both in my classification and regression sense. If an event has 5 tickets sold in the 3rd-to-last week, a Classifier might make this split, where any events with greater than 4 tickets sold in that week in the event timeline will be classified as having ANY sales in the final two weeks. Or, with this split, maybe a Regressor predicts 10 tickets will be sold in the final week for those events.

These rule-based trees essentially partition the feature space into many more regions, especially with the 30 features in my data. But at the end of the day, these models are just learning splits!

### 2.2 The Five Contenders

With that, I selected five tree-based models:

**1. Random Forest**
- **Type:** Ensemble decision trees
- **Strength:** Reduces variance and overfitting
- **Weakness:** Slow to train, won't predict higher than my max sales

**2. Gradient Boosting**
- **Type:** Sequential tree ensemble
- **Strength:** Handles non-linearity, proven on tabular data
- **Weakness:** Slower to train, can overfit

**3. XGBoost**
- **Type:** Optimized gradient boosting
- **Strength:** Built-in regularization, parallel processing, handles missing values
- **Weakness:** Many hyperparameters to tune

**4. LightGBM**
- **Type:** Leaf-wise gradient boosting
- **Strength:** Fast training, efficient memory use
- **Weakness:** Can overfit on small datasets

**5. CatBoost**
- **Type:** Gradient boosting with ordered boosting
- **Strength:** Handles categorical features natively, resistant to overfitting
- **Weakness:** Slower than LightGBM on large datasets

### 2.3 Evaluation Setup

**Metric 1:** Root Mean Squared Error (RMSE) on test set
- Calculated by square-rooting the average residuals (misses)
- Penalizes large errors more than MAE
- Reported in log-sales units, but I'll translate to tickets for business context

**Metric 2:** Precision-Recall Area Under the Curve (PR-AUC) on test set
- Handles class imbalanced data by focuses on how well I find the rare, nonzero sale events
- Considers the cost matrix of a false positive vs a false negative
- Focuses on the hits (true positives, false positives)

**Test Set:** Jan 1-12, 2026 (20,391 events with sales)
- Completely unseen during training
- Represents realistic production scenario

**Naive Baseline:** Predict mean sales = ~21 tickets MAE
- This is what we'd get by always predicting the average
- Any model must beat this to be useful

Let the games begin.

---

## 3. Model 1: Random Forest

I started with the simplest of the approaches: **Random Forest**. A random forest model is a collection of decision trees that are different from one another. Trees are built on different subsets of the data as well as different feature sets at random. 

Random forest models also implement bootstrap aggregating (bagging), which is simply sampling with replacement. The same subset of data, or the same feature space, can be similar between two decision trees.

<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/e095861a-c55f-42d3-a0ed-1926eae3ce4e" />

*Figure 4: Bagging visualized*

Then, Classifiers combine the predictions with majority voting, while Regressors take the average of all tree predictions.

### 3.1 Why Start Here?

Random Forest models have some advantages:
1. **Handles non-linearity:** Many rule-based splits can find the complex, non-linear relationships of my variables
2. **Reduces overfitting:** Averaging the results of many trees converges predictions
3. **Not affected by outliers:** Extreme values don't pull the model down, since rule-based splits are binning my feature space

Below is how I built the Random Forest Classifier and Regressor
```python
rfc = RandomForestClassifier(
    n_estimators=100, max_depth=10, min_samples_leaf=10,
    class_weight='balanced',  # Handle class imbalance
    n_jobs=-1, verbose=0
)
rfc.fit(X_train_cls, y_train_cls)

rfr = RandomForestRegressor(
    n_estimators=100, max_depth=10, min_samples_leaf=10,
    n_jobs=-1, verbose=0
)
rfr.fit(X_train_reg, y_train_reg)
```

This code above has some new hyperparameters I can dive into. 

- `n_estimators = 100` indicates the number of trees I want to build
- `max_depth = 10` is the limit for how deep these trees can grow, or how many decision splits a single tree can make
- `min_samples_leaf = 10` prevents any decision tree from making a niche split that results in less than 10 samples ending up in a leaf/node
- `verbose=0` simply tells VS Code to refrain from printing any log statements

### 3.2 Results

- **Test PR-AUC: 0.8057**
- **Test RMSE: 32.72 tickets**
- **Test MAE: 9.98 tickets**

The Random Forest Regressor was 2x more effective than my Linear Regression, which tells me there is some nonlinearity involved in my sales data. The Classifier also barely outperformed my Logistic Regression, so it seems like this could be the right direction for making any headway. What's interesting is that this classifier was able to correctly predict 89.4% of nonzero-sale events in my data.

---

## 4. Model 2: Gradient Boosting

**Gradient Boosting** works by building those decision trees sequentially, where each new tree corrects the errors of the previous trees. So, with each new tree, they are essentially correcting for the previous ones' mistakes/errors.

### 4.1 How Gradient Boosting Works

1. Start with a simple prediction (mean sales)
2. Build a small decision tree to predict the errors
3. Add this tree's predictions to the current model
4. Repeat 100 times (or however many estimators you choose)

<img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/1b915567-2545-4818-a6a4-64db5ac19708" />

*Figure 5: Gradient Boosting sequential trees*


Each tree is **shallow** (max_depth=4), learning simple patterns. But 100 shallow trees combined create complex decision boundaries.

```python
gbc = GradientBoostingClassifier(
    n_estimators=100, max_depth=4, learning_rate=0.1,
    subsample=0.8, verbose=0
)
gbc.fit(X_train_cls, y_train_cls)

gbr = GradientBoostingRegressor(
    n_estimators=100, max_depth=4, learning_rate=0.1,
    subsample=0.8, verbose=0
)
gbr.fit(X_train_reg, y_train_reg)
```

So.. I can explain those new hyperparameters you see again.

- `learning_rate=0.1` is actually a really important topic in machine learning because it diminishes the amount of impact the subsequent tree's correction provides by multiplying the full prediction by 10% before adding to the model
- `subsample=0.8` adds a layer of randomness to the training, where each tree in the sequence can only look at a random 80% of the training data, which helps prevent memorization and overfitting to specific outliers

### 4.2 Results

- **Test PR-AUC: 0.8113**
- **Test RMSE: 31.97 tickets**
- **Test MAE: 10.22 tickets**

This PR-AUC value outperformed both Random Forest and Logistic Regression, yet the RMSE and MAE on predictions of sales volume showed that this would be a poor model choice in comparison to my Random Forest model. However, when this Gradient Boosting Classifier did predict an event would have sales in the final two weeks, it was correct 72% of the time, the highest precision by far among my trees.

---

## 5. Model 3: XGBoost

Gradient Boosting will grow trees until they hit a depth limit. However, XGBoost is more cautious about potentially overfitting the data.

<img width="1280" height="742" alt="image" src="https://github.com/user-attachments/assets/1318517f-e121-4409-bb00-2ab274a93f03" />

*Figure 6: XGBoost visualization*


Created for Kaggle competitions, it adds:
- Regularization (L1 and L2) to prevent overfitting
- Parallel processing for faster training
- Native handling of missing values
- Built-in cross-validation

```python
xgbc = XGBClassifier(
    n_estimators=100, max_depth=6, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,  # Handle class imbalance
    tree_method='hist', verbosity=0, n_jobs=-1,
    eval_metric='logloss'
)
xgbc.fit(X_train_cls, y_train_cls)

xgbr = XGBRegressor(
    n_estimators=100, max_depth=6, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8,
    tree_method='hist', verbosity=0, n_jobs=-1
)
xgbr.fit(X_train_reg, y_train_reg)
```

Here is where both the complexity of the models and hyperparameters got a little crazy.

- `colsample_bytree=0.8` is similar to subsample in that this picks a random 80% subset of feature columns for my trees to train with
- `scale_pos_weight=scale_pos_weight` is XGBoost's version of `class_weight='balanced'` for my imbalanced class distribution
- `tree_method='hist'` speeds up training by binning my numerical values into histograms before splitting, allowing the model to look at far less splits than looking at every value

### 5.1 Results

- **Test PR-AUC: 0.8121**
- **Test RMSE: 31.97 tickets**
- **Test MAE: 10.42 tickets**

XGBoost had the highest PR-AUC value of all trees, proving that it could catch 91% of nonzero-sale events. It's regressor was on par with Gradient Boosting, yet the model trained in only 6 seconds, which is a real dealbreaker with more and more data.

---

## 6. Model 4: LightGBM

Microsoft's **LightGBM** takes a different approach to tree building. While XGBoost grows trees **level-by-level** (breadth-first), LightGBM grows **leaf-by-leaf** (depth-first), splitting the leaf that reduces loss the most.

<img width="756" height="287" alt="image" src="https://github.com/user-attachments/assets/1226b7d5-bc1f-4d95-b7f5-788045fe4ca4" />

*Figure 7: Level-wise (XGBoost) vs Leaf-wise (LightGBM) tree growth*

Here's how I build the Light GBM:

```python
lgbmc = LGBMClassifier(
    n_estimators=100, max_depth=-1, num_leaves=31,
    learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,  # Handle class imbalance
    verbosity=-1, n_jobs=-1
)
lgbmc.fit(X_train_cls, y_train_cls)

lgbmr = LGBMRegressor(
    n_estimators=100, max_depth=-1, num_leaves=31,
    learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,
    verbosity=-1, n_jobs=-1
)
lgbmr.fit(X_train_reg, y_train_reg)
```

There's only one hyperparameter to comment on this time, and that's `num_leaves`, which gives me the discretion to cap this branch growth at a maximum of 31 end leafs/nodes. Light GBMs don't care about imbalanced trees, so it can follow specific features down to a really niche split, which is nice for exploration.

### 6.1 Results

- **Test PR-AUC: 0.8117**
- **Test RMSE: 31.90 tickets**
- **Test MAE: 10.26 tickets**

**LightGBM is not only the fastest tree-based model** but also the most accurate I found in this tree-based search, with the lowest RMSE value in tickets. Regardless of scale, LGBM was able to most precisely predict those nonzero-sales.

### 5.2 Speed vs Accuracy Trade-Off

This was my first major insight: For this problem, **LightGBM gives you 99% of XGBoost's performance in 1/3 of the time**.

In production, where you might retrain weekly on millions of rows, this speed adds up. You get nearly the same predictions with dramatically faster cycles.

For this project? LightGBM was looking like a winner.

---

## 7. Model 5: CatBoost

Yandex's **CatBoost** ("Categorical Boosting") is similar to standard boosting but brings two new ideas:
1. **Ordered Boosting:** Prevents overfitting by using different data subsets with every new tree
2. **Symmetric Trees:** In every level of the tree, the same split on feature and threshold is used for all nodes in that level, helping regularize my model

<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/2da40a04-f34c-4ec0-8c4a-29949a462372" />

*Figure 8: CatBoost visualized*

```python
cbc = CatBoostClassifier(
    iterations=100, depth=6, learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,  # Handle class imbalance
    verbose=0, thread_count=-1
)
cbc.fit(X_train_cls, y_train_cls)

cbr = CatBoostRegressor(
    iterations=100, depth=6, learning_rate=0.1,
    verbose=0, thread_count=-1
)
cbr.fit(X_train_reg, y_train_reg)
```

### 7.1 Results

- **Test PR-AUC: 0.8087**
- **Test RMSE: 32.21 tickets**
- **Test MAE: 10.22 tickets**

CatBoost performed similarly to the other tree-based models, with an average PR-AUC and RMSE value in comparison. However, this model is helpful when many categories are present in your data because it can train quite quickly (12s).

### 7.2 The Surprising Finding

1. LightGBM: RMSE of 31.90 tickets
2. Gradient Boosting: RMSE of 31.97 tickets
3. XGBoost: RMSE of 31.97 tickets

All three within **>1% of each other**.

This told me something important: **All three algorithms were finding similar patterns in the data**. These rule-based splits were likely similar in many cases amongst models.

This would become critical in Part 7 when I built ensembles.

---

## 8. Head-to-Head Comparison

Let me put all six models side-by-side:

| Model            | RMSE (tickets) | MAE (tickets) | RMSE (log) | MAE (log) | R²     | Time (s) |
|------------------|----------------|---------------|------------|-----------|--------|----------|
| **LightGBM**         | 31.90          | 10.26         | 0.7110     | 0.5286    | 0.6519 | 3.53     |
| GradientBoosting | 31.97          | 10.22         | 0.7025     | 0.5245    | 0.6603 | 554.50   |
| XGBoost          | 31.97          | 10.42         | 0.7109     | 0.5293    | 0.6520 | 5.52     |
| CatBoost         | 32.21          | 10.22         | 0.7001     | 0.5226    | 0.6626 | 10.78    |
| RandomForest     | 32.72          | 9.98          | 0.6955     | 0.5174    | 0.6670 | 219.89   |
| LinearRegression | 77.65          | 13.31         | 0.7042     | 0.5515    | 0.6586 | 0.56     |

<img width="1390" height="489" alt="image" src="https://github.com/user-attachments/assets/4284a413-5286-41c8-9cb0-48fb39a9c99b" />

*Figure 9: Performance comparison bar chart showing tree models clustering together, Ridge far behind*

<img width="1385" height="489" alt="image" src="https://github.com/user-attachments/assets/fd8c0730-f649-44be-a3c6-727a4f722052" />

*Figure 10: PR curves for classification models; all tree models cluster near 0.80 PR-AUC*

### 8.1 The Winners

**LightGBM** won the foundation model showdown for regression, with Gradient Boosting and XGBoost close behind. The top five tree models were separated by only ~0.4 tickets MAE.

For classification (predicting IF sales occur), **XGBoost and LightGBM led with PR-AUC 0.812**.

### 8.2 Performance vs Complexity

An R² of ~0.65 means tree models explain **about 65% of the variance** in log-transformed sales. In business terms, predictions are typically within **±10 tickets** of actual sales on average. Given the inherent noise in secondary ticket markets (72% zero-sales days), this feels like solid baseline performance.

---

## 9. Combining Classification and Regression

I didn't just wrap up here, but actually wanted to connect the dots of the thought process. What I did above was all independent of one another, but I wanted to see if we could combine these classifications and predictions to make room for improvement.

### 9.1 Two-Staged Evaluation

So, I made a pipeline that is performed on ALL of my test data, both nonzero and zero-sale events. First, I'd have a classifier predict if there will be sales, and if my model does predict sales in the final two weeks, my regressor will predict how many, almost acting as a gating function before firing off.

So, I had a for loop test all combinations of classifiers and regressors together to find out how this logical hurdle pipeline works in practice.

### 9.2 Results

| Classifier       | Regressor        | RMSE_log | MAE_log | RMSE_tickets | MAE_tickets |
|------------------|------------------|----------|---------|--------------|-------------|
| **GradientBoosting** | **LightGBM**         | 0.716    | 0.342   | 19.150       | 3.891       |
| RandomForest     | LightGBM         | 0.743    | 0.384   | 19.153       | 3.971       |
| CatBoost         | LightGBM         | 0.754    | 0.400   | 19.154       | 4.002       |
| XGBoost          | LightGBM         | 0.756    | 0.403   | 19.155       | 4.008       |
| LightGBM         | LightGBM         | 0.759    | 0.408   | 19.155       | 4.015       |
| GradientBoosting | GradientBoosting | 0.710    | 0.339   | 19.180       | 3.865       |
| RandomForest     | GradientBoosting | 0.736    | 0.381   | 19.182       | 3.942       |
| CatBoost         | GradientBoosting | 0.748    | 0.397   | 19.184       | 3.973       |
| XGBoost          | GradientBoosting | 0.750    | 0.400   | 19.184       | 3.979       |
| LightGBM         | GradientBoosting | 0.752    | 0.405   | 19.184       | 3.986       |

> **Why is 19.15 so much lower than the 32-33 RMSE from individual regressors?**
>
> The denominator changed. Regression-only models are evaluated on the ~16,900 events
> that actually sold tickets — a hard subset where the model must estimate volume. The
> two-stage pipeline is evaluated on all 58,022 test events, including the ~71% with
> zero sales during the final two weeks. Correctly predicting zero for a zero-sale event
> is easy, and those correct predictions pull the aggregate RMSE down significantly.
>
> **19.15 tickets** is the deployed-system RMSE — how the full pipeline performs in
> production where most events have no sales. The ~32 RMSE figures measure pure
> regression accuracy on active events only. Both are valid; they answer different questions.

Surprisingly enough, this hurdle logic was able to cut my RMSE almost in half in terms of tickets! Logically, this is protecting my error because my classifier models are preventing my regressors from predicting on zero-sale events. 

<img width="1189" height="390" alt="image" src="https://github.com/user-attachments/assets/8408d7a6-10cf-4d6a-a6f3-cd649d5e0ea2" />

*Figure 10: Residuals of nonzero-sale events*

Another interesting takeaway is that, although LightGBM won outright in standalone classification, the Gradient Boosting classifier combined with LGBM regression resulted in the lowest RMSE ticket value amongst all competitors.

<img width="480" height="424" alt="image" src="https://github.com/user-attachments/assets/af0c62cd-b318-49a9-9c2a-6d80c70d9a92" />

*Figure 11: Classification Matrix of Gradient Boosting classifier*

To interpret these error terms, I can say that on average, the stacked Gradient Boosting and Light GBM models acheived, on average, a 4-ticket miss per event, which is pretty fantastic! Having certainty in the secondary market means having a better idea of demand as a proxy, which can be useful for primary markets as well.

## 10. Feature Importance

Now, most models operate as a black box: throw everything in, get something out. But with this project, I wanted to understand what drives these predictions and classifications, arguably the most useful for making decisions about future events. So, I took a look with the `.feature_importances_` function.

### 10.1 Top 10 Features

<img width="1390" height="821" alt="image" src="https://github.com/user-attachments/assets/d5af11bc-fa52-491d-ba44-17c12bdfba72" />

*Figure 12: Feature importance bar chart for both stages of hurdle model*

### 10.2 What This Tells Me

#### Classification

The left chart shows the features used to predict if a ticket will sell.

`sales_total_7d_log_scaled` is the dominant single feature in the classification model. This suggests that recent sales momentumis the strongest indicator of whether future sales will occur.

Other smaller factors include how many active listings are on StubHub for that event, the time until the event, and some venue capacity. But these are negligent factors. I can assume that the classifier is highly sensitive to recent historical trends rather than static event details like the category or capacity of the venue.

#### Regression

The right chart shows the features used to predict how many tickets will sell, given that the classifier predicted at least one sale.

`days_to_event_scaled` is the top predictor, indicating that urgency plays a major factor, like I saw in my EDA. This means the LightGBM correctly identified this urgency pressure as the primary driver of volume.

Unlike the classifier, the regressor uses a more diverse set of features. `sales_total_7d_log_scaled`, `venue_capacity_log_scaled` and more pricing and delta factors all play substantial roles. 

It's also interesting to note that some interaction effects with event types have importance here, whereas they were negligible in the classification stage.

---

## 11. Error Analysis Preview

To further pull out useful information from this black box, I wanted to find out how my hurdle model performed on each category.

### 11.1 Performance by Focus Bucket

| Event Bucket       | Event Count | RMSE (Tickets) | MAE (Tickets) |
|--------------------|-------------|----------------|---------------|
| Major Sports       | 5,909       | 48.44          | 19.19         |
| Other              | 9,225       | 17.79          | 3.64          |
| Minor/Other Sports | 2,596       | 11.46          | 4.13          |
| Festivals          | 157         | 11.16          | 2.99          |
| Broadway & Theater | 18,045      | 10.81          | 1.50          |
| Concert            | 15,818      | 10.70          | 1.96          |
| Comedy             | 6,272       | 5.80           | 1.50          |

**Major Sports** is hardest to predict (~48.5 tickets RMSE, ~19 MAE), likely because it had one of the greatest deviations in aggregate sales among categories.

**Comedy** is easiest (~6 tickets RMSE, 1.5 MAE), possibly because there are only so many seats in a venue while these events sit in the middle pricing tier. These events also have small deviations in sale totals, making them easier to predict in general.

This ~40+ ticket RMSE gap and ~18 ticket MAE gap between best and worst buckets hints at something important: **Maybe different segments need different models?**

This question will drive Part 8's segment-specific analysis.

---

## 12. Lessons Learned

### What Worked

**Tree-based models are the right tool for tabular data.** The 42% improvement over linear regression wasn't marginal - it was decisive.

**Feature engineering paid off.** Those interaction terms I labored over in Part 3? They're in the top 10 features. Venue capacities also matter in regression! Thank goodness.

**The top 3 tree models are nearly tied.** LightGBM, XGBoost, CatBoost all within 1% accuracy. They're all finding the same patterns, just with different approaches.

### What Surprised Me

**LightGBM's speed advantage was larger than expected.** 2.8x faster than XGBoost with 99% of the performance is a game-changer for production as data grows to the millions of events.

**Linear models failed hard, hard.** 42% worse RMSE suggests the relationships in ticket sales are deeply non-linear. This makes sense; buying behavior doesn't follow straight lines.

### What's Next

I have a solid baseline (~19 tickets RMSE, ~4 tickets MAE), but three questions remain:

1. **Could neural networks beat trees?** Part 5 will test this.
2. **Can hyperparameter tuning improve below 19 tickets RMSE?** Part 6 will optimize.
3. **Will combining models beat individual models?** Part 7 will ensemble.

---

## What's Coming: Neural Networks

Tree models dominated with ~4 tickets MAE, explaining ~67% of variance. That's impressive for baseline models.

But what about **neural networks?** Deep learning has revolutionized image classification, natural language processing, and game-playing AI. Could it find non-linear patterns that trees miss?

In **Part 5: Can Neural Networks Beat Gradient Boosting?**, I'll test three architectures:
- Naive feedforward network (2 layers)
- Deep network with heavy regularization (5 layers)
- ResNet-style network with skip connections
