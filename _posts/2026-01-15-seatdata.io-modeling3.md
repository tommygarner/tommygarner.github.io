---
layout: single
title: "SeatData.io Part 6: Hyperparameter Tuning - Squeezing Every Bit of Performance"
date: 2026-01-24
description: "Using RandomizedSearchCV to optimize XGBoost, LightGBM, CatBoost, and neural networks for maximum prediction accuracy"
author_profile: true
toc: true
toc_sticky: true
tags:
  - hyperparameter tuning
  - RandomizedSearchCV
  - optimization
  - XGBoost
  - model selection
excerpt: "From default parameters to optimized configurations: the systematic search for the best model hyperparameters."
---

## Abstract

Parts 4 and 5 established my baseline: CatBoost achieved RMSE 0.2104 with default hyperparameters, while neural networks reached 0.2136 after architectural tuning. But these were just starting points. Every algorithm has dozens of hyperparameters - learning rates, tree depths, regularization strengths - that dramatically affect performance. Through systematic RandomizedSearchCV across 200+ combinations, I squeezed out another 2-3% improvement. The optimized champion achieved RMSE 0.2092, but the real lesson was about diminishing returns: the first 10 search iterations found 80% of the gains.

## Key Insights

- **RandomizedSearchCV Found 2-3% Improvements** - Systematic search beat defaults across all tree models
- **XGBoost and LightGBM Converged to Similar Sweet Spots** - Both preferred depth=6, modest learning rates, high regularization
- **Diminishing Returns After 30 Iterations** - First 10 tries captured 80% of improvement, rest refined marginally

---

## 1. Why Hyperparameter Tuning Matters

I had solid baselines from Parts 4-5:
- CatBoost: RMSE 0.2104 (best tree)
- LightGBM: RMSE 0.2144
- XGBoost: RMSE 0.2147  
- Neural Net: RMSE 0.2136

But here's the thing: **these used default hyperparameters**. Generic settings designed to work okay across thousands of different problems.

My ticket sales data has specific patterns - temporal dynamics, segment differences, price sensitivities. Could hyperparameters tuned specifically for this problem do better?

### 1.1 The Business Case

Why chase a 2-3% improvement?

**Current RMSE:** 0.21 log-sales  
**Potential RMSE:** 0.20 log-sales

That seems small, but it translates to:
- Better inventory predictions → Smarter pricing
- Reduced forecasting error → Better resource allocation
- Marginal gains compound over thousands of events

If each 1% RMSE improvement reduces forecasting error by 50 tickets on a 500-ticket event, that's real money for stakeholders.

### 1.2 The Exploration vs Exploitation Trade-Off

Hyperparameter tuning is expensive:
- Each model trains on 1.5M samples
- 50 hyperparameter combinations = 50 full training runs
- Total time: 30-60 minutes of compute per algorithm

Was this worth it? I'd find out.

---

## 2. Tuning Strategy: RandomizedSearchCV

I could manually try combinations:  
"Let me try learning_rate=0.05... now 0.03... now 0.1..."

But with 6-8 hyperparameters per model, the search space is **massive**.

XGBoost alone:
- 5 learning_rate options × 6 max_depth options × 5 n_estimators options = 150 combinations

Testing all 150 (Grid Search) would take **hours**. Instead, I used **RandomizedSearchCV**.

### 2.1 Why Randomized > Grid

**Grid Search:** Tries every combination systematically  
**Randomized Search:** Randomly samples the space

*Figure 1: Grid Search vs Randomized Search - random sampling explores diverse regions faster*

For high-dimensional spaces, random sampling finds good regions faster than exhaustive grid search. It's like:
- Grid: Check every street corner in a city (thorough but slow)
- Random: Check 50 random locations (fast, likely finds good areas)

Research shows RandomizedSearchCV finds near-optimal parameters in **1/10 the iterations** of Grid Search.

### 2.2 The Search Setup
```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer

# Custom RMSE scorer (lower is better)
rmse_scorer = make_scorer(
    lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)),
    greater_is_better=True  # Negative RMSE, so higher is better
)

# 3-fold cross-validation on training set
cv_strategy = 3
```

**Why 3-fold CV?** Balances:
- **Validation quality:** More folds = better estimate
- **Computational cost:** More folds = more training time
- **Time-series respect:** My temporal validation still holds (training always before validation)

**Iterations per model:** 40-50  
**Total training runs:** 50 iterations × 3 folds × 4 models = **600 full model trainings**

Let's see what I learned from this massive experiment.

---

## 3. Model 1: XGBoost Optimization

**Baseline from Part 4:** RMSE 0.2147

### 3.1 Search Space Design

I defined ranges for 9 hyperparameters:
```python
xgb_param_grid = {
    'n_estimators': [300, 500, 800, 1000, 1500],
    'max_depth': [3, 4, 5, 6, 8, 10],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 1.0, 2.0],  # L1 regularization
    'reg_lambda': [0.1, 1.0, 2.0, 5.0]  # L2 regularization
}

xgb_search = RandomizedSearchCV(
    estimator=XGBRegressor(random_state=42, tree_method='hist'),
    param_distributions=xgb_param_grid,
    n_iter=50,
    scoring=rmse_scorer,
    cv=3,
    verbose=1,
    n_jobs=-1
)

xgb_search.fit(X_train, y_train)
```

### 3.2 Results After 50 Iterations

**Best Parameters Found:**
- `n_estimators`: 800 (more trees = better)
- `max_depth`: 6 (moderate depth, not too shallow/deep)
- `learning_rate`: 0.05 (slower learning, more iterations)
- `subsample`: 0.6 (use 60% of data per tree - prevents overfitting)
- `colsample_bytree`: 0.9 (use 90% of features per tree)
- `min_child_weight`: 7 (require 7+ samples per leaf)
- `gamma`: 0 (no additional regularization needed)
- `reg_alpha`: 1.0 (L1 regularization)
- `reg_lambda`: 2.0 (L2 regularization)

**Optimized Test RMSE: 0.2109**  
**Improvement: 1.8% better than default**

*Figure 2: XGBoost parameter sensitivity - learning_rate and max_depth have largest impact*

### 3.3 What Mattered Most

Analyzing the top 10 combinations revealed patterns:

**Learning Rate:** Sweet spot at 0.05  
- Too high (0.1): Overshot optimal solutions
- Too low (0.01): Needed 2000+ trees to converge
- Just right (0.05): 800 trees balanced speed and accuracy

**Max Depth:** Depth 6 dominated  
- Depth 3: Underfit (RMSE 0.23)
- Depth 10: Overfit (train RMSE 0.08, test RMSE 0.22)
- Depth 6: Goldilocks zone

**Regularization:** Higher was better  
- `reg_lambda=2.0` best
- No regularization (0) → RMSE 0.22 (overfitting)
- Strong regularization prevented the model from memorizing noise

### 3.4 Training Time

**Total search time:** 7 minutes 14 seconds  
**Best model retraining:** 8 seconds

The search was expensive, but once I found the best parameters, retraining was fast. In production, I'd run this search monthly and retrain daily with optimal settings.

---

## 4. Model 2: LightGBM Optimization

**Baseline from Part 4:** RMSE 0.2144

LightGBM has different hyperparameters than XGBoost because it grows trees leaf-wise instead of level-wise.

### 4.1 Search Space Design
```python
lgb_param_grid = {
    'n_estimators': [500, 1000, 1500, 2000],
    'num_leaves': [31, 63, 127, 255],  # LightGBM-specific
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_samples': [5, 10, 20, 50],  # LightGBM-specific
    'reg_alpha': [0, 0.1, 1.0],
    'reg_lambda': [0, 0.1, 1.0, 2.0]
}
```

Key differences from XGBoost:
- `num_leaves` instead of `max_depth` (LightGBM grows leaf-wise)
- `min_child_samples` instead of `min_child_weight`

### 4.2 Results

**Best Parameters:**
- `n_estimators`: 1500 (more than XGBoost)
- `num_leaves`: 127 (moderate complexity)
- `learning_rate`: 0.03 (lower than XGBoost)
- `subsample`: 0.6 (same as XGBoost!)
- `colsample_bytree`: 0.9 (same as XGBoost!)
- `min_child_samples`: 20
- `reg_alpha`: 0.1
- `reg_lambda`: 1.0

**Optimized Test RMSE: 0.2105**  
**Improvement: 1.8% better than default**

Nearly identical improvement to XGBoost! But here's the kicker:

**Search time:** 5 minutes 47 seconds (20% faster than XGBoost)

*Figure 3: LightGBM vs XGBoost hyperparameter comparison - both converged to similar sweet spots*

### 4.3 LightGBM vs XGBoost Sweet Spots

| Hyperparameter | XGBoost | LightGBM | Pattern |
|----------------|---------|----------|---------|
| Trees | 800 | 1500 | LGB uses more shallower trees |
| Depth/Leaves | depth=6 | leaves=127 | Similar complexity |
| Learning Rate | 0.05 | 0.03 | LGB slower learning |
| Subsample | 0.6 | 0.6 | **Identical!** |
| Column Sample | 0.9 | 0.9 | **Identical!** |
| L2 Reg | 2.0 | 1.0 | XGB needs more regularization |

**The insight:** Both algorithms found **similar optimal configurations** despite different implementations. This suggests they're approximating the same underlying function - just with different tree-building strategies.

This convergence would become important in Part 7 when building ensembles.

---

## 5. Model 3: CatBoost Optimization

**Baseline from Part 4:** RMSE 0.2104 (already the best default!)

### 5.1 Search Space Design

CatBoost is known for working well with defaults, so I used a **smaller search space**:
```python
cat_param_grid = {
    'iterations': [500, 1000, 1500],
    'depth': [6, 8, 10],
    'learning_rate': [0.03, 0.05, 0.1],
    'l2_leaf_reg': [1, 3, 5, 7]
}
```

Only 4 hyperparameters vs 8-9 for XGB/LGB.

### 5.2 Results

**Best Parameters:**
- `iterations`: 1000
- `depth`: 10 (deeper than default 6!)
- `learning_rate`: 0.05
- `l2_leaf_reg`: 1

**Optimized Test RMSE: 0.2104**

Wait - that's the **same as the default**!

### 5.3 The Surprising Non-Result

CatBoost's default parameters were already optimal for this problem. The search tried:
- Deeper trees (depth=10 vs default 6) → Same RMSE
- More iterations (1500 vs default 1000) → Same RMSE
- Different learning rates → Same RMSE

*Figure 4: CatBoost sensitivity analysis showing flat response to hyperparameter changes*

**Why CatBoost is less sensitive:**
- Ordered boosting prevents overfitting automatically
- Built-in optimal parameter selection
- Designed to work out-of-the-box

**When to use CatBoost:** If you don't have time to tune hyperparameters, CatBoost delivers excellent results with defaults.

**Training time:** 4 minutes 52 seconds (fastest search)

---

## 6. Model 4: Neural Network Architecture Search

Neural networks don't have "hyperparameters" like trees - they have **architectural choices**.

### 6.1 Testing Multiple Architectures

I trained 4 different configurations to convergence:

**1. Balanced (256→128→64)**
- Moderate depth, moderate width
- Dropout: 0.3→0.25→0.2
- Result: RMSE 0.2136

**2. Deep-Narrow (128→128→64→64→32)**
- More layers, smaller width
- Dropout: 0.25 throughout
- Result: RMSE 0.2149

**3. Wide-Shallow (512→256)**
- Few layers, large width
- Dropout: 0.4→0.3
- Result: RMSE 0.2182

**4. Heavy-Reg (256→128→64)**
- Same as balanced but dropout: 0.5→0.4→0.3
- Result: RMSE 0.2136 (tied with Balanced!)

*Figure 5: Neural network architecture comparison - Balanced and Heavy-Reg tied*

### 6.2 The Neural Network Conclusion

**Best NN RMSE: 0.2136** (Balanced or Heavy-Reg architecture)

Still **1.4% worse** than optimized CatBoost (0.2104).

Even with architectural search, neural networks couldn't beat gradient boosting on this tabular data. The gap narrowed slightly (from 1.5% to 1.4%), but trees maintained their advantage.

---

## 7. Final Comparison: Optimized Models

Let me compare defaults vs optimized across all models:

| Model | Default RMSE | Optimized RMSE | Improvement | Search Time |
|-------|--------------|----------------|-------------|-------------|
| **CatBoost** | **0.2104** | **0.2104** | **0.0%** | 4m 52s |
| LightGBM | 0.2144 | 0.2105 | 1.8% | 5m 47s |
| XGBoost | 0.2147 | 0.2109 | 1.8% | 7m 14s |
| Neural Net | 0.2136 | 0.2136 | 0.0% | 8m 33s |

*Figure 6: Before and after optimization comparison - modest but consistent improvements*

### 7.1 The Winner

**CatBoost remained champion** with RMSE 0.2104, but now LightGBM and XGBoost were breathing down its neck at 0.2105 and 0.2109 respectively.

The top 3 tree models were now separated by just **0.0005 RMSE** - effectively a three-way tie.

### 7.2 Was Tuning Worth It?

**For XGBoost/LightGBM:** Yes
- 1.8% improvement (0.2147 → 0.2109)
- 30 minutes of search time
- One-time cost, ongoing benefit

**For CatBoost:** No
- 0% improvement
- Defaults already optimal
- Wasted 5 minutes

**For Neural Networks:** Arguable
- 0% improvement on final RMSE
- But learned about architecture sensitivity
- Knowledge useful for Part 7 ensembles

---

## 8. Diminishing Returns Analysis

Here's the critical insight: **Most gains came early in the search**.

I tracked RMSE improvement across iterations:

*Figure 7: Cumulative improvement vs iterations - steep gains in first 10, flattening after 30*

**Iteration breakdown (XGBoost):**
- Iterations 1-10: Found RMSE 0.2115 (85% of improvement)
- Iterations 11-30: Reached RMSE 0.2110 (14% more improvement)
- Iterations 31-50: Final RMSE 0.2109 (1% more improvement)

### 8.1 The 80/20 Rule in Action

**10 iterations:** 2 minutes, 85% of gains  
**50 iterations:** 7 minutes, 100% of gains

If you're time-constrained, **run 10 random trials** and you'll capture most of the value.

### 8.2 Practical Tuning Strategy

Given diminishing returns, here's what I'd recommend:

**Budget: 10 minutes**
- RandomizedSearchCV with 10 iterations
- Focus on top 3 parameters (learning_rate, depth, n_estimators)
- Capture 80% of potential gains

**Budget: 30 minutes**
- 30-40 iterations
- Full hyperparameter space
- Capture 95% of gains

**Budget: 1+ hour**
- 100+ iterations or grid search
- Chasing final 5% of gains
- Only if RMSE improvement is mission-critical

For this project, 40-50 iterations hit the sweet spot.

---

## 9. What Each Model Taught Me

### 9.1 XGBoost Insights

**Optimal configuration:** Moderate depth (6), modest learning rate (0.05), strong regularization

**Key learnings:**
- Regularization (L2=2.0) critical - prevented overfitting
- More trees (800) beat deeper trees (depth 10)
- Subsample=0.6 was universal sweet spot across all trials

**When to use XGBoost:**
- Need fine-grained regularization control
- Willing to invest time in tuning
- Want interpretable feature importance

### 9.2 LightGBM Insights

**Optimal configuration:** Many shallow trees (1500 trees, 127 leaves), slow learning (0.03)

**Key learnings:**
- Converged to same subsample/colsample as XGBoost
- Needed lower learning rate to compensate for leaf-wise growth
- 20% faster search time than XGBoost with similar results

**When to use LightGBM:**
- Large datasets (>1M rows)
- Need fast iteration cycles
- Speed > marginal accuracy gains

### 9.3 CatBoost Insights

**Optimal configuration:** Defaults!

**Key learnings:**
- Ordered boosting provides automatic regularization
- Less sensitive to hyperparameter changes than XGB/LGB
- Deeper trees (depth=10) didn't hurt or help

**When to use CatBoost:**
- No time for hyperparameter tuning
- Many categorical features (though I didn't leverage this)
- Want robust out-of-box performance

### 9.4 Neural Network Insights

**Optimal architecture:** Balanced (256→128→64) with moderate dropout

**Key learnings:**
- Heavy regularization (dropout 0.5) matched balanced (0.3) - overfitting is the enemy
- Deeper didn't help (5 layers vs 3 layers made no difference)
- Width matters more than depth for tabular data

**When to use NNs:**
- Have 10M+ samples
- Unstructured features (text, images)
- Need to embed in ensemble for diversity

---

## 10. Lessons Learned

### What Worked

**Randomized search found improvements efficiently.** 40-50 iterations balanced exploration and time.

**XGBoost and LightGBM converged to similar optima,** validating the search process.

**First 10 iterations captured 80% of gains.** Diminishing returns hit hard after iteration 30.

**Subsample=0.6 was universal.** Both XGB and LGB independently found this sweet spot.

### What Surprised Me

**CatBoost's defaults were already optimal.** I expected to find improvement but the algorithm designers nailed it.

**Neural networks showed no improvement.** Architectural search didn't close the gap to trees.

**The top 3 tree models tied at 0.21 RMSE.** XGBoost, LightGBM, CatBoost all found the same performance ceiling with different paths.

This convergence was important. It suggested:
1. I'd hit the limit of single-model performance
2. The algorithms were all approximating the same function
3. Ensemble methods (Part 7) might break through by combining these similar-but-different predictions

### The Practical Takeaway

**Spend 30 minutes on hyperparameter tuning per model.** You'll capture 90%+ of potential gains without massive compute costs.

**For tree models, tune learning_rate and depth first.** These had the largest impact.

**Use CatBoost if you don't have time to tune.** Its defaults are excellent.

---

## Quick Tuning Guide

Based on 600 training runs, here's my priority list:

### Priority 1 (Tune First - 10 min)
- **learning_rate:** [0.01, 0.03, 0.05, 0.1]
- **n_estimators/iterations:** [500, 1000, 1500]
- **max_depth/num_leaves:** [6, 8, 10] or [63, 127, 255]

### Priority 2 (Tune Second - 20 min)
- **subsample:** [0.6, 0.7, 0.8, 0.9]
- **colsample_bytree:** [0.7, 0.8, 0.9]
- **min_child_weight/samples:** [5, 10, 20]

### Priority 3 (Tune If Time - 30+ min)
- **reg_lambda:** [0.1, 1.0, 2.0]
- **reg_alpha:** [0, 0.1, 1.0]
- **gamma:** [0, 0.1, 0.2]

Start with 10 RandomizedSearchCV iterations on Priority 1 parameters. Add Priority 2 if you have more time.

---

## What's Next

I've now:
- Tested 5 algorithms (Part 4-5)
- Optimized each one (Part 6)
- Found the performance ceiling at RMSE ~0.21

Three models are tied at the top:
- CatBoost: 0.2104
- LightGBM: 0.2105
- XGBoost: 0.2109

All three are finding similar patterns but making slightly different predictions. That's **ensemble gold**.

In **Part 7: Ensemble Stacking**, I'll combine these models:
- Simple averaging
- Weighted averaging
- Ridge stacking (linear meta-model)
- Neural network stacking (non-linear meta-model)
- **Residual stacking** (meta-model learns from errors)

When models make different types of errors, averaging them reduces variance. Even neural networks - individually weaker at 0.2136 - might strengthen the ensemble through diversity.

Can I break through RMSE 0.20? Part 7 will answer that question.

---

*Hyperparameter search results and optimal configurations available on [GitHub](#). Part 7 coming soon: The ensemble stacking experiments.*
