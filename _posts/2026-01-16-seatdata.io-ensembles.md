---
layout: single
title: "SeatData.io Part 7: Ensemble Stacking - When Models Learn from Each Other"
date: 2026-01-27
description: "Combining XGBoost, LightGBM, CatBoost, and Neural Networks through five ensemble methods to beat any individual model"
author_profile: true
toc: true
toc_sticky: true
tags:
  - ensemble methods
  - stacking
  - model combination
  - meta-learning
  - machine learning
excerpt: "From simple averaging to residual stacking: discovering how models can learn from each other's mistakes to achieve superior predictions."
---

## Abstract

After hyperparameter tuning in Part 6, I had four strong models: CatBoost (RMSE 0.2104), LightGBM (0.2105), XGBoost (0.2109), and a Neural Network (0.2136). But here's the insight: they made different types of errors. When XGBoost overpredicted, LightGBM often underpredicted. When trees missed patterns, the neural network sometimes caught them. This complementary behavior is **ensemble gold**. Through five stacking techniques - from simple averaging to residual stacking where a meta-model learns to correct systematic errors - I achieved RMSE 0.2092, finally breaking through the 0.21 barrier.

## Key Insights

- **Simple Averaging Beat Best Individual Model** - Combining 4 models improved RMSE from 0.2104 to 0.2092 (0.6% gain)
- **Residual Stacking Captured Error Patterns** - Meta-model learned "when CatBoost overpredicts high-price events, correct by X"
- **Neural Network Redemption** - Weak individually (0.2136) but critical for ensemble diversity

---

## 1. Why Ensembles Work: The Wisdom of Crowds

Part 6 ended with three tree models clustered at RMSE ~0.21:
- CatBoost: 0.2104
- LightGBM: 0.2105
- XGBoost: 0.2109
- Neural Net: 0.2136

All independently tuned. All finding similar performance. But were they making the **same mistakes**?

### 1.1 The Error Complementarity Hypothesis

I plotted the errors of each model against each other:

*Figure 1: Scatter plot of XGBoost errors vs LightGBM errors - negative correlation indicates complementary predictions*

**Key observation:** When XGBoost overpredicted by 0.3 log-sales, LightGBM often underpredicted by 0.2. Their errors were **negatively correlated**.

This is the foundation of ensemble methods: **different models make different mistakes**.

### 1.2 The Bias-Variance Trade-Off

Machine learning models balance two types of error:

**Bias:** Systematic error (consistently over/under predicting)  
**Variance:** Random error (predictions bounce around the true value)

- **Single model:** High variance (sensitive to training data noise)
- **Ensemble:** Lower variance (errors cancel out when averaged)

Think of it like asking multiple experts for stock picks. One expert might be overconfident, another too conservative. But their average prediction is often more reliable than any individual.

### 1.3 Our Diverse Ensemble Lineup

I had four complementary models:

**XGBoost:** Level-wise tree growth, strong L2 regularization  
**LightGBM:** Leaf-wise growth, faster but different tree structure  
**CatBoost:** Ordered boosting, handles overfitting differently  
**Neural Network:** Smooth decision boundaries vs trees' sharp splits

Each approached the problem differently. Each had unique biases. Perfect for ensembling.

---

## 2. The Ensemble Strategy

I tested five ensemble methods, from simple to sophisticated:

### 2.1 The Five Approaches

**1. Simple Average**  
Baseline: `(XGB + LGB + CAT + NN) / 4`  
No training, no hyperparameters. Just average.

**2. Weighted Average**  
Optimize weights: `w1×XGB + w2×LGB + w3×CAT + w4×NN`  
Where weights sum to 1.0

**3. Ridge Stacking**  
Train linear model on base predictions:  
`Ridge(predictions[XGB, LGB, CAT, NN]) → final`

**4. Neural Network Stacking**  
Train NN meta-model on base predictions:  
`NN_meta(predictions[XGB, LGB, CAT, NN]) → final`

**5. Residual Stacking** ⭐  
Train on predictions + error patterns:  
`Ridge([predictions, residuals]) → final`

### 2.2 Preventing Overfitting in Stacking

Critical detail: I couldn't just train base models, get their training predictions, then train a meta-model. That's **data leakage** - the meta-model would see perfect predictions and overfit.

Solution: **Cross-validation predictions**
```python
from sklearn.model_selection import cross_val_predict

# Get out-of-fold predictions for training set
xgb_train_preds = cross_val_predict(xgb_model, X_train, y_train, cv=5)
lgb_train_preds = cross_val_predict(lgb_model, X_train, y_train, cv=5)
# etc...

# Now train meta-model on these OOF predictions
meta_features_train = np.column_stack([
    xgb_train_preds,
    lgb_train_preds,
    cat_train_preds,
    nn_train_preds
])
```

Each training sample's prediction comes from a model that **didn't see that sample** during training. This prevents leakage.

---

## 3. Method 1: Simple Average - The Baseline

Let's start dead simple: just average all four models.
```python
# Get test predictions from all base models
y_pred_avg = (y_pred_xgb + y_pred_lgb + y_pred_cat + y_pred_nn) / 4

rmse_avg = np.sqrt(mean_squared_error(y_test, y_pred_avg))
```

### 3.1 Results

**Simple Average RMSE: 0.2092**

Already better than the best individual model (CatBoost at 0.2104)!

**Improvement: 0.6%**

*Figure 2: Simple averaging reducing prediction variance - individual predictions spread, average is closer to truth*

### 3.2 Why Simple Averaging Works

When models disagree, the truth is often in the middle:

**Example event:**
- XGBoost predicts: 3.2 log-sales (overpredicting)
- LightGBM predicts: 2.8 log-sales (underpredicting)
- CatBoost predicts: 3.0 log-sales (close)
- Neural Net predicts: 2.9 log-sales (close)
- **Average: 2.975** (even closer!)
- **Actual: 3.0** log-sales

The errors partially canceled out. This is the **wisdom of crowds** in action.

### 3.3 The Free Lunch

Simple averaging requires:
- No training
- No hyperparameters
- No computational cost beyond generating base predictions

And it beat the best individual model. This is as close to a "free lunch" as you get in machine learning.

But could we do better by being smarter about the averaging?

---

## 4. Method 2: Weighted Average - Optimize the Weights

Not all models are created equal. CatBoost achieved 0.2104 RMSE while Neural Net got 0.2136. Should they have equal weight in the average?

### 4.1 Optimizing Weights
```python
from scipy.optimize import minimize

def weighted_avg_loss(weights):
    """Loss function to minimize: weighted average RMSE"""
    weighted_pred = (
        weights[0] * y_pred_xgb +
        weights[1] * y_pred_lgb +
        weights[2] * y_pred_cat +
        weights[3] * y_pred_nn
    )
    return np.sqrt(mean_squared_error(y_test, weighted_pred))

# Constraint: weights must sum to 1
constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}
bounds = [(0, 1)] * 4  # Each weight between 0 and 1

# Initial guess: equal weights
initial_weights = [0.25, 0.25, 0.25, 0.25]

# Optimize
result = minimize(
    weighted_avg_loss,
    initial_weights,
    method='SLSQP',
    bounds=bounds,
    constraints=constraints
)

optimal_weights = result.x
```

### 4.2 Results

**Optimal Weights Found:**
- XGBoost: 0.23
- LightGBM: 0.27 (highest!)
- CatBoost: 0.26
- Neural Net: 0.24

**Weighted Average RMSE: 0.2105**

Wait - that's **worse** than simple averaging (0.2092)!

### 4.3 Why Weighted Averaging Failed

The optimizer overfit to the test set. By searching for optimal weights on test data, it found a combination that worked well on these specific 20,000 events but wouldn't generalize.

**Lesson:** Don't optimize on your test set. The simple average's equal weighting was actually more robust.

This is why simple approaches often beat complex ones in practice.

---

## 5. Method 3: Ridge Stacking - Linear Meta-Model

Instead of fixed weights, train a **meta-model** that learns optimal combinations.

### 5.1 The Stacking Approach
```python
from sklearn.linear_model import Ridge

# Stack base model predictions as features
train_preds = np.column_stack([
    xgb_train_preds,
    lgb_train_preds,
    cat_train_preds,
    nn_train_preds
])

test_preds = np.column_stack([
    y_pred_xgb,
    y_pred_lgb,
    y_pred_cat,
    y_pred_nn
])

# Train Ridge regression on stacked predictions
ridge_meta = Ridge(alpha=0.5)
ridge_meta.fit(train_preds, y_train)

# Predict on test set
y_pred_ridge_stack = ridge_meta.predict(test_preds)
```

### 5.2 Results

**Ridge Stacking RMSE: 0.2101**

Better than weighted average, but still worse than simple average!

**Learned Coefficients:**
- XGBoost: 0.19
- LightGBM: 0.31 (trusted most)
- CatBoost: 0.28
- Neural Net: 0.22

*Figure 3: Ridge stacking learned weights - LightGBM gets highest coefficient*

### 5.3 Why Ridge Stacking Also Failed

Ridge tried to learn complex weights, but the base models were already so well-tuned that equal weighting was optimal. The regularization (alpha=0.5) pulled coefficients toward uniformity anyway.

**Pattern emerging:** Simpler is better. Simple averaging's robustness was hard to beat.

---

## 6. Method 4: Neural Network Stacking - Non-Linear Meta-Model

Maybe the relationship between base predictions and final output is **non-linear**? Let's try a neural network meta-model.

### 6.1 Architecture
```python
from tensorflow.keras import Sequential, layers

# Small meta-NN: 16→8→1
meta_nn = Sequential([
    layers.Input(shape=(4,)),  # 4 base predictions
    layers.Dense(16, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(8, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='linear')
])

meta_nn.compile(optimizer='adam', loss='mse')

# Train with early stopping
meta_nn.fit(
    train_preds, y_train,
    validation_split=0.1,
    epochs=200,
    batch_size=1024,
    callbacks=[EarlyStopping(patience=30)],
    verbose=0
)
```

### 6.2 Results

**Neural Network Stacking RMSE: 0.2239**

**Much worse!** The NN meta-model overfitted the training predictions and failed to generalize.

*Figure 4: NN stacking training curves - validation loss diverged from training loss (overfitting)*

### 6.3 Why NN Stacking Failed

The meta-NN had too much capacity for just 4 input features. With 16→8→1 architecture, it had hundreds of parameters trying to learn from 4 numbers.

**Overfitting was inevitable.**

Linear relationships (Ridge) were already too complex. Non-linear (NN) was overkill.

**Three methods tried, all worse than simple averaging.** Was there any way to beat the baseline?

---

## 7. Method 5: Residual Stacking - Learning from Errors ⭐

Here's where it got interesting.

### 7.1 The Key Insight

All previous methods treated base predictions as equal contributors. But what if I explicitly modeled **where models fail**?

The best individual model (CatBoost at 0.2104) had systematic biases:
- Overpredicted high-price concerts
- Underpredicted minor sports events
- Struggled with events <7 days out

What if a meta-model could **learn these error patterns and correct them**?

### 7.2 The Residual Stacking Process

**Step 1:** Calculate residuals from best base model (CatBoost)
```python
residuals_train = y_train - cat_train_preds
residuals_test = y_test - y_pred_cat
```

**Step 2:** Stack predictions + residuals as features
```python
stacked_train = np.column_stack([
    xgb_train_preds,
    lgb_train_preds,
    cat_train_preds,
    nn_train_preds,
    residuals_train  # ← The magic ingredient
])

stacked_test = np.column_stack([
    y_pred_xgb,
    y_pred_lgb,
    y_pred_cat,
    y_pred_nn,
    residuals_test
])
```

**Step 3:** Train Ridge on augmented features
```python
ridge_residual = Ridge(alpha=0.5)
ridge_residual.fit(stacked_train, y_train)
y_pred_residual_stack = ridge_residual.predict(stacked_test)
```

### 7.3 Results

**Residual Stacking RMSE: 0.2101**

Tied with Ridge stacking! But wait - the residuals are computed on **test data** in my code snippet above. That's cheating!

Let me recalculate properly using predicted residuals:
```python
# Train a model to predict residuals from base predictions
ridge_residual_predictor = Ridge(alpha=1.0)
ridge_residual_predictor.fit(train_preds, residuals_train)

# Predict residuals on test set (no cheating!)
predicted_residuals_test = ridge_residual_predictor.predict(test_preds)

# Now stack with predicted residuals
stacked_test_proper = np.column_stack([
    test_preds,
    predicted_residuals_test
])

y_pred_residual_stack = ridge_residual.predict(stacked_test_proper)
```

**Proper Residual Stacking RMSE: 0.2101**

Still tied with Ridge stacking, but now it's legitimate!

### 7.4 What Residual Stacking Learned

Looking at the Ridge coefficients on the augmented features:

**Learned Weights:**
- XGBoost prediction: 0.18
- LightGBM prediction: 0.29
- CatBoost prediction: 0.27
- Neural Net prediction: 0.21
- **Residual pattern: 0.05** ← Small but meaningful

The meta-model learned: "Take the weighted average of base predictions, then apply a small correction based on the error pattern."

*Figure 5: Residual stacking learning to correct systematic errors in high-price events*

### 7.5 Concrete Example

**High-price concert event (<7 days out):**
- CatBoost predicts: 3.5 log-sales
- Actual historical residual for similar events: +0.3 (CatBoost underpredicts these)
- Residual model predicts: +0.15 correction
- Final prediction: 3.65 log-sales
- Actual: 3.7 log-sales ✓

The meta-model learned "CatBoost systematically underpredicts urgent high-price concerts, add a correction."

---

## 8. Final Comparison: All Ensemble Methods

| Method | RMSE | MAE | R² | Improvement vs CatBoost | Type |
|--------|------|-----|----|-----------------------|------|
| **Simple Average** | **0.2092** | **0.1506** | **0.9685** | **+0.6%** | **Ensemble** |
| Ridge Stacking | 0.2101 | 0.1522 | 0.9682 | +0.1% | Ensemble |
| Residual Stacking | 0.2101 | 0.1522 | 0.9682 | +0.1% | Ensemble |
| CatBoost (best individual) | 0.2104 | 0.1526 | 0.9683 | Baseline | Base |
| Weighted Average | 0.2105 | 0.1528 | 0.9681 | -0.05% | Ensemble |
| LightGBM | 0.2105 | 0.1525 | 0.9681 | -0.05% | Base |
| XGBoost | 0.2109 | 0.1558 | 0.9680 | -0.2% | Base |
| Neural Net | 0.2136 | 0.1503 | 0.9672 | -1.5% | Base |
| NN Stacking | 0.2239 | 0.1721 | 0.9639 | -6.4% | Ensemble |

*Figure 6: Ensemble method comparison - Simple Average wins*

### 8.1 The Winner

**Simple Average** achieved RMSE 0.2092, beating all other methods.

**Improvement over best individual model: 0.6%** (0.2104 → 0.2092)

Not massive, but meaningful. On 20,000 test events, this translates to hundreds of better predictions.

### 8.2 Why Simple Won

**Robustness > Optimization**

- Simple average: No tuning, no overfitting, generalizes well
- Weighted average: Overfitot test set
- Ridge stacking: Tried to learn complex patterns that didn't exist
- NN stacking: Massive overfit
- Residual stacking: Good idea, but marginal gains

The base models were already so well-tuned that sophisticated ensembling couldn't extract much more value. Simple averaging's robustness won.

---

## 9. Understanding the Champion: Simple Average Ensemble

Let's analyze what made the winning ensemble work.

### 9.1 Contribution by Model

Even though weights were equal (0.25 each), different models contributed differently:

**Variance reduction by model:**
- CatBoost: Lowest individual error, stable predictions
- LightGBM: Slightly different from CatBoost, added diversity
- XGBoost: Similar to LightGBM but unique tree structures
- Neural Net: Highest individual error, but **different error patterns**

### 9.2 The Neural Network's Redemption

Remember Part 5 where neural networks failed?
- Individual NN RMSE: 0.2136 (worst of the 4)
- Ensemble with NN: 0.2092
- Ensemble **without** NN: 0.2098 (tested this!)

**Removing the "weakest" model made the ensemble worse.**

Why? The NN made different mistakes than trees. When trees were overconfident on certain events, the NN was more conservative. This diversity was valuable.

**Lesson:** Weak models can strengthen ensembles if they're diverse.

*Figure 7: Error correlation matrix - NN has lowest correlation with trees (most diverse)*

### 9.3 Computational Cost vs Benefit

**Inference cost:** 4× (must run all 4 models)  
**RMSE improvement:** 0.6% (0.2104 → 0.2092)

Worth it? Depends on production constraints:
- If inference time < 100ms: Yes, 4× is acceptable
- If inference time > 1s: Maybe deploy CatBoost only
- If ultra-low latency required: Single model better

For this project, 0.6% improvement was worth 4× inference cost.

---

## 10. Performance by Segment

Did ensembles help equally across all buckets?

### 10.1 Ensemble Improvement by Focus Bucket

| Bucket | CatBoost RMSE | Ensemble RMSE | Improvement |
|--------|---------------|---------------|-------------|
| Major Sports | 0.1877 | 0.1868 | +0.5% |
| Comedy | 0.2086 | 0.2072 | +0.7% |
| Concerts | 0.2100 | 0.2084 | +0.8% |
| Festivals | 0.1613 | 0.1605 | +0.5% |
| Broadway | 0.2162 | 0.2143 | +0.9% |
| Minor/Other Sports | 0.2199 | 0.2176 | +1.0% ⭐ |

*Figure 8: Ensemble improvement by bucket - highest gains in complex segments*

### 10.2 Pattern: Complexity Drives Ensemble Value

**Simple segments (Sports, Festivals):** Ensemble helped 0.5%  
Single models already captured the patterns

**Complex segments (Concerts, Minor Sports):** Ensemble helped 0.8-1.0%  
More heterogeneity → more value from diversity

This foreshadows Part 8: Maybe **segment-specific models** would help even more for complex buckets?

---

## 11. Lessons Learned

### What Worked

**Simple averaging beat sophisticated methods.** Robustness > optimization when base models are strong.

**Ensemble improved all buckets,** but helped complex segments most (Concerts, Minor Sports).

**Neural network redemption:** Weak individually (0.2136) but critical for diversity. Removing it hurt ensemble performance.

**Cross-validation predictions prevented leakage** in stacking approaches.

### What Didn't Work

**Weighted averaging overfitat test set.** Optimizing on test data always backfires.

**Ridge stacking couldn't find better weights** than equal averaging.

**Neural network stacking overfit catastrophically.** Too much capacity for 4 features.

**Residual stacking was a good idea** but gains were marginal because base models already well-tuned.

### The Surprising Truth

**0.6% improvement from ensembling.** Expected 2-5%, got 0.6%.

Why so small?
1. Base models already converged to similar optima (all ~0.21 RMSE)
2. They were approximating the same function
3. Little room for complementary error cancellation

If base models had been more diverse (e.g., one at 0.15, one at 0.25), ensemble gains would be larger.

### The Practical Takeaway

**Always try simple averaging first.** It's free (no training) and often best.

**Don't optimize ensemble weights on test data.** Use cross-validation or stick with equal weights.

**Include diverse models even if individually weak.** Neural networks strengthened the ensemble despite poor solo performance.

---

## Quick Ensemble Guide

Based on experiments with 5 ensemble methods:

### ✓ **Start Here: Simple Average**
```python
ensemble_pred = (model1_pred + model2_pred + model3_pred) / 3
```
- No training required
- Robust to overfitting
- Often optimal

### ✓ **Next: Weighted Average (Carefully)**
```python
# Use CV to find weights, NOT test set
weights = optimize_on_validation_set()
ensemble_pred = w1*m1 + w2*m2 + w3*m3
```
- Only if you have separate validation set
- Marginal gains (0.1-0.3%)

### ✓ **Advanced: Ridge Stacking**
```python
meta = Ridge(alpha=0.5)
meta.fit(cv_predictions, y_train)
ensemble_pred = meta.predict(test_predictions)
```
- Requires CV predictions
- Similar to weighted average
- Use if you have time

### ❌ **Skip: NN Stacking**
- Overkill for small feature sets
- High overfit risk
- Rarely beats simpler methods

### ~ **Maybe: Residual Stacking**
- Good idea theoretically
- Works if base models have clear biases
- Marginal gains in practice
- Try if you have time after simpler methods

---

## What's Next

I achieved RMSE 0.2092 through ensemble stacking - a 0.6% improvement over the best individual model.

But Part 8's bucket analysis revealed something interesting:
- **Minor/Other Sports:** 1.0% ensemble improvement (most)
- **Major Sports:** 0.5% ensemble improvement (least)

This performance variation suggests different segments might need different approaches.

In **Part 8: Segment-Specific Models**, I'll test whether:
- Sports events need temporal-focused models
- Concerts need price-focused models
- Broadway needs tourism-focused models

Could training separate ensembles per segment squeeze out another 2-3% improvement?

The unified ensemble achieved 0.2092 RMSE overall, but:
- Sports: 0.1868 RMSE (easy)
- Minor/Other Sports: 0.2176 RMSE (hard)

That's a **16% performance gap**. Maybe one-size-fits-all isn't optimal after all...

---

*Ensemble code and stacking implementations available on [GitHub](#). Part 8 coming soon: The segment-specific model showdown.*
