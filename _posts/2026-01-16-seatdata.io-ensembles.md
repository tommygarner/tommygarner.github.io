---
layout: single
title: "SeatData.io Part 7: Ensemble Stacking - When Complexity Backfires"
date: 2026-01-25
description: "The surprising discovery that simpler models beat complex ensembles for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - ensemble learning
  - model stacking
  - machine learning
  - lessons learned
excerpt: "I had four tuned models. Stacking them should have beaten any individual model. Instead, it made predictions worse. Here's why."
published: false
---

## The Setup

Part 6 delivered four hyperparameter-tuned models:
- **CatBoost:** 31.06 RMSE (best regressor)
- **XGBoost:** 31.24 RMSE
- **LightGBM:** 31.41 RMSE
- **MLP (Neural Net):** 33.87 RMSE

Each model captured different patterns in the data. Theory says: **combine them, and the ensemble should beat any individual model**.

This is Part 7: the story of why that didn't happen.

---

## The Hypothesis

Ensemble stacking works when base models make **complementary errors**:
- Model A overpredicts event X
- Model B underpredicts event X
- Ensemble averages them ‚Üí closer to truth

With four models spanning different architectures (gradient boosting + neural network), I expected:
1. Each model captures unique patterns
2. Errors cancel out when combined
3. Stacked ensemble beats all individuals

**Hypothesis:** Stacking would improve from 31.06 RMSE ‚Üí **~30.0 RMSE** (~3% gain)

---

## What I Tried

I tested five stacking approaches on the training set (5.1M events):

### 1. Simple Average
```python
# Equal weight to all models
y_pred_stack = (pred_catboost + pred_xgboost + pred_lgbm + pred_mlp) / 4
```

### 2. Weighted Average
```python
# Optimize weights via Scipy minimize
from scipy.optimize import minimize

def weighted_avg_rmse(weights, predictions, targets):
    weights = weights / weights.sum()  # normalize
    y_pred = predictions @ weights
    return np.sqrt(mean_squared_error(targets, y_pred))

# Find optimal weights for each model
result = minimize(
    weighted_avg_rmse,
    x0=[0.25, 0.25, 0.25, 0.25],  # start equal
    args=(predictions, y_train_log),
    method='SLSQP',
    bounds=[(0, 1)] * 4
)
```

### 3. Ridge Stacking (Meta-Learner)
```python
from sklearn.linear_model import Ridge

# Use base model predictions as features
X_stack_train = np.column_stack([
    pred_catboost, pred_xgboost, pred_lgbm, pred_mlp
])

# Train Ridge to combine predictions
meta_model = Ridge(alpha=1.0)
meta_model.fit(X_stack_train, y_train_log)
```

### 4. Neural Network Stacking
```python
# 2-layer NN as meta-learner
stack_nn = Sequential([
    Dense(16, activation='relu', input_dim=4),
    Dropout(0.3),
    Dense(8, activation='relu'),
    Dense(1, activation='linear')
])
stack_nn.fit(X_stack_train, y_train_log)
```

### 5. Residual Stacking
```python
# Train model 2 on model 1's errors
residuals_catboost = y_train_log - pred_catboost
xgb_residual_model.fit(X_train, residuals_catboost)

# Final prediction = CatBoost + XGBoost(residuals)
y_pred_final = pred_catboost + pred_xgb_residuals
```

**Cross-validation setup:** 3-fold CV with out-of-fold predictions to prevent data leakage

---

## The Evaluation Problem

Initial results looked promising:

```
Training Set (Rows With Sales Only):
  Weighted Stack: 30.79 RMSE ‚Üê Better than 31.06!
  Simple Average: 31.15 RMSE
  Ridge Stack: 30.92 RMSE
```

But something felt off. The **baseline to beat was 19.15 RMSE** from foundation models (Part 4).

How did I get *worse* from 19.15 ‚Üí 30.79?

### The Disconnect

I discovered two evaluation inconsistencies:

**Foundation models (Part 4):**
- Evaluated on **full test set** (58,022 events)
- Included all events (70% with zero sales)
- RMSE: 19.15 tickets

**My stacking notebook:**
- Evaluated on **rows with sales only** (16,939 events)
- Excluded 70% of test set
- Not comparable to 19.15 baseline

**The problem:** Different evaluation subsets create artificially different metrics. I needed to evaluate on the **same full test set**.

---

## The Full Test Set Evaluation

I ran all models on the complete 58,022-event test set (including zero-sales events):

### Results

| Model | RMSE (tickets) | vs Foundation (19.15) |
|-------|----------------|-----------------------|
| **XGBoost (tuned, no stack)** | **18.98** | ‚úÖ **+0.9% better** |
| LightGBM (tuned, no stack) | 19.11 | ‚úÖ +0.2% better |
| Foundation (Part 4 baseline) | 19.15 | Baseline |
| CatBoost (tuned, no stack) | 19.87 | ‚ö†Ô∏è -3.8% worse |
| **Stacked Ensemble (weighted)** | **21.32** | ‚ö†Ô∏è **-11.3% worse** |

---

## The Shocking Discovery

**Three critical findings:**

### 1. Hyperparameter Tuning Worked
- XGBoost improved: 19.15 ‚Üí **18.98 RMSE**
- LightGBM improved: 19.15 ‚Üí **19.11 RMSE**
- Tuning delivered on its promise

### 2. Stacking Made Things WORSE
- Individual XGBoost: 18.98 RMSE
- Stacked ensemble: 21.32 RMSE
- **Gap: 12% worse by combining models**

### 3. The Champion Was Simple
- Best model: **XGBoost alone** (no stacking)
- No complex ensemble needed
- **Simpler beat more complex**

This contradicted every expectation about ensemble learning.

---

## Why Did Stacking Fail?

I investigated the root causes:

### 1. Extreme Model Correlation

I calculated prediction correlations between all base models:

```python
import pandas as pd

pred_df = pd.DataFrame({
    'CatBoost': pred_catboost,
    'XGBoost': pred_xgboost,
    'LightGBM': pred_lgbm,
    'MLP': pred_mlp
})

correlation_matrix = pred_df.corr()
```

**Results:**
```
           CatBoost  XGBoost  LightGBM    MLP
CatBoost      1.000    0.994     0.992  0.951
XGBoost       0.994    1.000     0.996  0.948
LightGBM      0.992    0.996     1.000  0.947
MLP           0.951    0.948     0.947  1.000
```

**Tree models were 99%+ correlated**. They made almost identical predictions on every event.

### 2. Lack of Diversity

All three top models were gradient-boosted decision trees:
- CatBoost: Ordered boosting
- XGBoost: Traditional boosting
- LightGBM: Leaf-wise boosting

Despite different implementations, they:
- Used the same features
- Learned similar decision boundaries
- Made nearly identical mistakes

**The MLP (neural network) was different** (95% correlation), but:
- It was the worst performer (33.87 RMSE on regression subset)
- Low weight in ensemble (~5%)
- Not enough to add diversity

### 3. No Complementary Errors

I analyzed prediction errors across models:

```python
# Where does each model make large errors?
large_errors_catboost = np.abs(y_true - pred_catboost) > 50
large_errors_xgboost = np.abs(y_true - pred_xgboost) > 50

# Do models make errors on different events?
overlap = (large_errors_catboost & large_errors_xgboost).mean()
print(f"Error overlap: {overlap:.1%}")
```

**Result:** 94% overlap in large errors

When CatBoost mispredicted an event by 50+ tickets, XGBoost and LightGBM made the **same mistake** 94% of the time.

**Ensemble stacking only helps when errors are uncorrelated**. Here, they were nearly perfectly correlated.

### 4. Overfitting in Weight Optimization

The weighted average approach optimized weights on training set predictions:

**Optimal weights found:**
- CatBoost: 0.42
- XGBoost: 0.38
- LightGBM: 0.15
- MLP: 0.05

These weights were tuned to minimize training error. But on the test set:
- Training RMSE: 30.79 (looked good!)
- Test RMSE: 21.32 (much worse)

**The optimization overfit**. Weighted ensemble memorized training patterns that didn't generalize.

---

## What Should Have Worked (But Didn't)

Classic ensemble success stories rely on:

**1. Diverse Base Models**
- Example: Random Forest (many shallow trees) + Gradient Boosting (few deep trees) + Logistic Regression (linear)
- My case: Three gradient boosting variants ‚Üê too similar

**2. Different Feature Subsets**
- Example: Model A uses price features, Model B uses venue features
- My case: All models used identical features

**3. Different Training Data**
- Example: Bagging (bootstrap samples)
- My case: All models trained on full dataset

**4. Complementary Errors**
- Example: Model A overpredicts low sales, Model B underpredicts them
- My case: All models made the same mistakes

I had **none of the four conditions** for successful ensembling.

---

## Lessons Learned

### 1. Model Correlation is the Killer Metric

Before investing in ensemble stacking:
```python
# Check base model correlations
if min(correlations) > 0.95:
    print("‚ö†Ô∏è Models too correlated - stacking won't help")
```

**Rule of thumb:** If correlations > 0.95, save your time.

### 2. Simpler Models Beat Complex Ensembles

- **XGBoost alone:** 18.98 RMSE (champion)
- **5-model weighted stack:** 21.32 RMSE (failed)

Adding complexity didn't add value. The simplest solution won.

**Occam's Razor applies to ML:** Prefer the simpler model with equal or better performance.

### 3. Different Architectures ‚â† Diversity

I thought tree models + neural network = diversity.

Reality: **Architecture diversity means nothing if predictions are 95%+ correlated**.

Diversity requires:
- Different features or feature engineering
- Different training strategies
- Complementary weaknesses

### 4. Cross-Validation Can Mislead

My cross-validation results:
- CV RMSE: 30.79 (promising!)
- Test RMSE: 21.32 (disaster)

**Why?** Weight optimization overfit on CV folds. The test set revealed the truth.

**Lesson:** Always validate ensembles on a held-out test set before declaring success.

### 5. Negative Results Are Valuable

This experiment "failed" - stacking made predictions worse.

But I learned:
- **Hyperparameter tuning worked** (18.98 < 19.15)
- Tree model homogeneity limits ensemble gains
- When to stop adding complexity
- How to diagnose why ensembles fail

**For recruiting:** Showing negative results + root cause analysis demonstrates:
- Scientific thinking
- Intellectual honesty
- Ability to debug complex systems
- Knowing when to stop and ship the simpler solution

---

## The Final Model Choice

After testing five stacking approaches, the winner was:

**XGBoost (tuned, no stacking)**
- **RMSE: 18.98 tickets**
- 0.9% better than foundation baseline (19.15)
- Simpler to deploy (one model vs. five-model pipeline)
- Easier to explain to stakeholders
- Faster inference (40ms vs. 200ms for ensemble)

**Two-stage pipeline:**
1. **Classifier:** XGBoost (PR-AUC 0.8158) ‚Üí Will event have sales?
2. **Regressor:** XGBoost (RMSE 18.98) ‚Üí How many tickets?

**Deployment advantages:**
- Single algorithm to maintain
- No weight optimization to retrain
- Clear feature importance from one model
- Stakeholders understand "one tuned XGBoost" better than "weighted ensemble of five models"

---

## When Does Stacking Work?

Stacking wasn't right for this problem, but it works when:

### ‚úÖ Use Stacking When:

**1. Base models are truly diverse (correlation < 0.90)**
- Example: Linear model + Tree model + Neural net with distinct predictions
- Test: Check correlation matrix before investing time

**2. Models have complementary strengths**
- Example: Model A excels on high-volume events, Model B on low-volume
- Test: Analyze errors across prediction buckets

**3. Large dataset for meta-learner training**
- Example: 1M+ training samples so meta-learner doesn't overfit
- My case: 5.1M samples was enough, but base models were too similar

**4. Prediction diversity matters more than raw accuracy**
- Example: Recommender systems (diversity in recommendations)
- My case: Pure accuracy on RMSE - no diversity needed

### ‚ùå Skip Stacking When:

**1. All models are similar architectures**
- My case: Three gradient boosting variants

**2. Correlations > 0.95**
- My case: Tree models 99%+ correlated

**3. One model already dominates**
- My case: XGBoost was already best

**4. Simplicity is a priority**
- My case: Production deployment favors simpler models

---

## Alternative Approaches That Might Have Worked

If I had more time to explore diversity:

### 1. Feature-Based Submodels
```python
# Model 1: Price-focused features only
price_features = ['get_in', 'listings_median', 'price_tier', ...]
model_prices = XGBRegressor().fit(X_train[price_features], y_train)

# Model 2: Venue-focused features only
venue_features = ['venue_capacity', 'venue_state', 'is_major_city', ...]
model_venues = XGBRegressor().fit(X_train[venue_features], y_train)

# Model 3: Temporal features only
time_features = ['days_to_event', 'day_of_week', 'is_holiday_week', ...]
model_temporal = XGBRegressor().fit(X_train[time_features], y_train)

# Ensemble combines three specialized models
```

**Expected correlation:** 0.75-0.85 (lower than 0.99)

### 2. Different Loss Functions
```python
# Model 1: Optimize for RMSE (most errors equal)
model_rmse = XGBRegressor(objective='reg:squarederror')

# Model 2: Optimize for MAE (robust to outliers)
model_mae = XGBRegressor(objective='reg:absoluteerror')

# Model 3: Optimize for quantile (predict median)
model_median = XGBRegressor(objective='reg:quantileerror', quantile_alpha=0.5)
```

**Expected:** Different loss functions ‚Üí different learned patterns ‚Üí lower correlation

### 3. Category-Specific Models
```python
# Separate model per event category
models = {}
for category in ['NBA', 'NFL', 'Concert', 'Theater']:
    data_subset = train_df[train_df['focus_bucket'] == category]
    models[category] = XGBRegressor().fit(data_subset[features], data_subset['target'])

# Prediction routes to category-specific model
y_pred = models[event_category].predict(X_test)
```

**Expected:** Category-specialized models capture unique patterns

But with **18.98 RMSE already achieved**, the juice wasn't worth the squeeze. Diminishing returns had set in.

---

## Practical Takeaways

### For Your Next Ensemble Project:

**Before you start:**
1. ‚úÖ Train diverse base models (different algorithms/features/data)
2. ‚úÖ Calculate prediction correlations (`df.corr()`)
3. ‚úÖ If correlation > 0.95, stop - stacking won't help
4. ‚úÖ If one model dominates, ship that model

**During experimentation:**
1. ‚úÖ Use out-of-fold predictions (prevent data leakage)
2. ‚úÖ Test on held-out test set (catch overfitting)
3. ‚úÖ Measure inference latency (ensembles are slower)
4. ‚úÖ Check if gains justify added complexity

**After results:**
1. ‚úÖ Compare ensemble vs. best individual model
2. ‚úÖ Analyze error correlations (complementary or redundant?)
3. ‚úÖ Consider deployment complexity (5 models vs. 1 model)
4. ‚úÖ Document negative results (valuable for future projects)

### Time Investment vs. Value:

**Hyperparameter tuning (Part 6):**
- Time: 30 min per model √ó 4 models = 2 hours
- Result: 19.15 ‚Üí 18.98 RMSE ‚úÖ **+0.9% improvement**
- Worth it: Yes - clear ROI

**Ensemble stacking (Part 7):**
- Time: 4 hours (5 approaches √ó 3-fold CV)
- Result: 18.98 ‚Üí 21.32 RMSE ‚ö†Ô∏è **-12% regression**
- Worth it: No - made things worse

**Key lesson:** Not every ML technique improves every problem. Know when to stop.

---

## Results Summary

### Final Model Performance (Test Set: 58,022 events)

| Model | RMSE (tickets) | vs Baseline | Deploy? |
|-------|----------------|-------------|---------|
| **XGBoost (tuned)** | **18.98** | ‚úÖ +0.9% | **YES** |
| LightGBM (tuned) | 19.11 | ‚úÖ +0.2% | Backup |
| Foundation | 19.15 | Baseline | - |
| CatBoost (tuned) | 19.87 | ‚ö†Ô∏è -3.8% | No |
| MLP | 21.14 | ‚ö†Ô∏è -10.4% | No |
| Weighted Stack | 21.32 | ‚ö†Ô∏è -11.3% | **NO** |
| Ridge Stack | 21.45 | ‚ö†Ô∏è -12.0% | No |
| Simple Average | 21.56 | ‚ö†Ô∏è -12.6% | No |

**Champion:** XGBoost alone (no ensemble)

**Why:** Simplest, fastest, most accurate

---

## What This Taught Me About Production ML

### 1. Complexity is a Liability

More models = more maintenance:
- 5 models to retrain on new data
- 5 model files to version control
- 5 potential failure points in production
- Harder to debug when predictions are wrong

**One tuned XGBoost:**
- 1 model to maintain
- Clear feature importance for debugging
- Faster inference (40ms vs. 200ms)
- Easier to explain to non-technical stakeholders

### 2. Stakeholder Communication

**Explaining the ensemble approach:**
"We combine five models using optimized weights calculated via cross-validated residual minimization..."
*Eyes glaze over*

**Explaining the simple approach:**
"We use XGBoost tuned specifically for ticket sales data. It predicts 18.98 tickets average error."
*Nods of understanding*

Simpler models are easier to trust and deploy.

### 3. When to Stop Optimizing

I could have continued:
- Try 10 more meta-learner architectures
- Tune ensemble weights per category
- Test mixture-of-experts gating

**But:** 18.98 RMSE was already:
- Better than baseline (19.15)
- Good enough for business value
- Simple enough to deploy confidently

**Perfect is the enemy of shipped.**

---

## Conclusion

I set out to beat individual models with ensemble stacking. Instead, I discovered:

**‚úÖ Hyperparameter tuning worked**
- 19.15 ‚Üí 18.98 RMSE (0.9% improvement)

**‚ö†Ô∏è Ensemble stacking failed**
- 18.98 ‚Üí 21.32 RMSE (12% regression)

**üîë Root cause: Model correlation**
- Tree models 99%+ correlated
- No error diversity = no ensemble gains

**üèÜ Champion: XGBoost alone**
- Simplest solution
- Best performance
- Easiest to deploy

**The meta-lesson:** Not every advanced ML technique improves every problem. Sometimes the simple, well-tuned model is the right answer.

**For recruiting:** This project showcases:
- Rigorous experimentation (tried 5 stacking approaches)
- Root cause analysis (diagnosed correlation issue)
- Intellectual honesty (reported negative results)
- Engineering judgment (chose simpler solution)
- Production mindset (deployment complexity matters)

---

*Full code for ensemble experiments and correlation analysis available on [GitHub](https://github.com/yourusername/seatdata). Next: Part 8 - Deploying the champion model to production.*
