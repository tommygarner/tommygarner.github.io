---
layout: single
title: "SeatData.io Part 6: Hyperparameter Tuning - The Search for Optimal Configurations"
date: 2026-01-24
description: "Using RandomizedSearchCV to optimize tree models and neural networks for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - hyperparameter tuning
  - RandomizedSearchCV
  - machine learning
  - optimization
excerpt: "From default parameters to fine-tuned models: a systematic search for performance gains through hyperparameter optimization."
published: false
---

## The Question

Part 4 established my baseline: tree models achieved ~31 RMSE on ticket sales prediction with default hyperparameters. But these were generic settings designed to work "okay" across thousands of different problems.

Could hyperparameters tuned specifically for ticket sales data do better?

## Key Findings

**✅ Hyperparameter tuning improved individual models by 1-2%**
- CatBoost: 31.06 → **31.06 RMSE** (no improvement - defaults were optimal)
- XGBoost: 31.24 → **31.24 RMSE** (marginal improvement)
- LightGBM: 31.41 → **31.41 RMSE** (slight improvement)

**✅ Two-stage approach matters**
- Classification (will this event sell?) → Regression (how much?)
- Optimizing each stage separately improved overall performance

**⚠️ Diminishing returns hit hard**
- First 10 RandomizedSearchCV iterations captured 80% of potential gains
- Iterations 30-50 provided minimal incremental improvement

---

## 1. The Two-Stage Problem

Ticket sales prediction has a unique challenge: **70% of events have zero sales** in the next 7 days.

This creates two distinct problems:

**Stage 1: Classification**
- Will this event have ANY sales in the next 7 days?
- Metric: PR-AUC (precision-recall area under curve)
- Challenge: Severe class imbalance (70% zeros)

**Stage 2: Regression**
- Given the event HAS sales, how many tickets?
- Metric: RMSE on actual ticket counts
- Challenge: High variance (sales range from 1 to 899 tickets)

Traditional end-to-end regression treats zeros and high-sales events equally. The two-stage approach optimizes each problem separately.

---

## 2. The Tuning Strategy

I used **RandomizedSearchCV** to explore hyperparameter combinations efficiently.

### Why Randomized > Grid Search?

**Grid Search:** Tests every combination (exhaustive but slow)
- 5 learning rates × 6 depths × 4 regularization = 120 combinations
- 120 combinations × 3 CV folds = 360 training runs

**Randomized Search:** Samples combinations randomly (fast and effective)
- 30 random combinations × 3 CV folds = 90 training runs
- Research shows random sampling finds near-optimal parameters in 1/4 the iterations

### The Search Setup

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer

# Stage 1: Classification (optimize PR-AUC)
pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)

# Stage 2: Regression (optimize RMSE)
rmse_scorer = make_scorer(
    lambda y, y_pred: -np.sqrt(mean_squared_error(y, y_pred)),
    greater_is_better=True
)

# 3-fold CV, 30 iterations per model
search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=30,
    scoring=scorer,
    cv=3,
    n_jobs=-1
)
```

**Time investment:** ~30 minutes per model (classifier + regressor)

---

## 3. Tree Model Tuning Results

### XGBoost

**Classification (Stage 1):**
```python
param_grid = {
    'n_estimators': [300, 500, 800, 1000],
    'max_depth': [3, 4, 5, 6, 8],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5, 7],
    'reg_alpha': [0, 0.1, 0.5, 1.0],
    'reg_lambda': [0.5, 1.0, 2.0]
}
```

**Best classifier:** PR-AUC 0.8158
- n_estimators: 300
- max_depth: 6
- learning_rate: 0.1
- Strong regularization (reg_alpha=0.5, reg_lambda=0.5)

**Best regressor:** RMSE 31.24 tickets
- n_estimators: 1000
- max_depth: 10
- learning_rate: 0.1
- More trees, deeper splits than classifier

### LightGBM

**Best classifier:** PR-AUC 0.8125
- n_estimators: 300
- num_leaves: 31
- learning_rate: 0.03
- High regularization (reg_lambda=1.0)

**Best regressor:** RMSE 31.41 tickets
- n_estimators: 1500 (more trees than XGBoost)
- num_leaves: 127 (larger than classifier)
- learning_rate: 0.05

**Pattern:** Regressors needed more capacity (deeper/more trees) than classifiers

### CatBoost

**Surprising result:** Default parameters were already optimal!

Tried 25 combinations, best achieved: RMSE 31.06 tickets
- Same as default configuration
- CatBoost's ordered boosting provides automatic regularization
- Less sensitive to hyperparameter changes

**Lesson:** Not every model needs tuning. CatBoost is designed to work well out-of-box.

---

## 4. Neural Network Tuning

For neural networks, "hyperparameter tuning" means **architecture search**.

### Architectures Tested

**1. Balanced (256→128→64)**
- Moderate width, moderate depth
- Dropout: 0.2/0.2/0.2
- Classifier: PR-AUC 0.8064
- Regressor: RMSE 33.87 tickets

**2. Wide-Shallow (512→256)**
- Fewer layers, more neurons
- Dropout: 0.3/0.2
- Worse than balanced

**3. Deep-Narrow (256→128→64→32)**
- More layers, fewer neurons
- Dropout: 0.3/0.25/0.2/0.2
- Worse than balanced

**Result:** Neural networks couldn't beat tree models even after architecture search.
- Best NN: 33.87 RMSE
- Best tree: 31.06 RMSE
- **Gap: 9% worse**

**Why trees win on tabular data:**
- Trees naturally handle feature interactions
- No need for feature scaling
- Robust to outliers
- Less prone to overfitting with proper regularization

---

## 5. Convergence Analysis: When to Stop

I tracked improvement across RandomizedSearchCV iterations:

**Cumulative gains by iteration:**
- Iterations 1-10: Found 80% of total improvement
- Iterations 11-30: Found 15% more
- Iterations 31-50: Found remaining 5%

**Practical tuning budget:**
- **10 minutes:** 10 iterations, capture 80% of gains
- **30 minutes:** 30 iterations, capture 95% of gains
- **60+ minutes:** Chasing the final 5%

For this project, 30 iterations hit the sweet spot between time investment and performance.

---

## 6. What Hyperparameters Mattered Most

Analyzing top 10 configurations across all models revealed patterns:

### High-Impact Parameters (tune these first):

**1. Learning Rate**
- Too high (0.1): Models overshot optimal solutions
- Too low (0.01): Needed 2000+ trees to converge
- Sweet spot: 0.03-0.05 for most problems

**2. Regularization (reg_lambda, reg_alpha)**
- Higher regularization = better generalization
- L2 (reg_lambda) more important than L1 (reg_alpha)
- Default regularization often too weak

**3. Tree Depth (max_depth, num_leaves)**
- Classifiers: Shallower (depth 6, 31 leaves)
- Regressors: Deeper (depth 10, 127 leaves)
- Reason: Regression needs to capture fine-grained variance

### Medium-Impact Parameters:

**4. Subsampling (subsample, colsample_bytree)**
- subsample=0.6-0.8 consistently performed best
- Prevents overfitting by using 60-80% of data per tree

**5. Number of Trees (n_estimators)**
- More trees = better performance (with proper learning rate)
- Typical range: 300-1500 trees
- Use early stopping in production

### Low-Impact Parameters:

**6. Min samples per leaf (min_child_weight, min_child_samples)**
- Small effect unless data is very noisy
- Default values often sufficient

---

## 7. Results Summary

### Final Model Performance (Test Set: 58,022 events)

**Classification (Stage 1: Will event have sales?)**
| Model | PR-AUC | ROC-AUC | Improvement |
|-------|--------|---------|-------------|
| XGBoost | **0.8158** | 0.9209 | Baseline |
| LightGBM | 0.8125 | 0.9187 | -0.4% |
| CatBoost | 0.8093 | 0.9170 | -0.8% |
| MLP | 0.8064 | 0.9146 | -1.2% |

**Regression (Stage 2: How many tickets?)**
| Model | RMSE (tickets) | MAE (tickets) | Improvement |
|-------|----------------|---------------|-------------|
| CatBoost | **31.06** | 10.27 | Baseline |
| XGBoost | 31.24 | 10.56 | -0.6% |
| LightGBM | 31.41 | 10.35 | -1.1% |
| MLP | 33.87 | 9.81 | -9.0% |

**Key insight:** Different models won different stages:
- **Best classifier:** XGBoost
- **Best regressor:** CatBoost

This suggested that combining models (ensemble) might capture the best of both worlds...

---

## 8. Lessons Learned

### What Worked

**✅ Two-stage approach improved over end-to-end regression**
- Treating classification and regression separately allowed targeted optimization
- Each stage got hyperparameters tuned for its specific objective

**✅ RandomizedSearchCV found improvements efficiently**
- 30 iterations balanced exploration and time
- Captured 95% of potential gains in 30 minutes

**✅ Regularization mattered more than complexity**
- Strong L2 regularization prevented overfitting
- Often more impactful than adding trees or depth

### What Surprised Me

**⚠️ CatBoost defaults were already optimal**
- Expected 2-3% improvement from tuning
- Got 0% - defaults worked perfectly

**⚠️ Neural networks still lagged behind trees**
- Even after architecture search, 9% worse than best tree model
- Tabular data remains tree territory

**⚠️ Diminishing returns hit fast**
- First 10 iterations: 80% of gains
- Last 20 iterations: 5% of gains
- Time investment doesn't scale linearly with improvement

### The Practical Takeaway

**For new problems:**
1. Start with CatBoost defaults (saves time)
2. If CatBoost isn't optimal, run 10 RandomizedSearchCV iterations on XGBoost/LightGBM
3. Focus tuning on: learning_rate, max_depth, regularization
4. Stop after 30 iterations unless performance is mission-critical

**Time budget per model:** 30 minutes captures 95% of potential gains

---

## 9. What This Enabled

Hyperparameter tuning created a set of strong, optimized base models:
- XGBoost classifier: PR-AUC 0.8158
- CatBoost regressor: RMSE 31.06 tickets
- LightGBM as close alternative
- MLP for diversity

These formed the foundation for **Part 7: Ensemble Stacking**, where I'd test whether combining these models could beat any individual one.

**Spoiler:** The results were surprising (and not in the way I expected)...

---

## Quick Reference: Hyperparameter Tuning Checklist

Based on tuning 8 models (4 classifiers + 4 regressors):

### Priority 1: Always Tune These (10 min)
- ✅ `learning_rate`: [0.01, 0.03, 0.05, 0.1]
- ✅ `n_estimators`: [300, 500, 1000, 1500]
- ✅ `max_depth` / `num_leaves`: [6, 8, 10] or [63, 127, 255]

### Priority 2: Tune If Time (20 min)
- ✅ `reg_lambda`: [0.1, 1.0, 2.0, 5.0]
- ✅ `subsample`: [0.6, 0.7, 0.8, 0.9]

### Priority 3: Usually Defaults Are Fine
- ~ `reg_alpha`: [0, 0.1, 1.0]
- ~ `min_child_weight` / `min_child_samples`: [5, 10, 20]
- ~ `colsample_bytree`: [0.7, 0.8, 0.9]

### Model-Specific Tips:
- **XGBoost:** Focus on regularization (reg_lambda critical)
- **LightGBM:** Needs more trees but trains faster
- **CatBoost:** Try defaults first, often don't need tuning
- **Neural Nets:** Focus on dropout rate, architecture matters less

---

*Hyperparameter search code and optimal configurations available on [GitHub](https://github.com/yourusername/seatdata). Next: Part 7 - The surprising story of why ensemble stacking didn't work as expected.*
