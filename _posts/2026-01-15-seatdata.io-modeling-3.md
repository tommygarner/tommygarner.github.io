---
layout: single
title: "SeatData.io Part 6: Hyperparameter Tuning"
date: 2026-01-15
description: "Using RandomizedSearchCV to optimize tree models and neural networks for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - hyperparameter tuning
  - RandomizedSearchCV
  - machine learning
  - optimization
excerpt: "Investigating hyperparameter levers of classifiers and regressors to optimize performance gains in the form of better classification and tighter predictions."
published: true
---

<img width="612" height="306" alt="image" src="https://github.com/user-attachments/assets/d38fcf18-5234-464f-ba55-e33a6ac7ba07" />


## Abstract

In Parts 4 and 5, I was able to successfully model my SeatData.io ticket data with nearly default hyperparameters. This post documents a systematic hyperparameter search across tree-based models and neural networks using RandomizedSearchCV. Investigating a 12+ dimensional search space with 200+ parameter combinations, I found that **CatBoost achieved 31.06 ticket RMSE** for regression alone and **XGBoost hits 0.8158 PR-AUC** for classification, while pushing the needle for the two-stage approach to **18.98 ticket RMSE**. I also found that **30 iterations capture 95% of potential gains**, with diminishing returns following, as well as the **learning rate and tree depth** as dominant levers to pull in importance.

---

## Key Findings

**Full Pipeline Performance**
- **Best Combination:** XGBoost clf + XGBoost reg = **18.98 RMSE** (on my full test set)
- **Improvement over Part 4 baseline:** **-0.9% RMSE (-0.17 tickets)**
- **Time invested:** ~180 minutes (90 min per model)
- **New baseline established**

**Individual Model Performance**
- **Best regressor:** CatBoost (31.06 RMSE on regression subset)
- **Best classifier:** XGBoost (0.8158 PR-AUC)
- **Trees beat neural networks:** Even after tuning, gradient boosting dominated

**Search Efficiency**
- **30 iterations captured 95% of gains** (diminishing returns after)
- **First 10 iterations:** 80% of total improvement
- **RandomizedSearchCV:** 4√ó faster than grid search

**Key Parameters to Tune**
- **Learning rate:** 45% of importance
- **Tree depth/leaves:** 32% of importance
- **Regularization (L1/L2):** 15% of importance
- **Everything else:** <10% combined

---

## 1. My Two-Stage Problem

Ticket sales prediction has a unique challenge: **70% of events have zero sales** in the next 7 days.

This severe class imbalance creates two distinct problems:

### Stage 1: Classification
- **Question:** Will this event have ANY sales in the next 7 days?
- **Metric:** PR-AUC (precision-recall area under curve)
- **Challenge:** Severe imbalance (70% zeros, 30% positives)
- **Goal:** Maximize recall while maintaining precision

### Stage 2: Regression
- **Question:** Given the event HAS sales, how many tickets will sell?
- **Metric:** RMSE on actual ticket counts
- **Challenge:** High variance (sales range from 1 to 899 tickets)
- **Goal:** Minimize prediction error on non-zero sales

> **Why not end-to-end regression?** Traditional regression treats zeros and high-sales events equally, leading to poor performance on both. The two-stage approach allows independent optimization of each sub-problem.

**Data Split:**
- **Classification:** 5.1M training events, 58K test events
- **Regression:** 1.4M training events (with sales), 16.9K test events
- **Features:** 29 engineered features (price, venue, temporal, categorical)

---

## 2. Tuning Strategy

I used **RandomizedSearchCV** to explore hyperparameter combinations efficiently.

### 2.1 What is RandomizedSearchCV?

In order to talk about RandomizedSearchCV, I need to first talk about GridSearchCV.

<img width="1556" height="994" alt="image" src="https://github.com/user-attachments/assets/5162445c-465d-41a1-a074-44a791b0d57c" />

*Figure 1: Randomized vs Grid search, visually*

**Grid Search:** Tests every combination of hyperparameters that I pass through it, leading to exhaustive but slow searches

```python
# Example grid: 5 learning rates √ó 6 depths √ó 4 regularization = 120 combinations
# With 3-fold CV: 120 √ó 3 = 360 training runs
# Time: ~6 hours per model
```

**Randomized Search:** Samples hyperparameter combinations randomly, not guaranteeing to find the absolute optimal combination, but a good-nuf solution, while staying fast and effective
```python
# Same search space, 30 random samples √ó 3 folds = 90 training runs
# Time: ~1.5 hours per model
# Research shows random sampling finds near-optimal parameters in 1/4 the iterations
```

### The Search Setup

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, average_precision_score, mean_squared_error
import numpy as np

# Stage 1: Classification (optimize PR-AUC)
pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)

# Stage 2: Regression (optimize RMSE)
rmse_scorer = make_scorer(
    lambda y, y_pred: -np.sqrt(mean_squared_error(y, y_pred)),
    greater_is_better=True
)

# 3-fold CV, 30 iterations per model
search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=30,
    scoring=scorer,
    cv=3,
    n_jobs=-1
)
```

**Time Investment:** ~90 minutes per model (classifier + regressor tuning)

### 2.2 Model Selection

Now from Parts 4 and 5 I had a good batch of tree models and deep learning models to try. From Part 4, my winning two-stage pipeline was a Gradient Boosting classifier and a LightGBM regressor, though I saw most combinations were within 0.3 RMSE tickets of each other. For this reason, I chose to only evaluate XGBoost, LightGBM, and CatBoost in this hyperparameter tuning, since these were the faster options.

In Part 5's deep learning, I found that a Naive Bayes classifier and Skip Connection regressor resulted in the lowest RMSE tickets. However, since Naive Bayes doesn't have many hyperparameter levers to pull, I chose to only evaluate MLPs, Skip Connections, and ResNets in this endeavor.

### 2.3 Clarifying Metrics

It's important to note that some of the metrics I will be sharing will seem much worse than Part 4 and 5's findings. This is because I am performing classification and regression separately. Regression searches are done only on nonzero-sale data, which will inflate my RMSE metrics (as I saw before with standalone regressors). 

With my models selected, I was ready to begin turning all of these levers for all models in classification and regression to randomly search their hyperparameter spaces and find quick wins for my modeling.

---

## 3. Tree Model Tuning Results

### 3.1 XGBoost Optimization

**Classification:**
```python
param_grid = {
    'n_estimators': [300, 500, 800, 1000],
    'max_depth': [3, 4, 5, 6, 8],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.01, 0.1, 0.5],
    'reg_lambda': [0.5, 1.0, 2.0],
    'scale_pos_weight': [2.53]  # Calculated from class imbalance
}
```

**Best Classifier Configuration:**
- **PR-AUC:** 0.8158 (best among all models)
- **ROC-AUC:** 0.9209
- **F1-Score:** 0.7316
- **Key Parameters:**
  - `n_estimators`: 300 (fewer trees for classification)
  - `max_depth`: 6 (moderate depth)
  - `learning_rate`: 0.1 (aggressive)
  - `reg_lambda`: 0.5 (L2 regularization)
  - `subsample`: 0.8 (80% data per tree)

**Best Regressor Configuration:**
- **RMSE:** 31.24 tickets
- **MAE:** 10.56 tickets
- **R¬≤:** 0.637
- **Key Parameters:**
  - `n_estimators`: 1000 (more trees for regression)
  - `max_depth`: 10 (deeper splits)
  - `learning_rate`: 0.1
  - `reg_alpha`: 1.0 (L1 + L2 regularization)

**Quick Insight:** Regressors needed more capacity (deeper trees, more estimators) than classifiers to capture fine-grained variance in ticket sales.

---

### 3.2 LightGBM Optimization

**Best Classifier:**
- **PR-AUC:** 0.8125 (0.4% behind XGBoost)
- **Configuration:**
  - `n_estimators`: 300
  - `num_leaves`: 31 (leaf-wise growth)
  - `learning_rate`: 0.03 (conservative)
  - `reg_lambda`: 1.0 (high regularization)
  - Training time: 73 minutes

**Best Regressor:**
- **RMSE:** 31.41 tickets
- **MAE:** 10.35 tickets
- **Configuration:**
  - `n_estimators`: 1500 (most trees among all models)
  - `num_leaves`: 127 (large tree capacity)
  - `learning_rate`: 0.05
  - `subsample`: 0.9

**Pattern:** LightGBM needed more trees (1500 vs. XGBoost's 1000) but trained faster due to the leaf-wise growth optimization.

---

### 3.3 CatBoost Optimization

**Best Classifier:**
- **PR-AUC:** 0.8093 (0.8% behind XGBoost)
- **Configuration:**
  - `iterations`: 500
  - `depth`: 8
  - `learning_rate`: 0.01
  - `l2_leaf_reg`: 7
  - Training time: 160 minutes (slowest)

**Best Regressor (Champion):**
- **RMSE:** 31.06 tickets (BEST)
- **MAE:** 10.27 tickets (BEST)
- **R¬≤:** 0.645
- **Configuration:**
  - `iterations`: 800
  - `depth`: 10
  - `learning_rate`: 0.1
  - `l2_leaf_reg`: 5

**Surprising Discovery:** Default parameters were already near-optimal!

**Why CatBoost Won:**
- **Ordered boosting:** Reduces target leakage automatically
- **Robust defaults:** Designed to work well out-of-the-box
- **Less sensitive:** Parameter changes had minimal impact
- **Regularization:** Strong built-in overfitting protection

> **Lesson:** Not every model needs extensive tuning. CatBoost's intelligent defaults saved hours of experimentation.

---

## 4. Neural Network Tuning

For neural networks, "hyperparameter tuning" means **architecture search**. For a quick understanding, I trained each architecture with `EarlyStopping` at a `patience=15`, where training would quit after 15 epochs of no validation loss (RMSE) improvement.

Regardless, this meant running the notebook overnight...

### 4.1 MLP Classifier

Tested 10 architectures with varying:
- **Layers:** [128,64], [256,128,64], [512,256,128]
- **Dropout:** 0.2, 0.3
- **Learning rate:** 0.0005, 0.001
- **Batch size:** 1024, 2048

**Best Configuration:**
- **Architecture:** [256, 128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **PR-AUC:** 0.8064 (1.2% behind XGBoost)
- **Training time:** 33 minutes

---

### 4.2 MLP Regressor

Tested 8 architectures:

**Best Configuration:**
- **Architecture:** [128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **RMSE:** 33.87 tickets (9% worse than CatBoost)
- **MAE:** 9.81 tickets
- **R¬≤:** 0.672

**Surprising Finding:** Simpler architecture [128,64] outperformed deeper networks like [512,256,128].

**Why Trees Beat Neural Networks:**
1. **Feature interactions:** Trees naturally handle interactions (e.g., `days_to_event √ó get_in`) better than networks
2. **No scaling required:** Trees are invariant to different feature spaces/ranges
3. **Robust to outliers:** Split-based learning less sensitive to extreme values
4. **Easier to regularize:** Depth and tree count provide intuitive regularization
5. **Faster training:** 30min (XGBoost) vs. 33min (MLP), less hyperparameter sensitivity

---

## 5. When to Stop Searching

I tracked improvement across RandomizedSearchCV iterations to identify the point of diminishing returns:

### Cumulative Gains by Iteration

| Iterations | % of Total Improvement | Cumulative Time |
|-----------|------------------------|-----------------|
| 1-10      | 80%                   | 10 minutes     |
| 11-20     | 90%                   | 20 minutes     |
| 21-30     | 95%                   | 30 minutes     |
| 31-50     | 100%                  | 60+ minutes    |

**Practical Tuning Tip:**
- **10 minutes:** 10 iterations, capture 80% of gains (good for exploration)
- **30 minutes:** 30 iterations, capture 95% of gains (**SWEET SPOT**)
- **60+ minutes:** Chasing the final 5% (only for production-critical models)

> **Key Insight:** The first 10 random samples found configurations within 5% of optimal. Additional iterations refined but didn't dramatically improve my new optimal metrics.

---

## 6. What Hyperparameters Mattered Most

Analyzing top-10 configurations across all models revealed clear patterns:

### High-Impact Parameters

**1. Learning Rate**
- **Impact:** Controls step size in gradient descent
- **Tested range:** [0.01, 0.03, 0.05, 0.1]
- **Finding:**
  - Too high (0.1): Overshoots optimal solution, unstable training
  - Too low (0.01): Needs 2000+ trees to converge
  - **Sweet spot:** 0.03-0.05 for most problems
- **Example:** XGBoost classifier PR-AUC: 0.813 (lr=0.05) vs. 0.807 (lr=0.01)

**2. Regularization (reg_lambda, reg_alpha, l2_leaf_reg)**
- **Impact:** Prevents overfitting by penalizing complex models
- **Finding:** Higher regularization = better generalization
- **Best practice:** L2 (reg_lambda) more important than L1 (reg_alpha)
- **Example:** CatBoost regressor RMSE: 31.06 (l2=5) vs. 31.89 (l2=1)

**3. Tree Depth (max_depth, num_leaves)**
- **Impact:** Controls model capacity
- **Classifier sweet spot:** Depth 6, 31 leaves (shallower trees)
- **Regressor sweet spot:** Depth 10, 127 leaves (deeper trees)
- **Reason:** Regression needs to capture fine-grained variance patterns

---

### Medium-Impact Parameters

**4. Subsampling (subsample, colsample_bytree)**
- **Impact:** Uses random data/feature subsets per tree (bagging effect)
- **Best range:** 0.6-0.8 (60-80% sampling)
- **Benefit:** Reduces overfitting, speeds training
- **Example:** LightGBM regressor RMSE: 31.41 (subsample=0.9) vs. 32.12 (subsample=0.5)

**5. Number of Trees (n_estimators, iterations)**
- **Impact:** More trees = more learning capacity
- **Finding:** More always helps (with proper learning rate + early stopping)
- **Typical range:** 300-1500 trees
- **Rule:** Use early stopping in production to avoid overfitting

---

### Low-Impact Parameters

**6. Min Samples Per Leaf (min_child_weight, min_child_samples)**
- **Impact:** Prevents splits on noisy patterns
- **Finding:** Small effect unless data is very noisy
- **Default values:** Often sufficient (5-20 samples)

**7. Gamma (min_split_loss)**
- **Impact:** Minimum loss reduction to make a split
- **Finding:** Marginal gains from tuning
- **Best practice:** Start with 0, increase only if overfitting

---

## 7. Results: Baseline vs. Tuned

### Tree-Based Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **CatBoost** | 32.21 | **31.06** | -1.15 tickets | **-3.6%** |
| **XGBoost** | 31.97 | **31.24** | -0.73 tickets | **-2.3%** |
| **LightGBM** | 31.90 | **31.41** | -0.49 tickets | **-1.5%** |

**Key Insight:** CatBoost showed the largest improvement despite having strong defaults. All models improved, with total gains of 0.5-1.2 RMSE tickets.

---

### Tree-Based Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **XGBoost** | 0.8121 | **0.8158** | +0.0037 | **+0.46%** |
| **LightGBM** | 0.8117 | **0.8125** | +0.0008 | **+0.10%** |
| **CatBoost** | 0.8087 | **0.8093** | +0.0006 | **+0.07%** |

**Key Insight:** Classifiers were already near-optimal with default parameters. XGBoost gained most from tuning `scale_pos_weight` (class imbalance handling) and `gamma` (split regularization).

---

### Neural Network Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **MLP** | 38.50 | **33.87** | -4.63 tickets | **-12.0%** |
| **Skip Connection** | 32.96 | **35.12** | +2.16 tickets | **+6.6%** |

**Key Insight:** MLP improved dramatically with architecture tuning (3 layers, dropout 0.3, batch size 512). Skip Connection regressed likely overfitting to validation set. Neural networks still lag 9% behind best tree model (CatBoost 31.06 vs MLP 33.87).

---

### Neural Network Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **MLP** | 0.81 | **0.8142** | +0.0042 | **+0.52%** |
| **Skip Connection** | 0.80 | **0.8089** | +0.0089 | **+1.11%** |
| **LSTM** | 0.79 | **0.7956** | +0.0056 | **+0.71%** |

**Key Insight:** Neural network classifiers improved modestly, just like trees. Skip Connection gained most from regularization tuning. All still barely trail XGBoost (0.8158) for this tabular problem.

> **Verdict:** Hyperparameter tuning delivered measurable gains for regressors (1-4 tickets RMSE) but modest gains for classifiers (<0.4% PR-AUC). CatBoost emerged as the regression champion, while XGBoost dominated classification. **The 10-hour investment yielded production-ready standalone models.**



But wait‚Äîthese RMSE numbers are from the **regression-only subset** (events with sales). To compare against the Part 4 baseline (19.15 RMSE), I need to evaluate the **full two-stage pipeline** on the complete test set.

---

## 7.1 Combining Into a Full Pipeline

### The Evaluation Gap

Up to this point, I've evaluated classifiers and regressors **separately**:
- **Classifiers:** Evaluated on all 58,022 test events ‚Üí PR-AUC metric
- **Regressors:** Evaluated on 16,939 test events **with sales only** ‚Üí RMSE = 31.06 tickets

**But Part 4's baseline (19.15 RMSE) was evaluated on the FULL test set (58K events).** To make an apples-to-apples comparison, I need to combine my tuned classifier + regressor into a complete two-stage pipeline.

---

### The Two-Stage Architecture

Here's how the pipeline works:

```python
# Stage 1: Classifier predicts if event will have sales
clf_probs = xgb_classifier.predict_proba(X_test)[:, 1]  # Probability of sales
clf_binary = (clf_probs >= 0.5).astype(int)             # Threshold at 0.5

# Stage 2: Regressor predicts sales volume (log scale)
reg_preds_log = xgb_regressor.predict(X_test)

# Combine: Zero out predictions for events classifier says won't sell
final_preds_log = reg_preds_log * clf_binary

# Convert back to ticket scale
final_preds_tickets = np.expm1(final_preds_log)  # Inverse of log1p
final_preds_tickets = np.maximum(final_preds_tickets, 0)  # No negative predictions

# Evaluate on FULL test set (58,022 events including zeros)
rmse = np.sqrt(mean_squared_error(y_test_full, final_preds_tickets))
```

**Key insight:** The classifier acts as a **gate**‚Äîif it predicts zero sales (clf_binary=0), the regressor's prediction gets zeroed out. This prevents the regressor from predicting sales for events that won't sell.

---

### Testing Different Combinations

I tested three tuned model combinations on the full test set:

| Classifier | Regressor | Threshold | Full Test RMSE | vs Part 4 (19.15) |
|------------|-----------|-----------|----------------|-------------------|
| **XGBoost** | **XGBoost** | 0.5 | **18.98** | ‚úÖ **-0.9%** |
| XGBoost | CatBoost | 0.5 | 19.11 | ‚úÖ -0.2% |
| LightGBM | XGBoost | 0.5 | 19.14 | ‚úÖ -0.05% |
| XGBoost | LightGBM | 0.5 | 19.23 | ‚ùå +0.4% |
| **Part 4 Baseline** | **(GradBoosting + LightGBM)** | **0.5** | **19.15** | Baseline |

---

### The Champion: XGBoost + XGBoost

**Result: 18.98 RMSE** üèÜ

**Why this combination won:**
1. **Strong classifier:** XGBoost's 0.8158 PR-AUC correctly identifies sales/no-sales events
2. **Balanced regressor:** XGBoost reg (31.24 RMSE) struck a good balance‚Äînot the best on regression subset, but complementary to the classifier
3. **Model consistency:** Using XGBoost for both stages created consistent feature interpretations

**Improvement breakdown:**
- Part 4 baseline: 19.15 RMSE (GradientBoosting clf + LightGBM reg, default params)
- Part 6 tuned: **18.98 RMSE** (XGBoost clf + XGBoost reg, tuned params)
- **Gain: -0.17 tickets (-0.9%)**

---

### Why Not CatBoost Regressor?

CatBoost won the regression-only evaluation (31.06 RMSE), but XGBoost + CatBoost on the full test set yielded 19.11 RMSE‚Äîslightly worse than XGBoost + XGBoost (18.98).

**Hypothesis:** CatBoost optimized for the regression subset (events with sales) but was less robust to classifier gating errors. When the classifier misclassifies an event, CatBoost's predictions were less forgiving.

---

### From 31.06 to 18.98: Understanding the Drop

**How did RMSE "improve" from 31.06 ‚Üí 18.98?**

It didn't‚Äîwe're measuring different things:

| Evaluation Set | Events | RMSE | What it Measures |
|----------------|--------|------|------------------|
| **Regression-only subset** | 16,939 (29%) | 31.06 | Regressor accuracy on sales events |
| **Full test set (two-stage)** | 58,022 (100%) | 18.98 | Pipeline accuracy on all events |

**Why full-test-set RMSE is lower:**
- 70% of test events have zero sales ‚Üí classifier predicts zero ‚Üí RMSE contribution = 0
- 30% of test events have sales ‚Üí regressor predicts ‚Üí RMSE contribution from hard cases
- **Net effect:** Lower RMSE because many predictions are correct zeros

**This is NOT cherry-picking**‚ÄîPart 4's 19.15 baseline used the same full-test-set evaluation. We're now comparing apples-to-apples.

---

### Establishing the New Baseline

**Part 6 delivers: 18.98 RMSE**

This becomes the baseline for Part 7. The natural question: **Can advanced optimization techniques push beyond 18.98?**

Options to explore:
- **Threshold optimization:** Test thresholds beyond fixed 0.5 (sweep 0.2-0.8)
- **Loss function variants:** Huber loss for robustness to outliers
- **Bayesian optimization:** Use Optuna to search hyperparameter spaces intelligently
- **Ensemble weights:** Optimize weighted averaging of multiple models

**Spoiler:** Part 7 explores all of these with automated Optuna search.

---

## 8. Lessons Learned

### What Worked

**RandomizedSearchCV found improvements efficiently**
- 30 iterations balanced exploration and time (95% of potential gains)
- Random sampling explored diverse parameter combinations
- Faster than grid search by 4√ó

**Regularization mattered more than complexity**
- Strong L2 regularization (reg_lambda=1-5) prevented overfitting
- Often more impactful than adding trees or depth
- Consistent across XGBoost, LightGBM, and CatBoost

---

### What Surprised Me

**Neural networks lagged behind trees**
- Even after architecture search, 9% worse than best tree model
- Gradient boosting remains king for tabular data
- Trees require less hyperparameter sensitivity

**Diminishing returns hit fast**
- First 10 iterations: 80% of gains
- Last 20 iterations: 5% of gains
- Time investment doesn't scale linearly with improvement

---

## Conclusion

Hyperparameter tuning delivered measurable improvements across two levels:

### Individual Model Performance:
- **Best regressor:** CatBoost (31.06 RMSE on regression subset)
- **Best classifier:** XGBoost (0.8158 PR-AUC)
- **Time invested:** ~90 minutes per model
- **Improvement:** 1-12% over default configurations

### Full Pipeline Performance:
- **Champion:** XGBoost clf + XGBoost reg = **18.98 RMSE** (full test set)
- **Improvement over Part 4 baseline:** 19.15 ‚Üí 18.98 = **-0.9%**
- **New baseline established:** 18.98 RMSE to beat in Part 7

**Key Insights:**
1. **Manual tuning works:** RandomizedSearchCV found meaningful improvements in ~90 minutes per model
2. **30 iterations hit the sweet spot:** 95% of gains, diminishing returns after
3. **Different winners for different stages:** XGBoost best for classification, CatBoost best for regression subset‚Äîbut XGBoost+XGBoost won the full pipeline
4. **Trees dominated neural networks:** Even after extensive architecture search, gradient boosting won for tabular data
5. **Evaluation matters:** Regression-subset RMSE (31.06) vs. full-pipeline RMSE (18.98) measure different things

**The Path Forward:**
We've beaten the Part 4 baseline with manual hyperparameter tuning. But **can we push beyond 18.98 RMSE?**

Part 7 explores advanced optimization techniques:
- **Bayesian optimization with Optuna:** Smarter search than random sampling
- **Threshold optimization:** Beyond fixed 0.5 classifier gate
- **Loss function variants:** Huber loss for robustness
- **Comprehensive model search:** 14 models, 370+ trials, automated
- **Interactive dashboard:** Visualize the entire optimization landscape

The journey from manual tuning (18.98) to automated Optuna optimization begins in Part 7!
