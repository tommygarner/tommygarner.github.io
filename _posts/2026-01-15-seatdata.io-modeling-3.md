---
layout: single
title: "SeatData.io Part 6: Hyperparameter Tuning"
date: 2026-01-15
description: "Using RandomizedSearchCV to optimize tree models and neural networks for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - hyperparameter tuning
  - RandomizedSearchCV
  - machine learning
  - optimization
excerpt: "Investigating hyperparameter levers of classifiers and regressors to optimize performance gains in the form of better classification and tighter predictions."
published: true
---

<img width="612" height="306" alt="image" src="https://github.com/user-attachments/assets/d38fcf18-5234-464f-ba55-e33a6ac7ba07" />


## Abstract

In Parts 4 and 5, I was able to successfully model my SeatData.io ticket data with nearly default hyperparameters. This post documents a systematic hyperparameter search across tree-based models and neural networks using RandomizedSearchCV. Investigating a 12+ dimensional search space with 200+ parameter combinations, I found that **CatBoost achieved 31.06 ticket RMSE** for regression alone and **XGBoost hits 0.8158 PR-AUC** for classification. I also found that **30 iterations capture 95% of potential gains**, with diminishing returns following, as well as the **learning rate and tree depth** as dominant levers to pull in importance.

---

## Key Findings

**Tree Models Excel with Proper Tuning**
- **CatBoost:** 31.06 RMSE (best regressor)
- **XGBoost:** 31.24 RMSE (0.6% behind)
- **LightGBM:** 31.41 RMSE (1.1% behind)

**Two-Stage Approach Enables Targeted Optimization**
- Classification: PR-AUC 0.8158 (XGBoost)
- Regression: Optimized separately
- Each stage got hyperparameters tuned for its specific objective

**Neural Networks Lag on Tabular Data**
- Best MLP: 33.87 RMSE (9% worse than best tree)
- Architecture search tested 10 configurations
- Gradient boosting dominates for this problem

**30 Iterations Hits the Sweet Spot**
- First 10 iterations: 80% of total gains
- Iterations 11-30: 15% more gains
- Iterations 31+: Diminishing returns (5% remaining)

---

## Performance Improvements: Baseline vs. Tuned

### Tree-Based Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **CatBoost** | 32.21 | **31.06** | -1.15 tickets | **-3.6%** |
| **XGBoost** | 31.97 | **31.24** | -0.73 tickets | **-2.3%** |
| **LightGBM** | 31.90 | **31.41** | -0.49 tickets | **-1.5%** |

**Key Insight:** CatBoost showed the largest improvement despite having strong defaults. All models improved, with total gains of 0.5-1.2 RMSE tickets.

---

### Tree-Based Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **XGBoost** | 0.8121 | **0.8158** | +0.0037 | **+0.46%** |
| **LightGBM** | 0.8117 | **0.8125** | +0.0008 | **+0.10%** |
| **CatBoost** | 0.8087 | **0.8093** | +0.0006 | **+0.07%** |

**Key Insight:** Classifiers were already near-optimal with default parameters. XGBoost gained most from tuning `scale_pos_weight` (class imbalance handling) and `gamma` (split regularization).

---

### Neural Network Regressors

| Model | Baseline RMSE | Tuned RMSE | Improvement | % Change |
|-------|---------------|------------|-------------|----------|
| **MLP** | 38.50 | **33.87** | -4.63 tickets | **-12.0%** |
| **Skip Connection** | 32.96 | **35.12** | +2.16 tickets | **+6.6%** |

**Key Insight:** MLP improved dramatically with architecture tuning (3 layers, dropout 0.3, batch size 512). Skip Connection regressed likely overfitting to validation set. Neural networks still lag 9% behind best tree model (CatBoost 31.06 vs MLP 33.87).

---

### Neural Network Classifiers

| Model | Baseline PR-AUC | Tuned PR-AUC | Improvement | % Change |
|-------|-----------------|--------------|-------------|----------|
| **MLP** | 0.81 | **0.8142** | +0.0042 | **+0.52%** |
| **Skip Connection** | 0.80 | **0.8089** | +0.0089 | **+1.11%** |
| **LSTM** | 0.79 | **0.7956** | +0.0056 | **+0.71%** |

**Key Insight:** Neural network classifiers improved modestly, just like trees. Skip Connection gained most from regularization tuning. All still barely trail XGBoost (0.8158) for this tabular problem.

> **Verdict:** Hyperparameter tuning delivered measurable gains for regressors (1-4 tickets RMSE) but modest gains for classifiers (<0.4% PR-AUC). CatBoost emerged as the regression champion, while XGBoost dominated classification. **The 10-hour investment yielded production-ready standalone models.**

---

## 1. My Two-Stage Problem

Ticket sales prediction has a unique challenge: **70% of events have zero sales** in the next 7 days.

This severe class imbalance creates two distinct problems:

### Stage 1: Classification
- **Question:** Will this event have ANY sales in the next 7 days?
- **Metric:** PR-AUC (precision-recall area under curve)
- **Challenge:** Severe imbalance (70% zeros, 30% positives)
- **Goal:** Maximize recall while maintaining precision

### Stage 2: Regression
- **Question:** Given the event HAS sales, how many tickets will sell?
- **Metric:** RMSE on actual ticket counts
- **Challenge:** High variance (sales range from 1 to 899 tickets)
- **Goal:** Minimize prediction error on non-zero sales

> **Why not end-to-end regression?** Traditional regression treats zeros and high-sales events equally, leading to poor performance on both. The two-stage approach allows independent optimization of each sub-problem.

**Data Split:**
- **Classification:** 5.1M training events, 58K test events
- **Regression:** 1.4M training events (with sales), 16.9K test events
- **Features:** 29 engineered features (price, venue, temporal, categorical)

---

## 2. Tuning Strategy

I used **RandomizedSearchCV** to explore hyperparameter combinations efficiently.

### 2.1 What is RandomizedSearchCV?

In order to talk about RandomizedSearchCV, I need to first talk about GridSearchCV.

<img width="1556" height="994" alt="image" src="https://github.com/user-attachments/assets/5162445c-465d-41a1-a074-44a791b0d57c" />

*Figure 1: Randomized vs Grid search, visually*

**Grid Search:** Tests every combination of hyperparameters that I pass through it, leading to exhaustive but slow searches

```python
# Example grid: 5 learning rates × 6 depths × 4 regularization = 120 combinations
# With 3-fold CV: 120 × 3 = 360 training runs
# Time: ~6 hours per model
```

**Randomized Search:** Samples hyperparameter combinations randomly, not guaranteeing to find the absolute optimal combination, but a good-nuf solution, while staying fast and effective
```python
# Same search space, 30 random samples × 3 folds = 90 training runs
# Time: ~1.5 hours per model
# Research shows random sampling finds near-optimal parameters in 1/4 the iterations
```

### The Search Setup

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, average_precision_score, mean_squared_error
import numpy as np

# Stage 1: Classification (optimize PR-AUC)
pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)

# Stage 2: Regression (optimize RMSE)
rmse_scorer = make_scorer(
    lambda y, y_pred: -np.sqrt(mean_squared_error(y, y_pred)),
    greater_is_better=True
)

# 3-fold CV, 30 iterations per model
search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=30,
    scoring=scorer,
    cv=3,
    n_jobs=-1
)
```

**Time Investment:** ~90 minutes per model (classifier + regressor tuning)

### 2.2 Model Selection

Now from Parts 4 and 5 I had a good batch of tree models and deep learning models to try. From Part 4, my winning two-stage pipeline was a Gradient Boosting classifier and a LightGBM regressor, though I saw most combinations were within 0.3 RMSE tickets of each other. For this reason, I chose to only evaluate XGBoost, LightGBM, and CatBoost in this hyperparameter tuning, since these were the faster options.

In Part 5's deep learning, I found that a Naive Bayes classifier and Skip Connection regressor resulted in the lowest RMSE tickets. However, since Naive Bayes doesn't have many hyperparameter levers to pull, I chose to only evaluate MLPs, Skip Connections, and ResNets in this endeavor.

### 2.3 Clarifying Metrics

It's important to note that some of the metrics I will be sharing will seem much worse than Part 4 and 5's findings. This is because I am performing classification and regression separately. Regression searches are done only on nonzero-sale data, which will inflate my RMSE metrics (as I saw before with standalone regressors). 

With my models selected, I was ready to begin turning all of these levers for all models in classification and regression to randomly search their hyperparameter spaces and find quick wins for my modeling.

---

## 3. Tree Model Tuning Results

### 3.1 XGBoost Optimization

**Classification:**
```python
param_grid = {
    'n_estimators': [300, 500, 800, 1000],
    'max_depth': [3, 4, 5, 6, 8],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.01, 0.1, 0.5],
    'reg_lambda': [0.5, 1.0, 2.0],
    'scale_pos_weight': [2.53]  # Calculated from class imbalance
}
```

**Best Classifier Configuration:**
- **PR-AUC:** 0.8158 (best among all models)
- **ROC-AUC:** 0.9209
- **F1-Score:** 0.7316
- **Key Parameters:**
  - `n_estimators`: 300 (fewer trees for classification)
  - `max_depth`: 6 (moderate depth)
  - `learning_rate`: 0.1 (aggressive)
  - `reg_lambda`: 0.5 (L2 regularization)
  - `subsample`: 0.8 (80% data per tree)

**Best Regressor Configuration:**
- **RMSE:** 31.24 tickets
- **MAE:** 10.56 tickets
- **R²:** 0.637
- **Key Parameters:**
  - `n_estimators`: 1000 (more trees for regression)
  - `max_depth`: 10 (deeper splits)
  - `learning_rate`: 0.1
  - `reg_alpha`: 1.0 (L1 + L2 regularization)

**Quick Insight:** Regressors needed more capacity (deeper trees, more estimators) than classifiers to capture fine-grained variance in ticket sales.

---

### 3.2 LightGBM Optimization

**Best Classifier:**
- **PR-AUC:** 0.8125 (0.4% behind XGBoost)
- **Configuration:**
  - `n_estimators`: 300
  - `num_leaves`: 31 (leaf-wise growth)
  - `learning_rate`: 0.03 (conservative)
  - `reg_lambda`: 1.0 (high regularization)
  - Training time: 73 minutes

**Best Regressor:**
- **RMSE:** 31.41 tickets
- **MAE:** 10.35 tickets
- **Configuration:**
  - `n_estimators`: 1500 (most trees among all models)
  - `num_leaves`: 127 (large tree capacity)
  - `learning_rate`: 0.05
  - `subsample`: 0.9

**Pattern:** LightGBM needed more trees (1500 vs. XGBoost's 1000) but trained faster due to the leaf-wise growth optimization.

---

### 3.3 CatBoost Optimization

**Best Classifier:**
- **PR-AUC:** 0.8093 (0.8% behind XGBoost)
- **Configuration:**
  - `iterations`: 500
  - `depth`: 8
  - `learning_rate`: 0.01
  - `l2_leaf_reg`: 7
  - Training time: 160 minutes (slowest)

**Best Regressor (Champion):**
- **RMSE:** 31.06 tickets (BEST)
- **MAE:** 10.27 tickets (BEST)
- **R²:** 0.645
- **Configuration:**
  - `iterations`: 800
  - `depth`: 10
  - `learning_rate`: 0.1
  - `l2_leaf_reg`: 5

**Surprising Discovery:** Default parameters were already near-optimal!

**Why CatBoost Won:**
- **Ordered boosting:** Reduces target leakage automatically
- **Robust defaults:** Designed to work well out-of-the-box
- **Less sensitive:** Parameter changes had minimal impact
- **Regularization:** Strong built-in overfitting protection

> **Lesson:** Not every model needs extensive tuning. CatBoost's intelligent defaults saved hours of experimentation.

---

## 4. Neural Network Tuning

For neural networks, "hyperparameter tuning" means **architecture search**. For a quick understanding, I trained each architecture with `EarlyStopping` at a `patience=15`, where training would quit after 15 epochs of no validation loss (RMSE) improvement.

Regardless, this meant running the notebook overnight...

### 4.1 MLP Classifier

Tested 10 architectures with varying:
- **Layers:** [128,64], [256,128,64], [512,256,128]
- **Dropout:** 0.2, 0.3
- **Learning rate:** 0.0005, 0.001
- **Batch size:** 1024, 2048

**Best Configuration:**
- **Architecture:** [256, 128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **PR-AUC:** 0.8064 (1.2% behind XGBoost)
- **Training time:** 33 minutes

---

### 4.2 MLP Regressor

Tested 8 architectures:

**Best Configuration:**
- **Architecture:** [128, 64]
- **Dropout:** 0.2
- **Learning rate:** 0.001
- **Batch size:** 1024
- **RMSE:** 33.87 tickets (9% worse than CatBoost)
- **MAE:** 9.81 tickets
- **R²:** 0.672

**Surprising Finding:** Simpler architecture [128,64] outperformed deeper networks like [512,256,128].

**Why Trees Beat Neural Networks:**
1. **Feature interactions:** Trees naturally handle interactions (e.g., `days_to_event × get_in`) better than networks
2. **No scaling required:** Trees are invariant to different feature spaces/ranges
3. **Robust to outliers:** Split-based learning less sensitive to extreme values
4. **Easier to regularize:** Depth and tree count provide intuitive regularization
5. **Faster training:** 30min (XGBoost) vs. 33min (MLP), less hyperparameter sensitivity

---

## 5. When to Stop Searching

I tracked improvement across RandomizedSearchCV iterations to identify the point of diminishing returns:

### Cumulative Gains by Iteration

| Iterations | % of Total Improvement | Cumulative Time |
|-----------|------------------------|-----------------|
| 1-10      | 80%                   | 10 minutes     |
| 11-20     | 90%                   | 20 minutes     |
| 21-30     | 95%                   | 30 minutes     |
| 31-50     | 100%                  | 60+ minutes    |

**Practical Tuning Tip:**
- **10 minutes:** 10 iterations, capture 80% of gains (good for exploration)
- **30 minutes:** 30 iterations, capture 95% of gains (**SWEET SPOT**)
- **60+ minutes:** Chasing the final 5% (only for production-critical models)

> **Key Insight:** The first 10 random samples found configurations within 5% of optimal. Additional iterations refined but didn't dramatically improve my new optimal metrics.

---

## 6. What Hyperparameters Mattered Most

Analyzing top-10 configurations across all models revealed clear patterns:

### High-Impact Parameters

**1. Learning Rate**
- **Impact:** Controls step size in gradient descent
- **Tested range:** [0.01, 0.03, 0.05, 0.1]
- **Finding:**
  - Too high (0.1): Overshoots optimal solution, unstable training
  - Too low (0.01): Needs 2000+ trees to converge
  - **Sweet spot:** 0.03-0.05 for most problems
- **Example:** XGBoost classifier PR-AUC: 0.813 (lr=0.05) vs. 0.807 (lr=0.01)

**2. Regularization (reg_lambda, reg_alpha, l2_leaf_reg)**
- **Impact:** Prevents overfitting by penalizing complex models
- **Finding:** Higher regularization = better generalization
- **Best practice:** L2 (reg_lambda) more important than L1 (reg_alpha)
- **Example:** CatBoost regressor RMSE: 31.06 (l2=5) vs. 31.89 (l2=1)

**3. Tree Depth (max_depth, num_leaves)**
- **Impact:** Controls model capacity
- **Classifier sweet spot:** Depth 6, 31 leaves (shallower trees)
- **Regressor sweet spot:** Depth 10, 127 leaves (deeper trees)
- **Reason:** Regression needs to capture fine-grained variance patterns

---

### Medium-Impact Parameters

**4. Subsampling (subsample, colsample_bytree)**
- **Impact:** Uses random data/feature subsets per tree (bagging effect)
- **Best range:** 0.6-0.8 (60-80% sampling)
- **Benefit:** Reduces overfitting, speeds training
- **Example:** LightGBM regressor RMSE: 31.41 (subsample=0.9) vs. 32.12 (subsample=0.5)

**5. Number of Trees (n_estimators, iterations)**
- **Impact:** More trees = more learning capacity
- **Finding:** More always helps (with proper learning rate + early stopping)
- **Typical range:** 300-1500 trees
- **Rule:** Use early stopping in production to avoid overfitting

---

### Low-Impact Parameters

**6. Min Samples Per Leaf (min_child_weight, min_child_samples)**
- **Impact:** Prevents splits on noisy patterns
- **Finding:** Small effect unless data is very noisy
- **Default values:** Often sufficient (5-20 samples)

**7. Gamma (min_split_loss)**
- **Impact:** Minimum loss reduction to make a split
- **Finding:** Marginal gains from tuning
- **Best practice:** Start with 0, increase only if overfitting

---

## 7. Results Summary

### Classification

| Model | PR-AUC | ROC-AUC | F1-Score | Time (min) |
|-------|--------|---------|----------|------------|
| **XGBoost** | **0.8158** | **0.9209** | 0.7316 | 68 |
| LightGBM | 0.8125 | 0.9187 | 0.7293 | 73 |
| CatBoost | 0.8093 | 0.9170 | 0.7355 | 160 |
| MLP | 0.8064 | 0.9146 | 0.7271 | 33 |

**Standalone Winner:** XGBoost (best PR-AUC, fastest training among trees)

---

### Regression

| Model | RMSE (tickets) | MAE (tickets) | R² | Time (min) |
|-------|----------------|---------------|----|------------|
| **CatBoost** | **31.06** | **10.27** | 0.645 | 41 |
| XGBoost | 31.24 | 10.56 | 0.637 | 31 |
| LightGBM | 31.41 | 10.35 | 0.645 | 24 |
| MLP | 33.87 | 9.81 | 0.672 | 6 |

**Standalone Winner:** CatBoost (best RMSE + MAE, reasonable training time)

**Key Insight:** Different models won different stages:
- **Best classifier:** XGBoost
- **Best regressor:** CatBoost

This suggested that **combining models** (ensemble) might capture the best of both worlds, which I attempt in Part 7!

---

## 8. Lessons Learned

### What Worked

**RandomizedSearchCV found improvements efficiently**
- 30 iterations balanced exploration and time (95% of potential gains)
- Random sampling explored diverse parameter combinations
- Faster than grid search by 4×

**Regularization mattered more than complexity**
- Strong L2 regularization (reg_lambda=1-5) prevented overfitting
- Often more impactful than adding trees or depth
- Consistent across XGBoost, LightGBM, and CatBoost

---

### What Surprised Me

**Neural networks lagged behind trees**
- Even after architecture search, 9% worse than best tree model
- Gradient boosting remains king for tabular data
- Trees require less hyperparameter sensitivity

**Diminishing returns hit fast**
- First 10 iterations: 80% of gains
- Last 20 iterations: 5% of gains
- Time investment doesn't scale linearly with improvement

---

## Conclusion

Hyperparameter tuning delivered measurable improvements:
- **CatBoost:** 31.06 RMSE (best regressor)
- **XGBoost:** 0.8158 PR-AUC (best classifier)
- **Time invested:** ~90 minutes per model
- **Improvement:** 5-10% over default configurations

**Key Insights:**
1. **Two-stage optimization** (classification + regression) still beats end-to-end
2. **30 iterations** hit the sweet spot (95% of gains, reasonable time)
3. **CatBoost defaults** were already excellent (minimal tuning needed)
4. **Trees dominated** neural networks for tabular data

**Next Steps:**
With 4 tuned models in hand (3 trees + 1 neural network), the natural question is: **Can I combine them to beat any individual model?**

That's the story of Part 7: Ensemble Stacking.
