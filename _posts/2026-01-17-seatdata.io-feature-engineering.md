---
layout: single
title: "SeatData.io Part 3: Feature Engineering"
date: 2026-01-17
description: "Transforming raw ticket data into model-ready features through scaling, encoding, and domain-driven interaction terms"
author_profile: true
toc: true
toc_sticky: true
tags:
  - feature engineering
  - data transformation
  - machine learning
  - python
  - pandas
excerpt: "Taking raw snapshots to prediction-ready features with transformations, imputations, and feature interactions."
---

<img width="2560" height="2133" alt="image" src="https://github.com/user-attachments/assets/6d70d229-4242-46f3-8d6c-9e5fae2ce4ed" />

## Abstract

After validating data quality in Part 2's EDA, I faced a critical question: how do I transform 5.78 million raw snapshots into features that models can actually learn from? Feature engineering bridges the gap between raw data and predictive models. Through outlier handling, log transformations, cyclical encoding, and domain-driven interaction terms, I engineered **30 features** that capture the temporal/seasonal dynamics, pricing psychology, and segment-specific behaviors of the secondary ticket market. The final dataset: **5.1M training rows** and **58K test rows** from 114K unique events.

## Key Insights

- **Outlier Clipping Before Transformation** - Extreme prices ($929K get-in!) must be clipped before log transforms to prevent distribution distortion
- **Log Transformations for Skewed Distributions** - Sales and prices follow power-law distributions requiring mathematical normalization
- **Cyclical Encoding for Temporal Patterns** - Days of the week are circular, not linear; sine/cosine encoding preserves this structure
- **Interaction Terms for Segment-Specific Behavior** - Sports fans buy differently than concert-goers as events approach

---

## 1. Outlier Handling

My EDA in Part 2 revealed several data quality issues that needed addressing before any feature engineering could begin. This section covers the dropping, clipping, and filtering I applied.

### 1.1 Price Outliers

The raw price distributions were dominated by absurd values:
- `get_in` max: **$929,999** (vs 99th percentile of $900)
- `listings_median` max: **$58,228,300** (vs 99th percentile of $1,760)

I don't believe these were data entry errors, but instead likely real meme listings. Still, they represented <1% of the data while dominating 99% of the variance, and they had to go.

**Solution: Clip at 99th Percentile**

```python
model_df['get_in'] = model_df['get_in'].clip(lower=0, upper=900)
model_df['listings_median'] = model_df['listings_median'].clip(lower=0, upper=1760)
```

**Impact on dataset:**
- `get_in` > $900: 38,740 rows clipped (0.67% of data)
- `listings_median` > $1,760: 51,778 rows clipped (0.90% of data)

Combined, less than 1.6% of rows were affected; a small price to pay for well-behaved distributions.

<img width="1389" height="1025" alt="image" src="https://github.com/user-attachments/assets/e05ed90e-e911-49e6-ac70-950a49bcd471" />

*Figure 1: Before/after clipping visualization showing compressed tails*

### 1.2 Days-to-Event Outliers

I found placeholder listings as well as legitimate listings for shows 2+ years in advance. These extreme lead times added noise without predictive value.

**Solution: Filter in SQL**

```sql
WHERE days_to_event < 1000
  AND event_date < '2028-01-01'
```

This focused my analysis on events happening within the next year, the window where pricing decisions actually matter.

### 1.3 Why Clip Before Transform?

The order matters. If I log-transformed first, then clipped:
- log($929,999) = 13.74
- log($900) = 6.80

The "clipped" value would still be 2x the ceiling in log space. By clipping in raw space first, log transformation uses that 99th percentile as the ceiling and compresses values into a manageable range to begin predictions.

---

## 2. Log Transformations

With outliers handled, I applied log transformations to all skewed numeric features. This is the single most impactful transformation in the pipeline.

### 2.1 Why Log Transform?

My EDA showed severe right-skew in nearly every numeric feature:

| Feature | Raw Skew | Log Skew |
|---------|----------|----------|
| get_in | 9.68 | 0.72 |
| listings_active | 4.89 | 0.12 |
| listings_median | 176.70 | 0.89 |
| sales_total_7d | 12.28 | 1.25 |
| venue_capacity | 3.87 | 0.32 |

Skew greater in magnitude than 1 indicates a distribution that will cause problems for most models. Log transformation compresses these distributions toward normality.

### 2.2 Applying Log Transforms

I used the `np.log1p(x)` = log(1 + x) function to handle the zeros in my features, since this will prevent later calculations from becoming undefined.

```python
log_features = ['get_in', 'listings_median', 'listings_active',
                'sales_total_7d', 'venue_capacity']

for col in log_features:
    model_df[f'{col}_log'] = np.log1p(model_df[col].clip(lower=0).fillna(0))
```

### 2.3 Visual Validation

After transformation, the distributions became **approximately normal** and much better for gradient-based optimization in neural networks.

<img width="1490" height="1055" alt="image" src="https://github.com/user-attachments/assets/9b506158-f066-46cb-a24f-8123fa640dfd" />

*Figure 2: Raw vs log-transformed distributions showing dramatically reduced skew*

---

## 3. Missing Value Imputation

Working with this real data, I quickly learned that **missing data is inevitable**. My approach: preserve as much signal as possible through strategic imputation.

### 3.1 Venue Capacity

This was the biggest challenge. From Part 2's EDA, **41.82% of venue_capacity values were missing** (2.42M rows). Dropping these rows would eliminate nearly half my training data.

**Multi-Stage Imputation Pipeline:**

**Stage 1: TouringData.org Lookup**

My first step was to include the data I already had from another ticketing source. By **concatenating two full CSV files** from TouringData.org, I could find any missing venue capacities from my database and **join** the capacity data from the other.

All I had to do was **standardize the naming conventions** of each source and find exact matches:

```python
venue_capacity_map = master_docs.groupby(["join_name", "join_city"])["Capacity"].max().to_dict()
```
- Reduced missing to 39.9%

**Stage 2: Fuzzy Matching by City**

However, this method did not fill as many missing values as I had hoped. So, I wanted to catch the fringe cases of matches between these venue names and cities between my two sources. 

A Python package called **`thefuzz`** can find these "fuzzy matches" between my two sources using a `.token_sort_ratio` scoring method. This function can do **all of the NLP processing** to my text strings, reorder all of the resulting tokens, and find strings with similar words that **aren't in perfect order**. 

Yet, this could get dangerous real fast if I didn't put guardrails up. So, I decided to only fuzzy match the venues **within their cities**. Therefore, the logic would first filter out all potential name pairs that are not in the same city as the venue I'm trying to match, reducing the risk of mapping incorrect venues together.

Finally, I used a `score_cutoff` of **85% similarity** to give my pipeline the go-ahead to bridge over the capacity number. 

```python
from thefuzz import process, fuzz
match = process.extractOne(venue, candidates, scorer=fuzz.token_sort_ratio, score_cutoff=85)
```
- Used `thefuzz` library with 85% similarity threshold
- Matched venues with slight name variations (e.g., "Madison Square Garden" vs "MSG")
- Ensured same-city lookup
- Reduced missing to 39.41%

**Stage 3: Keyword-Based Imputation**

I was still missing a lot of venue capacities and needed to be strategic now that I had lost my runway on my other source document. So, I made some **assumptions** on average venue sizes based off keywords in the name, and roughly assumed venue capacities missing in my data:

```python
venue_defaults = {
    "stadium": 60000,
    "arena": 15000,
    "amphitheatre": 10000,
    "theatre": 2500,
    "theater": 2500,
    "club": 500,
}
```
- Reduced missing to 24.4%

**Stage 4: Focus Bucket Median Fallback**

Lastly, as a final push to fill this feature space, I imputed the **conditional median values** of these events based on the category. This aggressive assumption suggests that most venues for comedy shows are similarly sized, for example.

```python
bucket_medians = model_df.groupby("focus_bucket")["venue_capacity"].median()
model_df["venue_capacity"] = model_df["venue_capacity"].fillna(
    model_df["focus_bucket"].map(bucket_medians)
)
```
- Final fallback: use median capacity for each event category
- Reduced missing to **0%**

Why this works: Major Sports venues average ~30,000 capacity, Broadway theaters ~1,500, Concerts ~8,000. Using bucket-specific medians **preserves these segment differences**.

Now, I admit, looking back on this I could've brought in more sources like **APIs** or even **LLM calls** to input actual venue capacity statistics. However, modeling mostly **favored the sales velocity** when predicting sales volume, so it ended up being negligent anyways.

### 3.2 Listings Median

Next, I wanted to impute the missing median listing price of event snapshots. So, I assumed that **missing values would fallback to the event's median price** in the secondary market.

### 3.3 Lag Features

For lag features where previous values don't exist (like the **very first snapshot of an event**) I chose to fill these cases with the **first observed value**. This assumption is just the naive method, assuming today's value will be tomorrow's, especially considering my data collection was for only a limited time.

### 3.4 Other Fills

Otherwise, I imputed the missing values for `listings_active` and its associated calculated metrics. These missing values were also likely from the first day of observations, so I interpreted these as **true zeros**:

```python
model_df['listings_active'] = model_df['listings_active'].fillna(0)
model_df['price_spread_ratio'] = model_df['price_spread_ratio'].fillna(0).clip(upper=1.0)
model_df['inv_per_day'] = model_df['inv_per_day'].fillna(0)
```

Final result: **0 missing values** in the feature set (down from 105K in `listings_active_prev`).

---

## 4. Cyclical Day-of-Week Encoding

My EDA showed that **Saturdays have 5,000 more daily sales than Sundays** and that ticket sales generally **increase** towards the end of the week. Day-of-week clearly matters, but encoding it properly is tricky.

### 4.1 The Problem with Integer Encoding

If I mapped Monday=1, Tuesday=2...Sunday=7, the model thinks Sunday (7) is "far" from Monday (1). But in reality, Sunday and Monday are just a day apart. 

The week should be interpreted as a **circle**, not a line.

### 4.2 Sine/Cosine Encoding

<img width="793" height="684" alt="image" src="https://github.com/user-attachments/assets/3a6c5e5d-c115-4572-80f1-2367db59b69b" />

*Figure 3: Cyclical encoding visualization, days mapped to unit circle*

Therefore, what I can do is take these day-of-week mappings and project them onto the **unit circle** in terms of `cos(X)` and `sin(Y)` coordinates as shown above.

Now Sunday (day 0) and Saturday (day 6) have similar sine/cosine values, preserving their adjacency in my data.

### 4.3 Weekend Flag

I also created a simple binary indicator to indicate **if the event occurs on a weekend**:

```python
model_df['is_weekend'] = model_df['event_dow'].isin([1, 7]).astype(int)
```

---

## 5. Anomaly Day Flags

My EDA discovered **5 days with Z-scores above 2.0** in absolute value, which were significant deviations from normal sales patterns. I wanted to **make sure models saw this flag** upon training.

### 5.1 Identified Anomalies

```python
ANOMALY_DATES = [
    '2025-10-22',
    '2025-11-04',
    '2026-01-03',
    '2026-01-05',
    '2026-01-08'
]
```

### 5.2 Creating the Flag

Taking this list of anomaly dates, I then created a simple binary `is_anomaly_day` variable to **flag** these snapshot records across my data.

```python
model_df['is_anomaly_day'] = model_df['snapshot_date'].isin(anomaly_dates).astype(int)
```

**Result:** 282,262 rows flagged (4.88% of data)

This gives models an explicit signal: "this day is weird, adjust your predictions and take this data with a grain of salt."

---

## 6. Price Tier Feature

My EDA revealed **3 distinct pricing tiers** in the resale market. I made sure to capture this as a **categorical feature**.

### 6.1 EDA-Derived Tiers

```python
PRICE_TIERS = {
    'Premium': ['Festivals', 'Broadway & Theater'],
    'Mid': ['Concerts', 'Comedy', 'Other Events'],
    'Low': ['Major Sports', 'Minor/Other Sports']
}
```

In a similar way as before, I mapped this categorical `price_tier` feature amongst my event categories.

### 6.2 Distribution

| Price Tier | Focus Buckets | Row Count |
|------------|---------------|-----------|
| Premium | Festivals, Broadway & Theater | 1,596,732 |
| Mid | Concerts, Comedy, Other | 3,414,061 |
| Low | Major Sports, Minor/Other Sports | 770,280 |

This categorical feature lets models learn "Premium tier events behave differently" without complex price thresholds.

---

## 7. Interaction Terms

This is where feature engineering gets interesting. My EDA revealed that **different segments show different temporal patterns**. A unified model might struggle to learn these differences in each market.

### 7.1 The Problem

Sports fans buy on predictable schedules tied to game announcements. Concert-goers buy based on price drops and artist popularity. A single `days_to_event` coefficient can't capture both behaviors.

### 7.2 Days-to-Event × Focus Bucket

So, I created interaction terms by **multiplying `days_to_event` with each bucket dummy**:

```python
# Create one-hot encoding for focus_bucket
bucket_dummies = pd.get_dummies(model_df['focus_bucket'], prefix='bucket')

# Create interaction terms
for col in bucket_dummies.columns:
    interaction_name = f'dte_X_{col}'
    model_df[interaction_name] = model_df['days_to_event'] * bucket_dummies[col]
```

**What this does:**
- For a Major Sports event: `dte_X_bucket_Major_Sports` = `days_to_event` value * 1 = `days_to_event` value
- For a Concert event: `dte_X_bucket_Major_Sports` = `days_to_event` value * 0 = 0

This allows models to learn **segment-specific time sensitivity**.

Here's visual example of sales patterns in each bucket as I just mentioned.

<img width="1489" height="1025" alt="image" src="https://github.com/user-attachments/assets/d8580833-6b85-4e30-ad67-de5ecbd94e1a" />

*Figure 4: Sales in different `days_to_event` bins per category*

### 7.3 Final Week Flags per Category

Also, **flagging the final week** of the event as a binary variable will help models in training understand the increase in sales that I previously saw in my EDA.

```python
model_df['is_final_week'] = (model_df['days_to_event'] <= 7).astype(int)
```
---

## 8. Two-Stage Target Variables

With features engineered, I finally defined what I was predicting. The **72.4% zero-sales rate** made me consider adding a two-stage approach to my predictions.

### 8.1 The Zero-Inflated Problem

Looking at my `sales_next_7d` distribution:
- **4,187,699 rows (72.4%)** had zero sales in the following week
- Only **1,594,374 rows (27.6%)** had any sales at all

A single regression model struggles with this distribution because it's trying to **simultaneously predict** "no sales" and "how many sales."

<img width="608" height="478" alt="image" src="https://github.com/user-attachments/assets/aa0b13e1-27f6-45cc-9db7-fb722bb8bb60" />

*Figure 5: Class imbalance showing sparsity of sales*

### 8.2 The Two-Stage Solution

**Stage 1 (Binary Classification):** Will there be ANY sales?
```python
model_df['target_sales_binary'] = (model_df['sales_next_7d'] > 0).astype(int)
```

**Stage 2 (Log Regression):** For events WITH sales, how many?
```python
model_df['target_sales_log'] = np.log1p(model_df['sales_next_7d'])
```

### 8.3 Why Log Transform the Target?

Raw sales ranged from 1 to 2,803 tickets. Taking the logarithm:
- **Compresses the scale** from 2,803 to 7.94
- Penalizes errors **proportionally**
- Creates a better distribution for regression

<img width="619" height="492" alt="image" src="https://github.com/user-attachments/assets/70c4547d-830a-4003-861f-41d3350f360d" />

*Figure 6: Log-transformed target distribution*

---

## 9. Temporal Train/Test Split

This is **critical** for preventing data leakage. Random splits on time-series data **allow future information to leak into training**. So, I needed to prevent my test data from every seeing its training data within the same event.

### 9.1 Date Range

So, I wanted to first understand the date ranges of my database and **create a reasonable cutoff** for my train/test split.

```python
print(f"Min: {model_df['snapshot_date'].min()}")  # 2025-10-01
print(f"Max: {model_df['snapshot_date'].max()}")  # 2026-01-12
```

### 9.2 Split Logic

The most logical split was to **use the beginning of the new year** as my test data, giving me 12 days worth of snapshots. Now also, I added some more split logic in here that ensures **only events that are in their final 2 weeks are included** in this test data.

```python
TRAIN_CUTOFF = pd.Timestamp('2026-01-01')

train_mask = model_df['snapshot_date'] < TRAIN_CUTOFF
test_mask = (
    (model_df['snapshot_date'] >= TRAIN_CUTOFF) &
    (model_df['days_to_event'] <= 14) &
    (model_df['days_to_event'] > 0)
)
```

<img width="1386" height="490" alt="image" src="https://github.com/user-attachments/assets/27700214-4626-4fde-a371-82613098ff7f" />

*Figure 7: Temporal split visualization showing train/test boundary*

**Result:**
- **Train:** 5,114,513 rows (all snapshots before Jan 1, 2026)
- **Test:** 58,022 rows (Jan 1-12, 2026, final 14 days before events)

### 9.3 Event Overlap Analysis

Now inherently, there will be overlap in my train and test data, since most events are promoting for their 2026 shows in late 2025. **These are the exact events I want to predict with**, simulating what we might know day-of to make predictions in the future.

```python
train_events = set(model_df[train_mask]['event_id_stubhub'])
test_events = set(model_df[test_mask]['event_id_stubhub'])
overlapping = train_events & test_events

print(f"Overlapping events: {len(overlapping)} ({100*len(overlapping)/len(test_events):.2f}% of test)")
# 8,228 events (92.56% of test)
```

With that, I'll be **training on nearly 110K events** and testing my upcoming models on **just over 8K event timelines**. For the events that do not overlap on this time range (essentially, events happening late January or beyond), these will be disregarded within my modeling sections, since they are **too far out** to predict any tangible sales movement.

<img width="1489" height="790" alt="image" src="https://github.com/user-attachments/assets/3d4094d0-3f46-4e00-994e-a902f2a7f5ea" />

*Figure 8: Visualizing a sample of valid train/test splits per event*

---

## 10. Feature Scaling

With all features engineered and the train/test split defined, I applied scaling as the **final** transformation step.

Scaling is a method to **standardize the range** of independent variables in my data. By showing every numerical feature with mean zero and a standard deviation of one, **standardization helps Neural Networks** as they try to take small steps within the gradient descent method. Variables with different scales will be unequally favored each step.

### 10.1 Why Scale at the End?

1. **Prevent leakage:** Scaler statistics must come from training data only
2. **Clean pipeline:** All derived features exist before scaling
3. **Neural networks need it:** Gradient descent converges poorly with unscaled features

### 10.2 RobustScaler (Fit on Train Only)

I used `RobustScaler` instead of `StandardScaler` because it's *resistant to remaining outliers (**uses median/IQR** instead of mean/std). The code below shows the scaling of all numerical features in my data, and then joining those back to the original dataframe with the categorical and binary features.

```python
from sklearn.preprocessing import RobustScaler

features_to_scale = [
    'get_in_log', 'listings_median_log', 'listings_active_log',
    'venue_capacity_log', 'sales_total_7d_log',
    'days_to_event', 'inv_per_day', 'price_spread_ratio',
    'sales_total_change_7d', 'listings_active_change_7d'
]

scaler = RobustScaler()

# Fit on training data only
train_data = model_df[model_df['split'] == 'train'][features_to_scale]
scaler.fit(train_data)

# Transform both train and test
scaled_features = scaler.transform(model_df[features_to_scale])
scaled_df = pd.DataFrame(
    scaled_features,
    columns=[f'{c}_scaled' for c in features_to_scale],
    index=model_df.index
)
model_df = pd.concat([model_df, scaled_df], axis=1)
```

### 10.3 Scaled Feature Ranges

After scaling, features are **centered around 0** with reasonable ranges:

| Feature | Mean | Std | Min | Max |
|---------|------|-----|-----|-----|
| get_in_log_scaled | -0.19 | 1.47 | -4.43 | 2.92 |
| listings_median_log_scaled | 0.09 | 0.89 | -4.80 | 2.97 |
| listings_active_log_scaled | 0.01 | 0.62 | -0.96 | 1.58 |
| venue_capacity_log_scaled | 0.06 | 0.95 | -2.25 | 3.18 |
| sales_total_7d_log_scaled | 0.70 | 1.30 | 0.00 | 10.05 |

---

## 11. Correlation Analysis

Before modeling, I also validated my engineered features by **checking correlations with the target**.

### 11.1 Feature-Target Correlations

```python
target_corr = corr_matrix['target_sales_log'].sort_values(ascending=False)
```

| Feature | Correlation |
|---------|-------------|
| sales_total_7d_log | +0.843 |
| listings_active_log | +0.495 |
| venue_capacity_log | +0.406 |
| inv_per_day | +0.246 |
| sales_total_change_7d | +0.169 |
| listings_median_log | +0.155 |
| get_in_log | +0.101 |
| days_to_event | -0.085 |
| price_spread_ratio | -0.145 |

**Key insight:** Past sales (`sales_total_7d_log` at 0.843) is **by far the strongest predictor**, signaling that momentum matters most in ticket sales.

### 11.2 Multicollinearity Check

I found that **`get_in_log`** and **`listings_median_log`** correlated at 0.87, which is not unexpected, since the floor and median ticket prices **move together**, but also not redundant enough to drop.

<img width="1299" height="1212" alt="image" src="https://github.com/user-attachments/assets/f2d69d74-1661-45ca-8f59-0916439ebc91" />

*Figure 9: Heatmap of numerical variables and their correlations*

---

## 12. Export to BigQuery

With feature engineering complete, I **pushed the processed dataframes** (both train and test) back to BigQuery for modeling.

### 12.1 Final Feature Count

**30 features ready for modeling:**
- 10 scaled log features (prices, inventory, sales, venue capacity)
- 5 temporal features (dow_sin, dow_cos, is_weekend, is_anomaly_day, is_final_week)
- 7 bucket dummies (one-hot encoded focus_bucket)
- 7 interaction terms (dte × bucket)
- 1 price tier categorical

---

## What's Next

With 30 engineered features stored in BigQuery, my data was finally ready for modeling. But which algorithm would work best for ticket sales prediction?

In **Part 4: Foundation Models**, I put five different approaches head-to-head:
- Ridge Regression (linear baseline)
- Gradient Boosting (sequential trees)
- XGBoost (optimized gradient boosting)
- LightGBM (fast gradient boosting)
- CatBoost (categorical-aware boosting)

The winner? Tree-based models dominated. But the gap between them was surprisingly small, setting up some ensemble methods in Part 7.

---
