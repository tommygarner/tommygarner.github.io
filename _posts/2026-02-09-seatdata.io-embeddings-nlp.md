---
layout: single
title: "SeatData.io Part 9: Embeddings and NLP"
date: 2026-02-09
description: "Using Wikipedia text, Last.fm data, and sentence transformers to give the model a sense of what each event actually is"
author_profile: true
toc: true
toc_sticky: true
tags:
  - embeddings
  - NLP
  - sentence transformers
  - PCA
  - Wikipedia
  - Last.fm
  - feature engineering
excerpt: "Building entity embeddings from Wikipedia and Last.fm data to test whether knowing what an event is improves ticket sales forecasting."
published: true
---

<img width="522" height="280" alt="image" src="https://github.com/user-attachments/assets/511db614-a239-4743-92db-092dad837ba6" />


## Overview

Every model I have built so far treats events as anonymous. The features describe the market state: how many listings are active, what the get-in price is, how many days until the event, what bucket it falls into. None of those features say anything about who the artist is, whether the venue is an arena or a small club, or whether the sports team is in contention for a playoff spot.

The hypothesis is simple. Two concerts with identical market snapshots might behave very differently if one is Taylor Swift and the other is a regional act playing a 500-seat venue. The model has no way to distinguish them today.

This post documents the setup for testing that hypothesis. I built a pipeline that:

1. Derives a canonical "slug" (a normalized identifier) for each event
2. Fetches Wikipedia summaries for artists, venues, teams, and events
3. Pulls Last.fm bios and tags for artists that Wikipedia doesn't cover
4. Converts all of that text into embedding vectors using a sentence transformer
5. Reduces those vectors with PCA and joins them onto the training data
6. Runs an ablation study across four embedding configurations and four model families
7. Evaluates results — none of the embedding configs beat the tabular baseline

---

## Why Embeddings for a Tabular Forecasting Problem

The existing feature set is good at capturing supply and demand signals. But it cannot capture anything about the identity of the event. This matters because identity drives demand patterns in ways that tabular signals can't fully reflect.

A high get-in price and low listing count means the market is tight, but it doesn't tell you whether that tightness is because Taylor Swift's fanbase is insatiable or because a promoter overpriced a mid-tier show and nobody is buying.

Embeddings offer a way to encode identity into the model without hand-engineering features for every artist or team. Instead of building "is this artist a top-40 act" or "does this team have a winning record" as explicit features, you represent each entity as a dense vector derived from text describing them. The model can then learn which dimensions of that vector are useful for predicting sales.

<img width="1200" height="572" alt="image" src="https://github.com/user-attachments/assets/a0e1cfd1-c34a-47b3-80e7-88885ed8a362" />

*Figure 1: Generated embeddings vectors*


This is borrowed from how NLP and recommendation systems handle cold-start problems: represent items as vectors in a learned space, then use those vectors as features alongside the tabular data.

---

## The Slug System

The first challenge is connecting event records to external data sources. Each training row has an `event_name` field like "Taylor Swift: The Eras Tour" or "Los Angeles Lakers vs Boston Celtics." To look these up in Wikipedia or Last.fm, I need a stable, normalized identifier.

I call these identifiers slugs. The slug derivation logic differs by event category:

- **Concert and Comedy events:** Strip venue info ("at Madison Square Garden"), feature tags ("presented by", "with special guest"), and format markers ("VIP Package") from the event name, then normalize to lowercase underscores. A slug looks like `artist::taylor_swift`.
- **Sports events:** Use the full event name normalized. A slug looks like `team::unknown::los_angeles_lakers_vs_boston_celtics`.
- **Everything else:** Normalize the full event name. A slug looks like `event::hamilton`.

Normalization removes accents, strips punctuation, and collapses whitespace. The goal is to be lenient enough that "O.A.R." and "OAR" map to the same slug, while being strict enough that "Taylor Swift" and "Taylor Swift - VIP" both reduce to `artist::taylor_swift`.

This slug system is used consistently across the data pipeline, the enrichment scripts, and the training notebook. Any mismatch between how a slug is derived at collection time versus join time means zero coverage, so keeping the logic centralized and identical across scripts was important.

---

## Data Collection

### Wikipedia

Wikipedia is the primary source. For each unique entity slug in the training and test data, I hit the Wikipedia API and pull the page summary (up to 2,000 characters).

<img width="1879" height="860" alt="image" src="https://github.com/user-attachments/assets/af73eb56-bf75-4b1f-8ad1-31fb60997619" />

*Figure 2: Wiki page for the San Antonio Spurs and the data I fetched for embeddings*

The disambiguation handling matters in practice. Many artist names return disambiguation pages. When that happens, I try each disambiguation option in order until one resolves to an actual article. If none resolve, the entity is marked `wiki_found=False` and gets a zero vector in the embedding matrix.

After running `expand_artist_embeddings.py` and `expand_venue_sports_embeddings.py`, the Wikipedia cache covers:

| Entity type | Slugs | Wikipedia found | Coverage |
|-------------|-------|----------------|---------|
| Artist | ~6,800 | ~4,525 | 61% |
| Team | varies | — | 95% |
| Venue | varies | — | 69% |
| Event | varies | — | 81% |

The hit rate for artists is around 61%, which is consistent with what you would expect: well-known touring artists tend to have Wikipedia pages, regional and emerging acts often do not. Teams have the highest coverage at 95% because major league and minor league team names resolve cleanly to Wikipedia articles.

### Last.fm

For the 1,299 concert and comedy artists with no Wikipedia page, I used the Last.fm API. The `artist.getInfo` endpoint returns a bio, a list of genre tags, and a listener count.

<img width="1874" height="822" alt="image" src="https://github.com/user-attachments/assets/b1eca6da-9ab2-4278-954d-124d13d8a09c" />

*Figure 3: Last.fm's artist information for Mt. Joy, one of my favorite bands*

I built a text string from those three fields:

```python
def _build_text(row) -> str:
    name  = row.get('lastfm_name') or row['entity_name']
    parts = [str(name)]
    if row.get('tags'):
        parts.append(f"Genres: {row['tags']}")
    if row.get('bio'):
        parts.append(str(row['bio']))
    if row.get('listeners', 0) > 0:
        parts.append(f"Last.fm listeners: {int(row['listeners']):,}")
    return '. '.join(p for p in parts if p)
```

The resulting string for O.A.R. looks like: "O.A.R.. Genres: rock, alternative, Jam, jam band, chill. O.A.R. (Of A Revolution) is an alternative rock band which formed in Rockville, Maryland...Last.fm listeners: 585,797"

That string gets passed through the sentence transformer alongside the Wikipedia summaries for other artists.

---

## Sentence Transformers

The model I used is `all-MiniLM-L6-v2` from the Sentence Transformers library. It is a distilled version of a larger BERT model fine-tuned to produce semantically meaningful sentence embeddings. It outputs 384-dimensional vectors. Two pieces of text that describe similar things will end up close together in that 384-dim space; two unrelated descriptions will be far apart.

<img width="1200" height="630" alt="image" src="https://github.com/user-attachments/assets/69662ceb-3f9a-4b67-87e0-449a38cd5274" />

*Figure 4: all-MiniLM-L6-v2 from HuggingFace*

I chose it for a few reasons. It is fast enough to encode 18,000 entities in a few minutes on CPU. It is small enough to run locally without a GPU. And it is well-tested on general English text, which is exactly what Wikipedia summaries are.

The encoding call:

```python
model = SentenceTransformer('all-MiniLM-L6-v2')
raw_embeddings = model.encode(
    texts,
    batch_size=256,
    show_progress_bar=True,
    normalize_embeddings=False,
)
```

The result is an (18,104, 384) matrix. One row per entity, 384 values per entity.

---

## AI Summary Generation

A secondary output of the same enrichment pipeline is 13,804 plain-English summaries, one per entity. These are 2-3 sentence descriptions generated for use in the Event Explorer dashboard's context cards, where a pricing manager can click on an event and see a brief description of the artist, venue, or team.

The generation model is GPT-4o-mini as the primary, with Gemini Flash as a fallback for cases where the OpenAI quota was exhausted. The input is the same Wikipedia or Last.fm text used to produce the embeddings. The output is then uploaded to a `dim_entity_summaries` table in BigQuery.

A venue example: "House of Blues Chicago" produces a 2-3 sentence description of the venue, its capacity, and its history as a live music destination. Those sentences appear in the dashboard card when a user selects an event at that venue.

The total generation cost was approximately $1.92, covering 7.4M input tokens and 1.3M output tokens. The job was checkpointed every 50 entities, making it fully resumable if interrupted.

These summaries are strictly for the user-facing dashboard. They are not model features and never enter the training pipeline.

---

## PCA Reduction

Feeding 384 raw embedding dimensions into a tree model on top of 29 existing features would be noisy. Many of those 384 dimensions carry redundant information. I apply PCA to compress them into a smaller set of dimensions that capture most of the variance.

<img width="682" height="414" alt="image" src="https://github.com/user-attachments/assets/d15e90ec-4c7e-47ec-ac3e-9fa49e6f63cb" />

*Figure 5: Dimensionality reduction using PCA*

The `expand_artist_embeddings.py` script fits PCA on the full embedding matrix after standardizing it:

```python
scaler  = StandardScaler()
scaled  = scaler.fit_transform(raw_embeddings)

pca     = PCA(n_components=n_components, random_state=42)
reduced = pca.fit_transform(scaled).astype(np.float32)
```

The resulting `entity_embeddings.parquet` contains 28,598 entity rows and 169 embedding dimensions (the number of components that fit given the entity count).

In an evaluation notebook, I also apply a second PCA pass specifically on the joined training data. This secondary PCA:

- Subsamples up to 200,000 matched training rows to keep memory manageable
- Finds the number of components explaining at least 75% of variance: **102 components**
- Transforms train, validation, and test splits consistently using that fitted PCA

The four feature configurations I test are:
- **baseline:** The original 29 tabular features only
- **baseline + emb10:** 29 features + first 10 PCA components
- **baseline + emb50:** 29 features + first 50 PCA components
- **baseline + emb200:** 29 features + all 102 PCA components (the full auto-selected set)

---

## Coverage

After joining embeddings onto train and test by slug:

| Split | Matched rows | Total rows | Coverage |
|-------|-------------|-----------|---------|
| Train | 2,781,692 | 4,073,559 | 68.3% |
| Val | 714,506 | 1,040,954 | 68.6% |
| Test | 35,936 | 58,022 | 61.9% |

Unmatched rows get zero vectors. The lower test coverage (61.9% vs 68%) is expected: the test set is a held-out time window with some events not present in training data, which means their slugs may not have been in the enrichment pipeline.

Coverage by bucket is not uniform:

- Concert: 87.2% match rate (artist enrichment was the primary focus of the pipeline)
- Major/Minor Sports: approximately 99% (team slugs resolve cleanly to Wikipedia)
- Broadway and Theater: 7.6% (event names don't slug-match Wikipedia well; most get zero vectors)
- Festivals and Other: lowest coverage

The Broadway gap is worth noting because it directly affects the results discussion. When 92% of Broadway rows carry zero vectors, any embedding config will behave almost identically to the baseline for that bucket.

---

## The Evaluation Design

The evaluation is structured as a feature ablation: does adding embeddings to the baseline features improve prediction?

**Feature configs:** 4 (baseline, +emb10, +emb50, +emb200)

**Model families:** 4 (XGBoost, LightGBM, CatBoost, sklearn MLP)

**Stages:** 2 (classifier, regressor)

**Total training slots:** 32

The full sweep covers 32 training slots (4 configs × 4 model families × 2 stages), each running 60 Optuna trials with a 15-minute timeout. Training data is 5.1M rows; validation is 1.04M rows. Positive-only rows for the regressor stage number 1.1M train and 325K val.

Each slot runs an Optuna search with 60 trials and a 15-minute timeout, using in-memory studies. The search spaces mirror the ones from Part 7. The classifier objective is PR-AUC on the validation fold. The regressor objective is RMSE on positive-only validation rows.

After all slots for a config are trained, Section 6 sweeps all combinations of classifier, regressor, and threshold (4 x 4 x 6 = 96 combos for a full model set) and computes two-stage RMSE on the held-out test set. The comparison metric is whether any embedding config beats the 18.53 RMSE champion from Part 7.

---

## Results

The full ablation ran overnight and completed all 32 training slots. Each slot produced a trained model for each config-family-stage combination. After that, I also swept all combos at threshold 0.90 (the optimal threshold from Part 7) and evaluated on the held-out test set.

### Full Test Set

The best model per config, measured by two-stage RMSE on the full 58,022-row test set:

| Config | Best clf | Best reg | RMSE | vs baseline |
|--------|---------|---------|------|------------|
| baseline | lgb | lgb | 19.09 | -- |
| baseline + emb10 | xgb | xgb | 19.25 | +0.15 |
| baseline + emb50 | xgb | xgb | 19.31 | +0.22 |
| baseline + emb200 | xgb | xgb | 19.21 | +0.12 |

None of the embedding configs improved on the baseline. Adding embeddings made things slightly worse, with emb200 being the least damaging (+0.12 RMSE).

For reference, the champion from Part 7 scored 18.53 RMSE with 200 Optuna trials. The baseline here scores 19.09 because this notebook used 60 trials per slot, so the comparison is internal: embedding configs vs the same baseline trained with a similar budget.

PR-AUC on the classifier stage tells a slightly different story. The embedding configs improve classification accuracy marginally:

| Config | Full PR-AUC |
|--------|------------|
| baseline | 0.814 |
| baseline + emb10 | 0.819 |
| baseline + emb50 | 0.819 |
| baseline + emb200 | 0.818 |

The classifier can use the embedding dimensions to better identify which events will have nonzero sales. But that small improvement in classification doesn't translate into lower RMSE on the full pipeline.

### Matched Rows Only

38% of test rows (22,086) have zero-vector embeddings because their slug didn't match anything in the entity store. Those rows introduce noise specific to the embedding configs. A fair question is whether embeddings help when you remove the unmatched rows from the evaluation.

Restricting to the 35,936 matched rows (61.9% of test):

| Config | Matched RMSE | vs baseline |
|--------|-------------|------------|
| baseline | 22.87 | -- |
| emb10 | 23.05 | +0.18 |
| emb50 | 23.17 | +0.30 |
| emb200 | 22.90 | +0.02 |

Even on matched rows, embeddings do not help overall. emb200 is essentially tied with baseline (+0.02 RMSE, within rounding). The case against embeddings holds even after removing the zero-vector noise.

### Where Embeddings Actually Help

The bucket-level breakdown is the most interesting part. Restricting to matched rows within each bucket:

| Bucket | n matched | Baseline RMSE | emb200 RMSE | Delta |
|--------|----------|--------------|------------|-------|
| Major Sports | 5,868 | 48.94 | 48.58 | -0.36 |
| Minor/Other Sports | 2,485 | 12.61 | 11.84 | **-0.77** |
| Comedy | 4,277 | 7.12 | 7.08 | -0.04 |
| Other | 8,021 | 18.37 | 18.43 | +0.06 |
| Broadway & Theater | 1,375 | 3.52 | 3.59 | +0.08 |
| Concert | 13,797 | 10.11 | 11.04 | **+0.93** |
| Festivals | 113 | 2.65 | 4.07 | +1.42 |

Sports events benefit from embeddings. Minor/Other Sports improves by 0.77 RMSE, Major Sports by 0.36. Team identity is predictive in a way that tabular features can't fully encode. "Lakers vs Celtics" has a different demand profile than a minor-league hockey matchup even if both show 200 active listings at a $40 get-in price. Wikipedia has 95%+ team coverage, so the embedding is actually present and carries meaningful signal about market size, rivalry context, and franchise history.

Concert is worse by nearly 1 RMSE point despite 87.2% match coverage. This is the opposite of what the hypothesis predicted. Artist Wikipedia pages are rich text: genre, career history, album releases, critical reception. But none of that tells you where in the demand curve a specific show sits at a specific point in its sales timeline. The market features already encode that implicitly. Taylor Swift shows up as high get-in price, low active listings, large venue capacity. That is identity, encoded through current market state. Adding a dense vector describing her career history adds noise rather than signal.

---

## What the Results Mean

The answer to the original question is: embeddings **do not help** this forecasting problem on net.

The intuition behind the experiment was reasonable. Artist identity should matter. Taylor Swift should sell differently than a regional act, even with identical market snapshots. But the model already captures identity effects indirectly. High-demand artists have high get-in prices and low listing counts. Low-demand acts have the opposite. The tabular features are a proxy for identity that is measured at the right time and at the right granularity.

Wikipedia text summarizes an artist's career and genre. That is useful background knowledge. But it does not tell you where in the demand curve a specific event is at a specific point in its selling timeline. A 384-dim sentence embedding of an artist's Wikipedia page and a 7-day sales velocity measurement are not the same type of information.

The tabular features are not identity-agnostic. They are identity-encoded-at-prediction-time. A $300 get-in price and 15 active listings in a 20,000-seat venue tells you more about likely demand in the next 7 days than knowing the artist won a Grammy. Embeddings describe what an entity is. Market features describe how that entity's event is performing right now.

The sports result is a partial exception. Team embeddings add a small but consistent improvement. Sports teams have fewer events than artists, and team identity (market size, current season performance, rivalry matchups) may be harder for the tabular features to encode purely through price and listing signals. A PCA-compressed Wikipedia embedding of "Los Angeles Lakers" carries something the model didn't have before.

---

## What Comes Next

A second angle worth testing is whether embeddings help in a neural network architecture rather than a tree model. PCA-compressed dense embeddings are exactly the kind of input that residual networks are designed to handle. The MLP configs in this experiment scored the worst of the four families, which is consistent with MLP not being the right architecture for this input type. A proper ResNet or TabNet trained jointly on the tabular and embedding features might show a different result.

For the current pipeline, the tabular features are sufficient. The 18.53 RMSE champion from Part 7 stands as the best result, and embeddings do not improve it.
