---
layout: single
title: "SeatData.io Part 5: Neural Networks"
date: 2026-01-24
description: "Testing neural network architectures in a two-stage classification and regression pipeline for ticket sales prediction"
author_profile: true
toc: true
toc_sticky: true
tags:
  - neural networks
  - deep learning
  - keras
  - tensorflow
excerpt: "Applying deep learning to ticket sales prediction to find for accuracy improvements while tuning up complexity."
published: true
---

<img width="600" height="500" alt="image" src="https://github.com/user-attachments/assets/88a577dc-0f06-4171-b20e-40f8e0c3aac6" />


## Abstract

Tree-based models dominated Part 4, with the best two-stage pipeline (GradientBoosting + LightGBM) achieving **RMSE 19.15 tickets and MAE 3.89 tickets**.

> **Note:** The 19.15 RMSE baseline (and the NN results below) cover all 58,022 test
> events, including zero-sale events. See Part 4 for a full explanation of how the
> two-stage denominator differs from standalone regression RMSE (~32 tickets).

But I wanted to test if neural networks could match or beat these results. Using the same two-stage approach; first classifying whether sales will occur, then predicting volume, I tested four more architectures: Naive Bayes, MLP, Skip-Connection Networks, and LSTM. 

The results surprised me: **neural networks nearly matched tree performance**, with the best combination achieving an **MAE of 3.96 tickets**, slightly worse than tree-based predictions while taking 100x longer to train.

## Key Insights

- **Neural Networks Competitive on Two-Stage Task** - Best NN pipeline achieved MAE 3.65 tickets vs trees at 3.89 tickets
- **MLP Dominates Among NN Architectures** - Simple feedforward network outperformed skip connections and LSTM; sequential is better
- **Classification Performance Strong** - All NNs achieved 0.79+ PR-AUC, with MLP reaching 0.8059
- **Training Time Trade-Off** - NNs took 20+ minutes vs 3.5 seconds for LightGBM with only marginal improvement

---

## 1. The Two-Stage Approach

In Part 4, I discovered that the best prediction strategy wasn't a single model... it was a **two-stage pipeline**:

### 1.1 How Did I End Up at Two Stages?

My data has a big class imbalance, where **71.7% of events have zero sales in the final two weeks**. This creates two distinct challenges:

**Challenge 1:** Predicting IF any tickets will sell (classification)
**Challenge 2:** Predicting HOW MANY tickets will sell (regression)

A single regression model struggles because it must:
- Predict exactly 0 for the 71.7% of zero-sale events
- Predict continuous values (1, 5, 10, 50 tickets) for the 28.3% with sales
- Balance the loss function between these competing objectives

The two-stage approach separates these concerns:

```
Stage 1 (Classifier): Will this event have ANY sales in the final two weeks?
- NO results in 0 ticket-prediction
- YES allows Stage 2 continuation
Stage 2 (Regressor): How many tickets will sell?
```

This improved Part 4's best single-stage regressor (LightGBM: 31.90 tickets RMSE) to a two-stage pipeline that achieved **19.15 tickets RMSE** on all test data.

### 1.2 Neural Networks

<img width="800" height="504" alt="image" src="https://github.com/user-attachments/assets/44752df7-3782-42ed-b748-114cec5594a1" />

*Figure 1: Neural Network diagram*

Neural networks function very similar to neurons in our brain, adding weights and biases between each connection. Yet, these complex models are built to learn the non-linear patterns in data by applying activation functions, such as ReLU or Tanh.

<img width="1092" height="742" alt="image" src="https://github.com/user-attachments/assets/40b8af80-e9fd-4e0b-8d89-df81cd679d06" />

*Figure 2: What happens inside a Neural Network*

Neural Nets are similar to trees in that they are very flexible. A single network can simultaneously:
- Output a probability (sigmoid activation) for classification
- Output a continuous value (linear activation) for regression

But I took a different approach. I wanted to **train separate networks for each stage**, allowing each to optimize for its specific task. This also let me test combining tree classifiers with NN regressors to find the optimized pipeline.

---

## 2. Data Preparation for Neural Networks

Before training, I needed to prepare data differently than I did for trees in Part 4.

### 2.1 Scaling

Trees don't care about feature scales. XGBoost happily splits on `days_to_event` (range 0-338) and `dow_sin` (range -1 to 1) without complaint.

Neural networks **care.**

Gradient descent is the training algorithm for NNs. This method updates weights proportionally to feature magnitudes. If one feature ranges from 0-1000 and another from 0-1, the optimizer will:
- Take huge steps for the 0-1000 feature
- Take tiny steps for the 0-1 feature
- Struggle to converge

<img width="600" height="393" alt="image" src="https://github.com/user-attachments/assets/5c58d132-c68d-4515-9e79-eb76fd181dc2" />

*Figure 3: Comparing non-scaled and scaled gradient descent*

The solution is found in scaling these feature spaces to force Gradient Descent to take equal steps. For this, I used **StandardScaler**

```python
from sklearn.preprocessing import StandardScaler

scaler_cls = StandardScaler()
X_train_cls_scaled = scaler_cls.fit_transform(X_train_cls)
X_test_cls_scaled = scaler_cls.transform(X_test_cls)

scaler_reg = StandardScaler()
X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
X_test_reg_scaled = scaler_reg.transform(X_test_reg)
```

This transforms every feature to:
- Mean zero (center the data)
- Standard deviation of 1 (same scale)

Importantly, I fit the scaler on **training data only**, then applied the same transformation to test data. This prevents data leakage because I'm preventing my test data from seeing the distribution of features. 

### 2.2 Time-Based Validation Split

For early stopping, I needed a validation set to use within training these Neural Nets. I used **the last 3 days of training data** (just before the test period):

- Training: Oct 1 - Dec 28
- Validation: Dec 29-31 (for early stopping)
- Test: Jan 1-12 (final evaluation)

This mimics a final production, where model-users would want to predict the future using only the past.

```python
VAL_START = train_df['snapshot_date'].max() - pd.Timedelta(days=3)
val_mask = train_df['snapshot_date'] >= VAL_START

X_train_nn = X_train_cls_scaled[~val_mask]
X_val_nn = X_train_cls_scaled[val_mask]
```

### 2.3 Class Imbalance Handling

With 71.7% zero-sales events, classifiers would achieve 71.7% accuracy by always predicting "no sales." I addressed this with **class weights**:

```python
neg_count = (y_train_cls == 0).sum()  # Zero-sale events
pos_count = (y_train_cls == 1).sum()  # Nonzero-sale events
scale_pos_weight = neg_count / pos_count  # ~2.53

class_weight_dict = {0: 1.0, 1: scale_pos_weight}
```

This outright tells the network: "Misclassifying a positive sample costs 2.53× more than misclassifying a negative sample," defining my data's cost matrix. The network learns to find that 28.3% of sales events.

---

## 3. Architecture 1: Naive Bayes

Before diving into neural networks, I established a simple baseline.

### 3.1 Why Naive Bayes?

Naive Bayes makes a strong assumption: **features are independent given the class**. For ticket sales, this means:

```
P(sales | price, days_to_event, venue_capacity, etc..) =
    P(sales | price) × P(sales | days_to_event) × P(sales | venue_capacity) x ...
```

This is obviously wrong (price and days_to_event interact, as I saw in my EDA), but Naive Bayes is fast and interpretable.

```python
from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train_cls_scaled, y_train_cls)
```

### 3.2 Results (Classification Only)

- **AUC-ROC: 0.8943**
- **PR-AUC: 0.7624**
- **Precision: 0.7167** (when it predicts sales, it's right 72% of the time)
- **Recall: 0.7577** (finds 76% of all events with sales)
- **Training Time: 2.2 seconds**

For a model that assumes independence and trains in 2 seconds, this is impressive. It correctly identifies 3 out of 4 events that will have sales.

It's good to note that Naive Bayes can only do classification, since it tries to find the probability of a class or category. So here, I chose not to implement what could be a Bayesian Regression, or simply predicting yesterday's sales will be the same as today's.

---

## 4. Architecture 2: MLP Neural Network

Next, I wanted to finally jump into deep learning with a Multi-Layer Perceptron (MLP).

### 4.1 MLP Classifier Architecture

```python
def build_mlp_classifier(input_dim, layers_config=[256, 128, 64], dropout=0.3):
    inputs = Input(shape=(input_dim,))
    x = BatchNormalization()(inputs)

    for units in layers_config:
        x = Dense(units, kernel_regularizer=regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Dropout(dropout)(x)

    outputs = Dense(1, activation='sigmoid')(x)  # Probability output
    return Model(inputs, outputs)
```

This creates a 4-layer network:
- **Layer 1:** 256 input neurons
- **Layer 2:** 128 hidden neurons
- **Layer 3:** 64 hidden neurons
- **Output:** 1 output neuron with sigmoid activation (probability of sales)

Each layer uses:
- **L2 Regularization:** Ridge penalty on large weights (0.001 strength)
- **Batch Normalization:** Stabilizes training by normalizing layer inputs
- **Dropout (30%):** Randomly drops neurons during training to prevent overfitting
- **ReLU Activation:** `f(x) = max(0, x)` to introduce non-linearity

### 4.2 MLP Regressor Architecture

The regression network is pretty identical to the classifier except that:
- Trained only on the 28.3% of events with sales
- Output layer uses **linear activation** 
- Loss function is **MSE** instead of binary cross-entropy

```python
def build_mlp_regressor(input_dim, layers_config=[256, 128, 64], dropout=0.3):
    # ... same architecture ...
    outputs = Dense(1, activation='linear')(x)  # Continuous output
    return Model(inputs, outputs)
```

### 4.3 Training

After defining these functions, I need to put some guardrails around building these models, such as defining stopping points when the model begins overfitting the data.

<img width="708" height="431" alt="image" src="https://github.com/user-attachments/assets/e346cf6b-1c03-4e32-9492-05d2d60e6bee" />

*Figure 4: Early Stopping*

```python
early_stop = callbacks.EarlyStopping(
    monitor='val_loss', patience=10, restore_best_weights=True
)
reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
)

mlp_cls.fit(
    X_train_nn, y_train_nn_cls,
    validation_data=(X_val_nn, y_val_nn_cls),
    epochs=100, batch_size=1024,
    callbacks=[early_stop, reduce_lr],
    class_weight=class_weight_dict  # Handle imbalance
)
```

**Early Stopping:** If validation loss doesn't improve for 10 epochs, stop training and restore the best weights.

**Learning Rate Reduction:** If validation loss plateaus for 5 epochs, cut the learning rate in half. This helps the optimizer escape local minima.

### 4.4 Results

**Classification:**
- **AUC-ROC: 0.9140** (best among all classifiers)
- **PR-AUC: 0.8059** (handles class imbalance well)
- **Precision: 0.6213** (when it predicts sales, it's right 64% of the time)
- **Recall: 0.8973** (finds 88% of all events with sales)
- **F1-Score: 0.7342**
- **Training Time: 20 minutes**

**Regression (on events with sales only):**
- **RMSE (tickets): 38.5**
- **MAE (tickets): 9.77**
- **R²: 0.6860** (explains 68.6% of variance)
- **Training Time: 4.5 minutes**

The MLP classifier dominated, achieving the highest AUC-ROC (0.9140) and finding 89.73% of sales events. The high recall came at a cost though, as precision dropped to 62%. But for this problem, I'd rather overpredict sales than miss opportunities.

The regressor performed worse than Part 4's LightGBM (31.90 tickets RMSE), achieving **38.5 tickets RMSE (about 21% worse)**. 

---

## 5. Architecture 3: Skip-Connection Neural Network

Inspired by ResNet (Residual Networks), I tested whether skip connections could improve performance.

### 5.1 Vanishing Gradients

In deep networks, gradients (signals for weight updates) get smaller as they propagate backward through layers. By layer 5, gradients might be 0.0001, essentially too small to update weights effectively.

<img width="716" height="454" alt="image" src="https://github.com/user-attachments/assets/3470d9b6-f83b-4113-a286-68bf44dce99d" />

*Figure 5: Vanishing gradients as layers increase*

This is the **vanishing gradient problem**, limiting how deep you can go.

### 5.2 Skip Connections

<img width="651" height="371" alt="image" src="https://github.com/user-attachments/assets/fb98f89e-3c65-43c9-b4ae-8e06750e9678" />

*Figure 6: Skip connections visualized*

ResNet solved this vanishing gradient by adding **skip connections** for gradients:

```python
def build_skip_classifier(input_dim, hidden_units=128, num_blocks=3):
    inputs = Input(shape=(input_dim,))
    x = Dense(hidden_units)(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    for _ in range(num_blocks):
        residual = x  # Save the input to this block

        # Process through two layers
        x = Dense(hidden_units, kernel_regularizer=regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Dropout(0.3)(x)

        x = Dense(hidden_units)(x)
        x = BatchNormalization()(x)

        x = Add()([x, residual])  # Skip connection
        x = Activation('relu')(x)

    outputs = Dense(1, activation='sigmoid')(x)
    return Model(inputs, outputs)
```

The `Add()` layer creates a shortcut: "Here's the processed signal AND the original signal from before this block." Gradients can now flow backward through these shortcuts, enabling deeper networks.

### 5.3 Results

**Classification:**
- **AUC-ROC: 0.9129** 
- **PR-AUC: 0.8013** 
- **Precision: 0.6150**
- **Recall: 0.8972**
- **F1-Score: 0.7297**
- **Training Time: 11 minutes**

**Regression:**
- **RMSE (tickets): 32.96**
- **MAE (tickets): 10.5**
- **R²: 0.6431**
- **Training Time: 11.5 minutes**

Skip connections did seem to help. Though the classifier performed essentially the same as MLP, the regressor was **better** (38.5 vs 32.96 tickets RMSE). But R² dropped, suggesting worse fit.

The architecture added complexity and marginal performance. While exploring these skip connections, I found that they are most helpful in very deep networks when trying to preserve signal, going beyond 50 layers, for example. At 3 residual blocks, they're probably overkill.

---

## 6. Architecture 4: LSTM

LSTM (Long Short-Term Memory) networks excel at sequences. But my features aren't sequential, they're tabular (price, days_to_event, venue_capacity).

<img width="674" height="345" alt="image" src="https://github.com/user-attachments/assets/bacc3304-fcaf-4a17-b07f-bf0fef47e6a2" />

*Figure 7: LSTM Diagram*

However, my assumption was that an LSTM would help preserve the signal in my sales (and other) features. Visually, this is the orange line running across the top of the diagram.

### 6.1 Why Test LSTM?

I wanted to see if LSTM could find **implicit sequential patterns** in feature ordering. Maybe the network would learn: "If `days_to_event` is small AND `get_in` is high, weight `listings_active` differently," almost similar to the rule-based learning I tried previously.

```python
def build_lstm_classifier(input_dim, lstm_units=64):
    inputs = Input(shape=(input_dim,))
    x = Reshape((input_dim, 1))(inputs)  # Treat features as sequence
    x = LSTM(lstm_units, return_sequences=False, dropout=0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    return Model(inputs, outputs)
```

The `Reshape` layer treats 30 features as a sequence of length 30, each with 1 value. The LSTM processes this "sequence" left-to-right.

### 6.2 Results

**Classification:**
- **AUC-ROC: 0.9024**
- **PR-AUC: 0.7851**
- **Precision: 0.6124**
- **Recall: 0.8885**
- **F1-Score: 0.7251**
- **Training Time: 43 minutes**

**Regression:**
- **RMSE (tickets): 42**
- **MAE (tickets): 11.68**
- **R²: 0.5524**
- **Training Time: 14 minutes**

LSTM struggled. The classifier was the worst NN. The regressor was also the worst I had seen beyond Linear Regression: **42 tickets RMSE**, 9% worse than MLP's 38.5.

Why? **Tabular data has no sequential structure.** The order of features is arbitrary. LSTM wasted capacity learning left-to-right dependencies that don't exist.

This confirmed that **LSTMs are the wrong tool for tabular data.**

---

## 7. Neural Network Comparison

Let me compare all four architectures side-by-side.

### 7.1 Classification Leaderboard

| Model          | AUC-ROC | PR-AUC | Precision | Recall | F1-Score | Time (m) |
|----------------|---------|--------|-----------|--------|----------|----------|
| MLP            | 0.91    | 0.81   | 0.62      | 0.90   | 0.73     | 20       |
| SkipConnection | 0.91    | 0.80   | 0.61      | 0.90   | 0.73     | 11       |
| LSTM           | 0.90    | 0.79   | 0.61      | 0.89   | 0.73     | 43       |
| NaiveBayes     | 0.89    | 0.76   | 0.72      | 0.76   | 0.74     | 0        |

**Winner: MLP** (best PR-ROC)

The simple MLP barely beat out the other sophisticated architectures. Skip connections added no value in classification while LSTM took 43 minutes for worse performance.

<img width="825" height="596" alt="image" src="https://github.com/user-attachments/assets/3d6a4d72-bdbf-4f70-b5f3-5806c34d3a5d" />

*Figure 7: PR Curves of each deep learning classifier*


Naive Bayes does deserves respect: 2 seconds for 0.89 AUC-ROC and 0.76 PR-AUC is very good. Its precision (0.72) was highest, making it useful when false positives are costly.

### 7.2 Regression Leaderboard

| Model          | RMSE (Tickets) | MAE (Tickets) | Time (Min) |
|----------------|----------------|---------------|------------|
| SkipConnection | 32.96          | 10.50         | 11.51      |
| MLP            | 38.50          | 9.77          | 4.42       |
| LSTM           | 42.06          | 11.68         | 14.12      |

**Winner? Depends.** 

Here is where metrics really matter. RMSE or MAE? 

Mean Average Error is probably easier to understand for stakeholders: on average, a model was off on its predictions by X tickets. 

However, Root Mean Squared Error takes into consideration the magnitude of misses by penalizing worse predictions. Particular to this ticketing data, I believe RMSE is the better metric to keep in mind! Especially because the ticket data is so sparse, but also because there are some really big hits in sales!

Therefore, I say Skip Connections won out here. This model had the lowest RMSE of ticket-error by a wide margin, although with a lower R² measuring overall fit than an MLP.

MLP, though, wins on speed (4.5 min vs 11.5) and explains 69% of variance in my ticket data.

---

## 8. Two-Stage Combined Performance

The real test, as I found in the previous blog, is combining these two stages on the full test set.

### 8.1 Combining Neural Networks

For each classifier-regressor pair:
1. Classifier predicts which events will have sales
2. For predicted positive events, regressor predicts volume
3. All other events get 0 tickets

### 8.2 Results: Neural Network Combinations

| Classifier     | Regressor      | RMSE (Log) | MAE (Log) | RMSE (Tickets) | MAE (Tickets) |
|----------------|----------------|------------|-----------|----------------|---------------|
| NaiveBayes     | SkipConnection | 0.73       | 0.35      | 19.70          | 3.96          |
| LSTM           | SkipConnection | 0.76       | 0.40      | 19.70          | 4.07          |
| SkipConnection | SkipConnection | 0.77       | 0.40      | 21.85          | 4.22          |
| NaiveBayes     | MLP            | 0.70       | 0.34      | 22.04          | 3.63          |
| LSTM           | MLP            | 0.73       | 0.39      | 22.04          | 3.73          |
| MLP            | MLP            | 0.74       | 0.38      | 22.14          | 3.84          |
| SkipConnection | MLP            | 0.74       | 0.39      | 22.35          | 3.83          |
| MLP            | SkipConnection | 0.77       | 0.40      | 22.63          | 4.26          |
| NaiveBayes     | LSTM           | 0.72       | 0.37      | 22.95          | 3.95          |
| LSTM           | LSTM           | 0.79       | 0.45      | 22.97          | 4.16          |

**Winner: Naive Bayes classifier + Skip Connection regressor**
- **MAE: 3.96 tickets**
- **RMSE: 19.7 tickets**

The surprise winner is a fast Naive Bayes classification paired with the more complex Skip Connection regression. Why?

**Naive Bayes has the highest precision**. When it predicts sales, it's right 72% of the time. This reduces false positives, meaning the regressor isn't wasting effort on events unlikely to sell.

<img width="833" height="613" alt="image" src="https://github.com/user-attachments/assets/ec05de56-371b-4eed-a7b8-b2223b79e6a3" />

*Figure 8: Confusion Matrix of the NB classifier*

MLP classifiers have higher recall (90%) but lower precision (62%). They send more events to the regressor, including many false positives. The regressor then predicts positive sales for zero-sale events, inflating the error.

So for two-stage models, **precision matters more than recall** in the classifier. It is actually better to miss some sales events than waste the regressor on false positives.

<img width="828" height="572" alt="image" src="https://github.com/user-attachments/assets/b5136e77-c856-4e4c-8ac1-b2e7a7997c48" />

*Figure 9: Spread of residuals in the two-stage model*

---

## 9. More Comparisons

Next, I wanted to investigate how my trees held up against these deep learning models.

### 9.1 Regression-Only Comparison (Events with Sales)

| Model Type | Best Model | RMSE (tickets) | MAE (tickets) | R² |
|------------|------------|----------------|---------------|----|
| **Trees** | **LightGBM** | **31.90** | **10.26** | **0.65** |
| Neural Networks | Skip Connection | 32.96 | 10.5 | 0.64 |

Trees barely won across the board.

### 9.2 Two-Stage Comparison

But that's standalone when it comes to regression. I need to compare the best hurdle combos I found in each notebook.

| Model Type | Best Combination | RMSE (tickets) | MAE (tickets) | 
|------------|------------------|------------|---------------|
| **Trees** | **GB + LightGBM** | **19.15** | **3.89** |
| Neural Networks | NB + Skip Connection | 19.7 | 3.96 |

Still, tree combinations of classification and regression found the lowest error among my ticket data in their predictions. 

This surprised me, as I assumed that more complex modeling might achieve tighter predictions for my ticket data. However, what I learned here is that sometimes simple isn't that bad!

## 10. Feature Importances

It's important that these models don't stay black boxes, as I mentioned in the last post. However, there isn't a simple `.feature_importances_` function attached to tree-based learning. Actually, Neural Nets are notorious for being hard to interpret.

### 10.1 Sklearn's `permutation_importance`

`Sklearn.inspection` holds the `permuation_importance` package inside it. The logic is actually quite simple. I can understand how accuracy decreases or increases when a specific feature is ruined. 

**How does it work?**

1. Baseline - my Naive Bayes + Skip Connection model performs on the data normally
2. Shuffle - then, `permutation_importance` takes one feature and randomly shuffles its values across all rows. So, the distribution remains the same, but the relationships tied to a certain event, for example, is broken, especially between the feature and sales.
3. Repredict - my Naive Bayes + Skip Connection model attempts to predict on this new, shuffled data representation. If accuracy drops, that feature is determined important.
4. Repeat - this process occurs multiple times to make sure importances aren't just lucky shuffles and actually noisy features

### 10.2 Results

<img width="824" height="545" alt="image" src="https://github.com/user-attachments/assets/ee76d2d6-725c-465b-826d-f5e45a792b9b" />

*Figure 10: Classifier's top features*

It's apparent and consistent across tree-based classifiers and my naive bayes that, if an event has previous sales in the secondary market, it will likely have more in the future!

<img width="724" height="523" alt="image" src="https://github.com/user-attachments/assets/52e0734a-da16-49de-a7b9-8111e39f19c0" />

*Figure 11: Regressor's top features*

This is a bit tricky to read, but negative value is best, since the calculation is based on negative mean squared error impact. So, categorical features and anomaly days are some of the most important features according to my regressor.

An interpretation of this could be that, if shuffling `bucket_Concert` within my data, importance drops by about 0.0055 points. This means that my model's error spiked signficantly without that categorical information. The model must rely on those bucket categories to anchor its volume predictions!

---

## 11. Why Neural Networks Nearly Matched Trees

Three factors enabled NNs to compete:

### 11.1 Proper Regularization

Without dropout, batch normalization, and L2 penalties, neural networks overfit like crazy. 

### 11.2 The Two-Stage Approach

Separating classification from regression let each network specialize. The classifier focused on finding the 28.3% of sales events. The regressor focused on predicting volume for a more similar subset.

This wasn't totally necessary, however, because these Neural Networks specialize in finding nonlinear patterns in data. 

### 11.3 Feature Engineering

The 30 features I engineered in Part 3 were designed for tree models. But they also work for neural networks:
- Log-scaled features reduce skew
- Cyclical encoding (sin/cos) for day-of-week captures periods of time
- Interaction terms (e.g., `days_to_event × get_in`) provide non-linear combinations

Good features make any model better, even the "wrong" model.

---

## 12. When Would Neural Networks Win?

Neural networks nearly tied trees despite being at a disadvantage when facing tabular data. When would they win?

### 12.1 Unstructured Features

If I had:
- **Event descriptions** (free text): "Taylor Swift Eras Tour - Opening Night"
- **Venue images** (seating chart photos)
- **Artist bios** (Wikipedia text)

<img width="2382" height="1322" alt="image" src="https://github.com/user-attachments/assets/e2a6e5d9-e6f0-41cb-89b1-f879de593e5a" />

*Figure 12: Unstructured vs Tabular data*

Then neural networks would dominate. CNNs for images, Transformers for text. Trees can't process unstructured data.

### 12.2 Massive Datasets

At 10M+ training samples, neural network capacity becomes an advantage. Trees plateau. You can't add 10,000 trees without overfitting.

Neural networks scale by:
- Adding layers
- Adding neurons per layer
- Training longer

### 12.3 Transfer Learning

Pre-trained models (BERT, ResNet) can be fine-tuned on small datasets. If I had event descriptions, I could:
1. Use a pre-trained BERT model (trained on billions of text examples)
2. Fine-tune on my 5M events
3. Leverage linguistic knowledge learned from Wikipedia, books, etc.

Trees have no transfer learning equivalent. Every dataset starts from scratch.

<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/11bffa7d-5851-417d-9201-9f5ead3b5068" />

*Figure 13: Transfer Learning diagram*

---

## 13. Lessons Learned

### What Worked

**Naive Bayes + Skip Connection was the best combination.** Simple classifier with high precision, paired with powerful regressor learning weights and biases of residuals.

**MLP dominated NN architectures.** Skip connections and LSTM added complexity without adding performance when looking at nonzero-sales alone.

**Regularization transformed NNs.** Dropout, batch normalization, and L2 penalties are essential to prevent overfitting to data.

**Neural networks nearly matched trees** on two-stage performance (MAE 3.96 vs 3.89 tickets).

### What Surprised Me

**Naive Bayes beat NN classifiers in two-stage.** Its high precision (72%) reduced false positives, helping the regressor.

**LSTM failed completely.** 38 minutes of training for the worst performance. Tabular data has no sequential structure.

**R² told a different story than RMSE.** NNs had better R² but worse RMSE than other regressors. A few outlier predictions likely skewed RMSE.

### The Truth

**For tabular data with <10M rows, gradient boosting wins on speed.** Trees train 187× faster with comparable accuracy.

But neural networks aren't far behind. With GPUs and proper regularization, they can compete. For production systems with unstructured data (text, images), they're the only option.

---

## 14. What's Next

I've tested:
- Linear models (Part 4): MAE 13.31 tickets
- Tree models (Part 4): MAE 3.89 tickets
- Neural networks (Part 5): MAE 3.96 tickets

Both trees and NNs used **default or lightly-tuned hyperparameters**. Could an optimization squeeze out better performance?

In **Part 6: Hyperparameter Tuning**, I'll:
- Use RandomizedSearchCV to optimize tree-based learning
- Test 50+ hyperparameter combinations per model
- Find the optimal learning rate, depth, regularization
- See if I can push below RMSE of 19 tickets and MAE of 3.5 tickets

Then in **Part 7: Ensemble Stacking**, I'll combine the best models. Neural networks and trees make different types of errors. Averaging them might reduce variance and improve predictions.
